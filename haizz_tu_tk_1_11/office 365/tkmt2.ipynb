{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsccfSDERjBo"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Nvp8jjSMsk"
   },
   "source": [
    "### C√¢u 1 (3 ƒëi·ªÉm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxZCV9vTRtMh"
   },
   "source": [
    "M·ªôt c∆° s·ªü s·∫£n xu·∫•t ƒë·ªì g·ªó d·ª± ƒë·ªãnh s·∫£n xu·∫•t ba lo·∫°i s·∫£n ph·∫©m l√† b√†n,\n",
    "gh·∫ø v√† t·ªß. ƒê·ªãnh m·ª©c s·ª≠ d·ª•ng lao ƒë·ªông, chi ph√≠ s·∫£n xu·∫•t v√† gi√° b√°n m·ªói s·∫£n ph·∫©m m·ªói lo·∫°i ∆∞·ªõc t√≠nh trong b·∫£ng sau:\n",
    "\n",
    "| C√°c y·∫øu t·ªë   |  B√†n |  Gh·∫ø | T·ªß |\n",
    "|---|---|---|---|\n",
    "| Lao ƒë·ªông (ng√†y c√¥ng)   |  2 | 1  | 3  |\n",
    "| Chi ph√≠ s·∫£n xu·∫•t (ng√†n ƒë·ªìng)   | 100 | 40  | 250  |\n",
    "| Gi√° b√°n (ng√†n ƒë·ªìng)   | 260  | 120  |  600 |\n",
    "\n",
    "B√†i to√°n ƒë·∫∑t ra l√† x√°c ƒë·ªãnh s·ªë s·∫£n ph·∫©m m·ªói lo·∫°i c·∫ßn ph·∫£i s·∫£n xu·∫•t sao cho kh√¥ng b·ªã ƒë·ªông trong s·∫£n xu·∫•t v√† t·ªïng doanh thu ƒë·∫°t ƒë∆∞·ª£c cao nh·∫•t, bi·∫øt r·∫±ng c∆° s·ªü c√≥ s·ªë lao ƒë·ªông t∆∞∆°ng ƒë∆∞∆°ng v·ªõi 500 ng√†y c√¥ng, s·ªë ti·ªÅn d√†nh cho chi ph√≠ s·∫£n xu·∫•t l√† 40 tri·ªáu ƒë·ªìng v√† s·ªë b√†n, gh·∫ø ph·∫£i theo t·ªâ l·ªá 1/6.\n",
    "\n",
    "1.  H√£y l·∫≠p m√¥ h√¨nh t·ªëi ∆∞u c·ªßa b√†i to√°n tr√™n theo d·∫°ng d∆∞·ªõi ƒë√¢y b·∫±ng c√°ch ch·ªâ ra $f_0, A, b$ v√† $h$. \n",
    "\\begin{align}\n",
    "\\text{minimize}\\quad & f_0(x),\\\\\n",
    "\\text{subject to}\\quad & Ax\\leq b,\\\\\n",
    "& h(x)=0.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "2.  S·ª≠ d·ª•ng `scipy.optimize.linprog` ƒë·ªÉ gi·∫£i b√†i to√°n tr√™n.\n",
    "\n",
    "\n",
    "3.  S·ª≠ d·ª•ng `cvxpy` ƒë·ªÉ gi·∫£i b√†i to√°n tr√™n.\n",
    "\n",
    "*L∆∞u √Ω: Ch·∫•p nh·∫≠n s·ªë l∆∞·ª£ng b√†n gh·∫ø l√† s·ªë th·ª±c.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒë·∫∑t x1 l√† b√†n, x2 l√† gh·∫ø, x3 l√† t·ªß</br>\n",
    "L·∫•y gi√° b√°n tr·ª´ ƒëi tri ph√≠ s·∫£n xu·∫•t, ta ƒë∆∞·ª£c s·ªë ti·ªÅn l·ªùi</br>\n",
    "==>ƒë·ªÉ ƒë·∫°t l·ª£i nhu·∫≠n cao nh·∫•t, ta t√¨m max 160x1 + 80x2 + 360x3</br>\n",
    "> f0 = max 160x1 + 80x2 + 360x3</br>\n",
    "> sb to : 2x1 + 1x2 + 3x3 <= 500</br>\n",
    ">         100x1 + 40x2 + 250x3 <= 40000</br>\n",
    "> h(x) :     6x1 - x2 = 0 </br>\n",
    " >x1,x2,x3 > 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cau 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# C√°c h·ªá s·ªë c·ªßa h√†m m·ª•c ti√™u (doanh thu), c·∫ßn t·ªëi thi·ªÉu h√≥a n√™n ta nh√¢n v·ªõi -1\n",
    "c = np.array([-260, -120, -600])\n",
    "\n",
    "# R√†ng bu·ªôc b·∫•t ƒë·∫≥ng th·ª©c Ax <= b\n",
    "A_ub = np.array([\n",
    "    [2, 1, 3],          # Lao ƒë·ªông\n",
    "    [100, 40, 250]      # Chi ph√≠ s·∫£n xu·∫•t\n",
    "])\n",
    "b_ub = np.array([500, 40000])\n",
    "\n",
    "# R√†ng bu·ªôc ƒë·∫≥ng th·ª©c A_eq*x = b_eq\n",
    "A_eq = np.array([\n",
    "    [6, -1, 0]          # T·ª∑ l·ªá b√†n/gh·∫ø\n",
    "])\n",
    "b_eq = np.array([0])\n",
    "\n",
    "# R√†ng bu·ªôc v·ªÅ bi·∫øn: s·ªë l∆∞·ª£ng s·∫£n ph·∫©m kh√¥ng √¢m\n",
    "bounds = [(0, None), (0, None), (0, None)]\n",
    "\n",
    "# Gi·∫£i b√†i to√°n\n",
    "result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n",
    "\n",
    "if result.success:\n",
    "    x_opt = result.x\n",
    "    total_revenue = -result.fun\n",
    "    print(f\"S·ªë l∆∞·ª£ng s·∫£n ph·∫©m t·ªëi ∆∞u:\")\n",
    "    print(f\"  B√†n (x1): {x_opt[0]:.2f}\")\n",
    "    print(f\"  Gh·∫ø (x2): {x_opt[1]:.2f}\")\n",
    "    print(f\"  T·ªß (x3): {x_opt[2]:.2f}\")\n",
    "    print(f\"Doanh thu t·ªëi ƒëa: {total_revenue:.2f} ng√†n ƒë·ªìng\")\n",
    "else:\n",
    "    print(\"B√†i to√°n kh√¥ng t√¨m th·∫•y l·ªùi gi·∫£i t·ªëi ∆∞u.\")\n",
    "    print(result.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cau 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Khai b√°o c√°c bi·∫øn\n",
    "x = cp.Variable(3, nonneg=True)\n",
    "\n",
    "# H√†m m·ª•c ti√™u (t·ªëi ƒëa h√≥a doanh thu)\n",
    "objective = cp.Maximize(260*x[0] + 120*x[1] + 600*x[2])\n",
    "\n",
    "# C√°c r√†ng bu·ªôc\n",
    "constraints = [\n",
    "    2*x[0] + 1*x[1] + 3*x[2] <= 500,       # Lao ƒë·ªông\n",
    "    100*x[0] + 40*x[1] + 250*x[2] <= 40000, # Chi ph√≠\n",
    "    6*x[0] - x[1] == 0                    # T·ª∑ l·ªá\n",
    "]\n",
    "\n",
    "# T·∫°o v√† gi·∫£i b√†i to√°n\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "if problem.status == cp.OPTIMAL:\n",
    "    print(f\"S·ªë l∆∞·ª£ng s·∫£n ph·∫©m t·ªëi ∆∞u:\")\n",
    "    print(f\"  B√†n (x1): {x.value[0]:.2f}\")\n",
    "    print(f\"  Gh·∫ø (x2): {x.value[1]:.2f}\")\n",
    "    print(f\"  T·ªß (x3): {x.value[2]:.2f}\")\n",
    "    print(f\"Doanh thu t·ªëi ƒëa: {problem.value:.2f} ng√†n ƒë·ªìng\")\n",
    "else:\n",
    "    print(\"B√†i to√°n kh√¥ng t√¨m th·∫•y l·ªùi gi·∫£i t·ªëi ∆∞u.\")\n",
    "    print(problem.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4\n",
    "import pulp as p\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o b√†i to√°n\n",
    "# LpMinimize ƒë·ªÉ gi·∫£i b√†i to√°n t·ªëi thi·ªÉu h√≥a\n",
    "# LpMaximize ƒë·ªÉ gi·∫£i b√†i to√°n t·ªëi ƒëa h√≥a\n",
    "# ·ªû ƒë√¢y ta t·ªëi ƒëa h√≥a doanh thu n√™n ta d√πng LpMaximize\n",
    "problem = p.LpProblem(\"Production_Optimization\", p.LpMaximize)\n",
    "\n",
    "# 2. Khai b√°o c√°c bi·∫øn quy·∫øt ƒë·ªãnh\n",
    "# cat='Continuous' ƒë·ªÉ ch·∫•p nh·∫≠n s·ªë th·ª±c, non-negative=True ƒë·ªÉ ƒë·∫£m b·∫£o >= 0\n",
    "x1 = p.LpVariable(\"ban\", lowBound=0, cat='Continuous')\n",
    "x2 = p.LpVariable(\"ghe\", lowBound=0, cat='Continuous')\n",
    "x3 = p.LpVariable(\"tu\", lowBound=0, cat='Continuous')\n",
    "\n",
    "# 3. Th√™m h√†m m·ª•c ti√™u v√†o b√†i to√°n\n",
    "# Doanh thu = 260*x1 + 120*x2 + 600*x3\n",
    "problem += 260 * x1 + 120 * x2 + 600 * x3, \"Doanh thu\"\n",
    "\n",
    "# 4. Th√™m c√°c r√†ng bu·ªôc\n",
    "# R√†ng bu·ªôc v·ªÅ lao ƒë·ªông (<= 500 ng√†y c√¥ng)\n",
    "problem += 2 * x1 + x2 + 3 * x3 <= 500, \"Lao dong\"\n",
    "\n",
    "# R√†ng bu·ªôc v·ªÅ chi ph√≠ s·∫£n xu·∫•t (<= 40 tri·ªáu ƒë·ªìng = 40000 ng√†n ƒë·ªìng)\n",
    "problem += 100 * x1 + 40 * x2 + 250 * x3 <= 40000, \"Chi phi san xuat\"\n",
    "\n",
    "# R√†ng bu·ªôc v·ªÅ t·ª∑ l·ªá (b√†n/gh·∫ø = 1/6)\n",
    "problem += 6 * x1 == x2, \"Ty le ban_ghe\"\n",
    "\n",
    "# 5. Gi·∫£i b√†i to√°n\n",
    "problem.solve()\n",
    "\n",
    "# 6. In k·∫øt qu·∫£\n",
    "print(f\"Tr·∫°ng th√°i gi·∫£i quy·∫øt: {p.LpStatus[problem.status]}\")\n",
    "print(\"S·ªë l∆∞·ª£ng s·∫£n ph·∫©m t·ªëi ∆∞u:\")\n",
    "print(f\"  B√†n (x1): {x1.value():.2f}\")\n",
    "print(f\"  Gh·∫ø (x2): {x2.value():.2f}\")\n",
    "print(f\"  T·ªß (x3): {x3.value():.2f}\")\n",
    "print(f\"Doanh thu t·ªëi ƒëa: {p.value(problem.objective):.2f} ng√†n ƒë·ªìng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> k·∫øt lu·∫≠n, c·∫£ 2 c√¢u 1.2 v√† 1.3 ƒë·ªÅu ƒë∆∞a ra k·∫øt qu·∫£ l√† 5 b√†n 30 gh·∫ø v√† 153 t·ªß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLqKbo8CeQy1"
   },
   "source": [
    "### C√¢u 2 (4 ƒëi·ªÉm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W00lHNXQfg15"
   },
   "source": [
    "1. Cho m·ªôt h√†m s·ªë m·ª•c ti√™u $f(x)=x^2+\\sin \\left(\\dfrac{x}{2}\\right)+2023x.$\n",
    "H√£y t√≠nh ƒë·∫°o h√†m c·ªßa h√†m s·ªë n√†y.\n",
    "\n",
    "\n",
    "2. S·ª≠ d·ª•ng thu·∫≠t to√°n Gradient Descent, h√£y t√¨m gi√° tr·ªã c·ªßa $x$ ƒë·ªÉ t·ªëi ∆∞u h√≥a h√†m s·ªë $f(x)=x^2+\\sin \\left(\\dfrac{x}{2}\\right)+2023x,$ v·ªõi gi√° tr·ªã $x$ ban ƒë·∫ßu l√† $x^{(0)}=2024$, s·ª≠ d·ª•ng learning rate l√† $0.9$ v√† th·ª±c hi·ªán t·ªëi ƒëa $100$ v√≤ng l·∫∑p. In ra gi√° tr·ªã c·ªßa $x$ v√† $f(x)$ t∆∞∆°ng ·ª©ng sau m·ªói v√≤ng l·∫∑p.\n",
    "\n",
    "\n",
    "3. B·∫±ng c√°ch thay ƒë·ªïi gi√° tr·ªã learning rate trong thu·∫≠t to√°n tr√™n, h√£y gi·∫£i th√≠ch s·ª± quan tr·ªçng c·ªßa vi·ªác ƒëi·ªÅu ch·ªânh tham s·ªë learning rate trong thu·∫≠t to√°n Gradient Descent v√† c√°ch l·ª±a ch·ªçn gi√° tr·ªã t·ªëi ∆∞u c·ªßa n√≥.\n",
    "\n",
    "\n",
    "4. H√£y tr√¨nh b√†y m·ªôt v√≠ d·ª• c·ª• th·ªÉ v·ªÅ m·ªôt v·∫•n ƒë·ªÅ th·ª±c t·∫ø trong h·ªçc m√°y ho·∫∑c khoa h·ªçc d·ªØ li·ªáu m√† Gradient Descent c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i quy·∫øt.\n",
    "\n",
    "Code m·∫´u (c√¢u 2.1, 2.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvals\n",
    "import sympy as sp\n",
    "# --- 1Ô∏è‚É£ Khai b√°o bi·∫øn v√† h√†m ---\n",
    "x = sp.Symbol('x', real=True)\n",
    "f = x**2 + sp.sin(x/2) + 2023*x\n",
    "\n",
    "# --- 2Ô∏è‚É£ T√≠nh ƒë·∫°o h√†m v√† Hessian ---\n",
    "grad_f = sp.diff(f, x)\n",
    "hessian_f = sp.diff(grad_f, x)\n",
    "\n",
    "print(\"H√†m f(x) =\", f)\n",
    "print(\"ƒê·∫°o h√†m f'(x) =\", grad_f)\n",
    "print(\"Hessian f''(x) =\", hessian_f)\n",
    "\n",
    "# --- 3Ô∏è‚É£ Chuy·ªÉn sang h√†m s·ªë ƒë·ªÉ t√≠nh to√°n ---\n",
    "f_func = sp.lambdify(x, f, 'numpy')\n",
    "hessian_func = sp.lambdify(x, hessian_f, 'numpy')\n",
    "\n",
    "# --- 4Ô∏è‚É£ Ki·ªÉm tra Hessian tr√™n nhi·ªÅu ƒëi·ªÉm ---\n",
    "x_vals = np.linspace(-50, 50, 200)\n",
    "H_vals = hessian_func(x_vals)\n",
    "\n",
    "# --- 5Ô∏è‚É£ Ki·ªÉm tra t√≠nh x√°c ƒë·ªãnh d∆∞∆°ng ---\n",
    "min_H = np.min(H_vals)\n",
    "max_H = np.max(H_vals)\n",
    "\n",
    "print(\"\\n--- KI·ªÇM TRA B·∫∞NG HESSIAN ---\")\n",
    "print(f\"Gi√° tr·ªã nh·ªè nh·∫•t c·ªßa Hessian f''(x): {min_H:.4f}\")\n",
    "print(f\"Gi√° tr·ªã l·ªõn nh·∫•t c·ªßa Hessian f''(x): {max_H:.4f}\")\n",
    "\n",
    "if np.all(H_vals > 0):\n",
    "    print(\"‚úÖ Hessian lu√¥n d∆∞∆°ng ‚Üí H√†m l·ªìi nghi√™m ng·∫∑t (strictly convex).\")\n",
    "elif np.all(H_vals >= 0):\n",
    "    print(\"‚úÖ Hessian kh√¥ng √¢m ‚Üí H√†m l·ªìi (convex).\")\n",
    "else:\n",
    "    print(\"‚ùå Hessian ƒë·ªïi d·∫•u ‚Üí H√†m KH√îNG l·ªìi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üßÆ H√ÄM 2 CHI·ªÄU: f(x1, x2) = x1^2 + 3x2^2 + 2x1x2 + sin((x1+x2)/2)\n",
    "# ============================================================\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "from scipy.linalg import eigvals\n",
    "\n",
    "# --- 1Ô∏è‚É£ Khai b√°o bi·∫øn v√† h√†m ---\n",
    "x1, x2 = sp.symbols('x1 x2', real=True)\n",
    "f2 = x1**2 + 3*x2**2 + 2*x1*x2 + sp.sin((x1 + x2)/2)\n",
    "\n",
    "# --- 2Ô∏è‚É£ Gradient v√† Hessian ---\n",
    "grad_f2 = [sp.diff(f2, var) for var in (x1, x2)]\n",
    "H_f2 = sp.hessian(f2, (x1, x2))\n",
    "\n",
    "print(\"H√†m f(x1, x2) =\", f2)\n",
    "print(\"Gradient =\", grad_f2)\n",
    "print(\"Hessian =\")\n",
    "sp.pprint(H_f2)\n",
    "\n",
    "# --- 3Ô∏è‚É£ H√†m t√≠nh gi√° tr·ªã Hessian s·ªë ---\n",
    "H_func_2d = sp.lambdify((x1, x2), H_f2, 'numpy')\n",
    "\n",
    "# --- 4Ô∏è‚É£ Ki·ªÉm tra Hessian t·∫°i nhi·ªÅu ƒëi·ªÉm ---\n",
    "test_points = [(0,0), (1,1), (-2,3), (5,-1)]\n",
    "print(\"\\n--- KI·ªÇM TRA HESSIAN 2D ---\")\n",
    "for pt in test_points:\n",
    "    H_val = np.array(H_func_2d(*pt), dtype=float)\n",
    "    eigs = np.real(eigvals(H_val))\n",
    "    print(f\"ƒêi·ªÉm {pt}: eigenvalues = {eigs}\")\n",
    "    if np.all(eigs > 0):\n",
    "        print(\"‚Üí ‚úÖ Hessian x√°c ƒë·ªãnh d∆∞∆°ng ‚Üí H√†m l·ªìi nghi√™m ng·∫∑t t·∫°i ƒëi·ªÉm n√†y\\n\")\n",
    "    elif np.all(eigs >= 0):\n",
    "        print(\"‚Üí ‚úÖ Hessian b√°n x√°c ƒë·ªãnh d∆∞∆°ng ‚Üí H√†m l·ªìi t·∫°i ƒëi·ªÉm n√†y\\n\")\n",
    "    else:\n",
    "        print(\"‚Üí ‚ùå Hessian c√≥ gi√° tr·ªã ri√™ng √¢m ‚Üí KH√îNG l·ªìi t·∫°i ƒëi·ªÉm n√†y\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üßÆ H√ÄM 3 CHI·ªÄU: f(x1, x2, x3) = x1^2 + x2^2 + x3^2 + x1x2 + 2x2x3 + sin((x1+x2+x3)/3)\n",
    "# ============================================================\n",
    "x1, x2, x3 = sp.symbols('x1 x2 x3', real=True)\n",
    "f3 = x1**2 + x2**2 + x3**2 + x1*x2 + 2*x2*x3 + sp.sin((x1 + x2 + x3)/3)\n",
    "\n",
    "# --- Gradient & Hessian ---\n",
    "grad_f3 = [sp.diff(f3, v) for v in (x1, x2, x3)]\n",
    "H_f3 = sp.hessian(f3, (x1, x2, x3))\n",
    "\n",
    "print(\"H√†m f(x1, x2, x3) =\", f3)\n",
    "print(\"Gradient =\", grad_f3)\n",
    "print(\"Hessian =\")\n",
    "sp.pprint(H_f3)\n",
    "\n",
    "# --- H√†m t√≠nh Hessian ---\n",
    "H_func_3d = sp.lambdify((x1, x2, x3), H_f3, 'numpy')\n",
    "\n",
    "# --- Ki·ªÉm tra Hessian t·∫°i nhi·ªÅu ƒëi·ªÉm ---\n",
    "test_points_3d = [(0,0,0), (1,1,1), (-2,1,3), (3,-1,0)]\n",
    "print(\"\\n--- KI·ªÇM TRA HESSIAN 3D ---\")\n",
    "for pt in test_points_3d:\n",
    "    H_val = np.array(H_func_3d(*pt), dtype=float)\n",
    "    eigs = np.real(eigvals(H_val))\n",
    "    print(f\"ƒêi·ªÉm {pt}: eigenvalues = {np.round(eigs, 4)}\")\n",
    "    if np.all(eigs > 0):\n",
    "        print(\"‚Üí ‚úÖ Hessian x√°c ƒë·ªãnh d∆∞∆°ng ‚Üí H√†m l·ªìi nghi√™m ng·∫∑t t·∫°i ƒëi·ªÉm n√†y\\n\")\n",
    "    elif np.all(eigs >= 0):\n",
    "        print(\"‚Üí ‚úÖ Hessian b√°n x√°c ƒë·ªãnh d∆∞∆°ng ‚Üí H√†m l·ªìi t·∫°i ƒëi·ªÉm n√†y\\n\")\n",
    "    else:\n",
    "        print(\"‚Üí ‚ùå Hessian c√≥ gi√° tr·ªã ri√™ng √¢m ‚Üí KH√îNG l·ªìi t·∫°i ƒëi·ªÉm n√†y\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 ƒë·∫°o h√†m c·ªßa x*x l√† 2x, ƒë·∫°o h√†m c·ªßa sin(x/2) =1/2*cos(x/2) ,ƒë·∫°o h√†m c·ªßa 2023x l√† 2023</br>\n",
    "v·∫≠y ƒë·∫°o h√†m c·ªßa f(x) l√† 2x + 1/2 cos(x/2) + 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# H√†m s·ªë m·ª•c ti√™u\n",
    "def objective_function(x):\n",
    "    of = x*x + math.sin(x/2) + 2023*x\n",
    "    return of\n",
    "\n",
    "# ƒê·∫°o h√†m c·ªßa h√†m s·ªë m·ª•c ti√™u\n",
    "def gradient(x):\n",
    "\n",
    "    g = 2*x + 1/2*math.cos(x/2) + 2023\n",
    "    return g\n",
    "\n",
    "def gradient_descent(learning_rate, max_iterations, initial_x):\n",
    "    x = initial_x\n",
    "    for i in range(max_iterations):\n",
    "        x = x - learning_rate * gradient(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "learning_rate = 0.9\n",
    "max_iterations = 100\n",
    "initial_x = 2024\n",
    "\n",
    "final_x = gradient_descent(learning_rate, max_iterations, initial_x)\n",
    "print(\"Gi√° tr·ªã x cu·ªëi c√πng sau Gradient Descent:\", final_x)\n",
    "print(\"Gi√° tr·ªã f(x) t∆∞∆°ng ·ª©ng:\", objective_function(final_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 thay ƒë·ªïi learning rate\n",
    "learning_rate = np.arange(0,1,0.01)\n",
    "fX =[]\n",
    "Gx = []\n",
    "for t in learning_rate:\n",
    "    final_x = gradient_descent(t, max_iterations, initial_x)\n",
    "    fX.append(final_x)\n",
    "    Gx.append(objective_function(final_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(learning_rate, Gx)\n",
    "plt.title('tuong quan cua learning rate va gia tri toi uu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nh·∫≠n x√©t, khi c√†ng tƒÉng learning rate th√¨ gi√° tr·ªã t·ªëi uu c√†ng gi·∫£m, nh∆∞ng khi g·∫ßn ƒë·∫°t ƒë·∫øn 1 th√¨ gi√° tr·ªã t·ªëi u∆∞ t·∫°i tƒÉng l√™n, ch·ª©ng t·ªè gi√° tr·ªã t·ªëi ∆∞u c·ªßa b√†i n√†y n·∫±m kho·∫£m 0 t·ªõi 0,95, n·∫øu ch·ªçn qu√° th·∫•p th√¨ gi√° tr·ªã t·ªëi ∆∞u s·∫Ω qu√° l·ªõn v√¨ 100 v√≤ng l·∫≠p ko ƒë·ªß ƒë·ªÉ ch·∫°y t·ªõi gi√° tr·ªã t·ªëi ∆∞u. v·∫≠y nen ta c·∫ßn ch·ªçn learningrate h·ª£p l√Ω, ko qu√° l·ªõn c≈©ng ko qu√° nh·ªè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. V√≠ d·ª• th·ª±c t·∫ø trong khoa h·ªçc d·ªØ li·ªáu c·∫ßn d√πng gd, t√≠nh gi√° tr·ªã t·ªëi ∆∞u trong vi·ªác truy·ªÅn tin c·ªßa h√†m entropy ƒë∆°n gi·∫£n h∆°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. ƒê·ªãnh nghƒ©a h√†m m·ª•c ti√™u v√† ƒë·∫°o h√†m ---\n",
    "# f(x) = x^2 + sin(x/2) + 2023x\n",
    "def f(x):\n",
    "    return x**2 + np.sin(x/2) + 2023*x\n",
    "\n",
    "# ƒê·∫°o h√†m c·ªßa f(x)\n",
    "# f'(x) = 2x + (1/2)cos(x/2) + 2023\n",
    "def gradient_f(x):\n",
    "    return 2*x + 0.5 * np.cos(x/2) + 2023\n",
    "\n",
    "# --- 2. C√†i ƒë·∫∑t thu·∫≠t to√°n Gradient Descent ---\n",
    "# Tham s·ªë ƒë·∫ßu v√†o\n",
    "x_start = 2024.0\n",
    "learning_rate = 0.9\n",
    "max_iterations = 100\n",
    "\n",
    "x_current = x_start\n",
    "print(f\"Gi√° tr·ªã kh·ªüi ƒë·∫ßu: x_0 = {x_start}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# V√≤ng l·∫∑p Gradient Descent\n",
    "for k in range(max_iterations):\n",
    "    # T√≠nh gradient t·∫°i ƒëi·ªÉm hi·ªán t·∫°i\n",
    "    grad = gradient_f(x_current)\n",
    "    \n",
    "    # C·∫≠p nh·∫≠t gi√° tr·ªã x m·ªõi theo c√¥ng th·ª©c\n",
    "    x_current = x_current - learning_rate * grad\n",
    "    \n",
    "    # In k·∫øt qu·∫£ sau m·ªói 10 v√≤ng l·∫∑p ƒë·ªÉ theo d√µi\n",
    "    if (k + 1) % 10 == 0:\n",
    "        current_f_val = f(x_current)\n",
    "        print(f\"V√≤ng l·∫∑p {k+1}: x = {x_current:.6f}, f(x) = {current_f_val:.6f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"K·∫øt qu·∫£ sau {max_iterations} v√≤ng l·∫∑p:\")\n",
    "print(f\"  Gi√° tr·ªã x cu·ªëi c√πng: {x_current:.6f}\")\n",
    "print(f\"  Gi√° tr·ªã f(x) t∆∞∆°ng ·ª©ng: {f(x_current):.6f}\")\n",
    "\n",
    "# --- 3. Gi·∫£i th√≠ch v·ªÅ Learning Rate ---\n",
    "print(\"\\n--- 3. Gi·∫£i th√≠ch t·∫ßm quan tr·ªçng c·ªßa Learning Rate ---\")\n",
    "print(\"Learning Rate (t·ªëc ƒë·ªô h·ªçc) l√† m·ªôt tham s·ªë c·ª±c k·ª≥ quan tr·ªçng trong Gradient Descent, quy·∫øt ƒë·ªãnh ƒë·ªô d√†i c·ªßa m·ªói b∆∞·ªõc ƒëi khi di chuy·ªÉn v·ªÅ ph√≠a c·ª±c ti·ªÉu.\")\n",
    "print(\"- N·∫øu Learning Rate qu√° l·ªõn: Thu·∫≠t to√°n c√≥ th·ªÉ 'nh·∫£y' qua ƒëi·ªÉm t·ªëi ∆∞u, kh√¥ng bao gi·ªù h·ªôi t·ª•, ho·∫∑c th·∫≠m ch√≠ ph√¢n k·ª≥ (ƒëi ra xa v√¥ c√πng).\")\n",
    "print(\"- N·∫øu Learning Rate qu√° nh·ªè: Thu·∫≠t to√°n s·∫Ω h·ªôi t·ª• r·∫•t ch·∫≠m, c·∫ßn r·∫•t nhi·ªÅu v√≤ng l·∫∑p ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët.\")\n",
    "print(\"Vi·ªác l·ª±a ch·ªçn Learning Rate t·ªëi ∆∞u th∆∞·ªùng l√† m·ªôt qu√° tr√¨nh th·ª≠ nghi·ªám. C√≥ c√°c ph∆∞∆°ng ph√°p n√¢ng cao h∆°n nh∆∞ Adam, RMSprop gi√∫p t·ª± ƒëi·ªÅu ch·ªânh Learning Rate trong qu√° tr√¨nh hu·∫•n luy·ªán.\")\n",
    "\n",
    "# --- 4. V√≠ d·ª• th·ª±c t·∫ø v·ªÅ ·ª©ng d·ª•ng ---\n",
    "print(\"\\n--- 4. V√≠ d·ª• th·ª±c t·∫ø v·ªÅ ·ª©ng d·ª•ng c·ªßa Gradient Descent ---\")\n",
    "print(\"Gradient Descent ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong h·ªçc m√°y v√† khoa h·ªçc d·ªØ li·ªáu. M·ªôt v√≠ d·ª• ƒëi·ªÉn h√¨nh l√† trong:\")\n",
    "print(\"- H·ªìi quy tuy·∫øn t√≠nh (Linear Regression): Thu·∫≠t to√°n n√†y gi√∫p t√¨m c√°c h·ªá s·ªë t·ªët nh·∫•t cho ƒë∆∞·ªùng h·ªìi quy sao cho t·ªïng sai s·ªë b√¨nh ph∆∞∆°ng gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu v√† ƒë∆∞·ªùng th·∫≥ng l√† nh·ªè nh·∫•t.\")\n",
    "print(\"- Hu·∫•n luy·ªán m·∫°ng n∆°-ron (Neural Networks): Gradient Descent (c√πng v·ªõi thu·∫≠t to√°n lan truy·ªÅn ng∆∞·ª£c - backpropagation) ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√°c tr·ªçng s·ªë c·ªßa m·∫°ng, gi√∫p m·∫°ng h·ªçc v√† ƒë∆∞a ra d·ª± ƒëo√°n ch√≠nh x√°c h∆°n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUI LAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. ƒê·ªãnh nghƒ©a h√†m m·ª•c ti√™u v√† ƒë·∫°o h√†m ---\n",
    "def f(x):\n",
    "    return x**2 + np.sin(x/2) + 2023*x\n",
    "\n",
    "def df_dx(x):\n",
    "    return 2*x + 0.5 * np.cos(x/2) + 2023\n",
    "\n",
    "# --- 2. Thu·∫≠t to√°n Gradient Descent ---\n",
    "def gradient_descent_1d(f, df_dx, x_init, learning_rate, num_iterations, tolerance=1e-8):\n",
    "    x = x_init\n",
    "    history = {'x': [x], 'f': [f(x)], 'gradient': [df_dx(x)], 'step_size': []}\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = df_dx(x)\n",
    "        if abs(gradient) < tolerance:\n",
    "            print(f\"H·ªôi t·ª• t·∫°i v√≤ng l·∫∑p {i+1}, gradient = {gradient:.2e}\")\n",
    "            break\n",
    "            \n",
    "        x_new = x - learning_rate * gradient\n",
    "        step_size = abs(x_new - x)\n",
    "        x = x_new\n",
    "        \n",
    "        history['x'].append(x)\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(gradient)\n",
    "        history['step_size'].append(step_size)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# --- 3. Th·ª±c thi b√†i to√°n ---\n",
    "x_initial = 2024.0\n",
    "alpha = 0.9\n",
    "iterations = 100\n",
    "\n",
    "history = gradient_descent_1d(f, df_dx, x_initial, alpha, iterations)\n",
    "\n",
    "x_opt = history['x'][-1]  # nghi·ªám g·∫ßn ƒë√∫ng\n",
    "errors = [abs(x - x_opt) for x in history['x']]  # sai s·ªë t·∫°i m·ªói v√≤ng\n",
    "\n",
    "# --- In k·∫øt qu·∫£ ---\n",
    "print(f\"--- K·∫øt qu·∫£ sau {iterations} v√≤ng l·∫∑p ---\")\n",
    "print(f\"x cu·ªëi c√πng: {x_opt:.6f}\")\n",
    "print(f\"f(x cu·ªëi c√πng): {f(x_opt):.6f}\")\n",
    "\n",
    "# --- 4. V·∫Ω ƒë·ªì th·ªã ---\n",
    "iteration_indices = range(len(history['x']))\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# (1) f(x) theo s·ªë v√≤ng l·∫∑p\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(iteration_indices, history['f'], marker='o', color='tab:blue')\n",
    "plt.title('Gi√° tr·ªã h√†m s·ªë f(x) qua c√°c v√≤ng l·∫∑p', fontsize=14)\n",
    "plt.xlabel('S·ªë v√≤ng l·∫∑p')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "\n",
    "# (2) x theo s·ªë v√≤ng l·∫∑p\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(iteration_indices, history['x'], marker='o', color='tab:orange')\n",
    "plt.title('Gi√° tr·ªã x qua c√°c v√≤ng l·∫∑p', fontsize=14)\n",
    "plt.xlabel('S·ªë v√≤ng l·∫∑p')\n",
    "plt.ylabel('x')\n",
    "plt.grid(True)\n",
    "\n",
    "# (3) Sai s·ªë |x - x*| theo s·ªë v√≤ng l·∫∑p\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(iteration_indices, errors, marker='o', color='tab:red')\n",
    "plt.title('Sai s·ªë |x - x*| qua c√°c v√≤ng l·∫∑p', fontsize=14)\n",
    "plt.xlabel('S·ªë v√≤ng l·∫∑p')\n",
    "plt.ylabel('Sai s·ªë')\n",
    "plt.yscale('log')  # d√πng log scale ƒë·ªÉ d·ªÖ nh√¨n s·ª± gi·∫£m sai s·ªë\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Th√¥ng tin th√™m ---\n",
    "print(\"\\n--- Chi ti·∫øt h·ªôi t·ª• ---\")\n",
    "print(f\"x_0 = {history['x'][0]:.6f}, f(x_0) = {history['f'][0]:.6f}\")\n",
    "print(f\"x_10 = {history['x'][10]:.6f}, f(x_10) = {history['f'][10]:.6f}\")\n",
    "print(f\"x_50 = {history['x'][50]:.6f}, f(x_50) = {history['f'][50]:.6f}\")\n",
    "print(f\"x_cuoi = {x_opt:.6f}, f(x_cuoi) = {f(x_opt):.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3Ô∏è‚É£ Th·ª≠ nghi·ªám v·ªõi c√°c learning rate kh√°c nhau ---\n",
    "x_initial = 2024.0\n",
    "iterations = 50\n",
    "learning_rates = [0.001, 0.05, 1.0]\n",
    "histories = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    histories[lr] = gradient_descent_1d(f, df_dx, x_initial, lr, iterations)\n",
    "\n",
    "# --- 4Ô∏è‚É£ V·∫Ω ƒë·ªì th·ªã so s√°nh ---\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# a) Gi√° tr·ªã h√†m f(x) theo s·ªë v√≤ng l·∫∑p\n",
    "plt.subplot(1, 2, 1)\n",
    "for lr, hist in histories.items():\n",
    "    plt.plot(hist['f'], marker='o', label=f'Œ∑ = {lr}')\n",
    "plt.title('Gi√° tr·ªã f(x) qua c√°c v√≤ng l·∫∑p', fontsize=13)\n",
    "plt.xlabel('S·ªë v√≤ng l·∫∑p')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# b) Gi√° tr·ªã x qua c√°c v√≤ng l·∫∑p\n",
    "plt.subplot(1, 2, 2)\n",
    "for lr, hist in histories.items():\n",
    "    plt.plot(hist['x'], marker='o', label=f'Œ∑ = {lr}')\n",
    "plt.title('Gi√° tr·ªã x qua c√°c v√≤ng l·∫∑p', fontsize=13)\n",
    "plt.xlabel('S·ªë v√≤ng l·∫∑p')\n",
    "plt.ylabel('x')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 5Ô∏è‚É£ Ph√¢n t√≠ch k·∫øt qu·∫£ ---\n",
    "print(\"\\n--- PH√ÇN T√çCH ·∫¢NH H∆Ø·ªûNG C·ª¶A LEARNING RATE ---\")\n",
    "for lr, hist in histories.items():\n",
    "    x_final = hist['x'][-1]\n",
    "    f_final = hist['f'][-1]\n",
    "    print(f\"Œ∑ = {lr:<6} ‚Üí x_cu·ªëi ‚âà {x_final:.6f}, f(x_cu·ªëi) ‚âà {f_final:.6f}\")\n",
    "\n",
    "print(\"\\n--- GI·∫¢I TH√çCH CHI TI·∫æT ---\")\n",
    "print(\"üîπ Khi Œ∑ = 0.001 (r·∫•t nh·ªè):\")\n",
    "print(\"   - C√°c b∆∞·ªõc c·∫≠p nh·∫≠t c·ª±c k·ª≥ nh·ªè, do ƒë√≥ thu·∫≠t to√°n h·ªôi t·ª• ch·∫≠m.\")\n",
    "print(\"   - f(x) gi·∫£m d·∫ßn r·∫•t ch·∫≠m, c·∫ßn nhi·ªÅu v√≤ng l·∫∑p m·ªõi t·ªõi g·∫ßn ƒëi·ªÉm t·ªëi ∆∞u.\")\n",
    "\n",
    "print(\"üîπ Khi Œ∑ = 0.05 (v·ª´a ph·∫£i):\")\n",
    "print(\"   - C√°c b∆∞·ªõc di chuy·ªÉn h·ª£p l√Ω, gi√∫p thu·∫≠t to√°n h·ªôi t·ª• nhanh v√† ·ªïn ƒë·ªãnh.\")\n",
    "print(\"   - ƒê√¢y l√† v√πng learning rate ‚Äòt·ªët‚Äô, ƒë·∫£m b·∫£o t·ªëc ƒë·ªô v√† ·ªïn ƒë·ªãnh.\")\n",
    "\n",
    "print(\"üîπ Khi Œ∑ = 1.0 (qu√° l·ªõn):\")\n",
    "print(\"   - Thu·∫≠t to√°n di chuy·ªÉn qu√° xa m·ªói v√≤ng, c√≥ th·ªÉ v∆∞·ª£t qua ƒëi·ªÉm t·ªëi ∆∞u.\")\n",
    "print(\"   - D·∫´n ƒë·∫øn dao ƒë·ªông ho·∫∑c ph√¢n k·ª≥ (f(x) tƒÉng l√™n thay v√¨ gi·∫£m xu·ªëng).\")\n",
    "\n",
    "print(\"\\nüí° K·∫æT LU·∫¨N:\")\n",
    "print(\"   - Learning rate l√† tham s·ªë then ch·ªët trong Gradient Descent.\")\n",
    "print(\"   - Œ∑ qu√° nh·ªè ‚Üí ch·∫≠m. Œ∑ qu√° l·ªõn ‚Üí dao ƒë·ªông ho·∫∑c kh√¥ng h·ªôi t·ª•.\")\n",
    "print(\"   - Œ∑ t·ªëi ∆∞u n√™n ƒë∆∞·ª£c ch·ªçn sao cho f(x) gi·∫£m ƒë·ªÅu, kh√¥ng dao ƒë·ªông, th∆∞·ªùng qua th·ª≠ nghi·ªám ho·∫∑c d√πng k·ªπ thu·∫≠t adaptive (Adam, RMSProp).\")\n",
    "# --- 4. V√≠ d·ª• th·ª±c t·∫ø v·ªÅ ·ª©ng d·ª•ng ---\n",
    "print(\"\\n--- 4. V√≠ d·ª• th·ª±c t·∫ø v·ªÅ ·ª©ng d·ª•ng c·ªßa Gradient Descent ---\")\n",
    "print(\"Gradient Descent ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong h·ªçc m√°y v√† khoa h·ªçc d·ªØ li·ªáu. M·ªôt v√≠ d·ª• ƒëi·ªÉn h√¨nh l√† trong:\")\n",
    "print(\"- H·ªìi quy tuy·∫øn t√≠nh (Linear Regression): Thu·∫≠t to√°n n√†y gi√∫p t√¨m c√°c h·ªá s·ªë t·ªët nh·∫•t cho ƒë∆∞·ªùng h·ªìi quy sao cho t·ªïng sai s·ªë b√¨nh ph∆∞∆°ng gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu v√† ƒë∆∞·ªùng th·∫≥ng l√† nh·ªè nh·∫•t.\")\n",
    "print(\"- Hu·∫•n luy·ªán m·∫°ng n∆°-ron (Neural Networks): Gradient Descent (c√πng v·ªõi thu·∫≠t to√°n lan truy·ªÅn ng∆∞·ª£c - backpropagation) ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√°c tr·ªçng s·ªë c·ªßa m·∫°ng, gi√∫p m·∫°ng h·ªçc v√† ƒë∆∞a ra d·ª± ƒëo√°n ch√≠nh x√°c h∆°n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√¢u 3 (3 ƒëi·ªÉm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5_DJ49xgi-x"
   },
   "source": [
    "G·ªçi $\\gamma=\\overline{\\gamma_1\\gamma_2\\gamma_3\\gamma_4\\gamma_5\\gamma_6\\gamma_7\\gamma_8}$ l√† m√£ s·ªë sinh vi√™n c·ªßa anh/ch·ªã. ƒê·∫∑t $\\alpha=\\gamma_7$ v√† $\\beta=\\sum_{i=1}^8\\gamma_i$, x√©t b√†i to√°n t·ªëi ∆∞u sau\n",
    "\\begin{align}\n",
    "\\min_{x=(x_1,x_2)\\in \\mathbb{R}^2} f(x)=\\dfrac{1}{2}x_1^2+\\dfrac{1}{2}x_2^2+(\\alpha+1) x_2-\\beta\\quad (1)\n",
    "\\end{align}\n",
    "\n",
    "1. H√£y t√¨m $\\alpha,\\beta$ v√† gi√° tr·ªã t·ªëi ∆∞u $p^*$ c·ªßa b√†i to√°n (1).\n",
    "\n",
    "2. 2. S·ª≠ d·ª•ng thu·∫≠t to√°n Gradient Descent cho b√†i to√°n (1) v·ªõi learning rate l√† $\\dfrac{1}{\\alpha+1}$ v√† ƒëi·ªÉm kh·ªüi t·∫°o $x^{(0)}=(1,0)$, h√£y t√¨m c√¥ng th·ª©c cho ƒëi·ªÉm c·∫≠p nh·∫≠t $x^{(k)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. H√£y t√¨m $\\alpha,\\beta$ v√† gi√° tr·ªã t·ªëi ∆∞u $p^*$ c·ªßa b√†i to√°n (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. S·ª≠ d·ª•ng thu·∫≠t to√°n Gradient Descent cho b√†i to√°n (1) v·ªõi learning rate l√† $\\dfrac{1}{\\alpha+1}$ v√† ƒëi·ªÉm kh·ªüi t·∫°o $x^{(0)}=(1,0)$, h√£y t√¨m c√¥ng th·ª©c cho ƒëi·ªÉm c·∫≠p nh·∫≠t $x^{(k)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================================\n",
    "# 1Ô∏è‚É£ Th√¥ng tin sinh vi√™n\n",
    "# ====================================\n",
    "gamma = [2, 3, 6, 3, 1, 9, 7, 1]\n",
    "alpha = gamma[6]           # Œ≥7\n",
    "beta = sum(gamma)\n",
    "print(f\"Œ± = {alpha}, Œ≤ = {beta}\")\n",
    "\n",
    "# ====================================\n",
    "# 2Ô∏è‚É£ ƒê·ªãnh nghƒ©a h√†m v√† gradient\n",
    "# ====================================\n",
    "def f(x1, x2, alpha, beta):\n",
    "    return 0.5*x1**2 + 0.5*x2**2 + (alpha+1)*x2 - beta\n",
    "\n",
    "def grad_f(x1, x2, alpha):\n",
    "    return np.array([x1, x2 + (alpha+1)])\n",
    "\n",
    "# ====================================\n",
    "# 3Ô∏è‚É£ Gradient Descent\n",
    "# ====================================\n",
    "\n",
    "eta = 1 / (alpha + 1)\n",
    "print(\"\\n=== C√îNG TH·ª®C C·∫¨P NH·∫¨T (theo Gradient Descent) ===\")\n",
    "print(f\"Learning rate (Œ∑) = 1/(Œ±+1) = 1/{alpha+1} = {eta:.4f}\")\n",
    "print(\"C√¥ng th·ª©c t·ªïng qu√°t:\")\n",
    "print(\"  x‚ÇÅ^(k+1) = x‚ÇÅ^(k) - Œ∑ * x‚ÇÅ^(k) = (1 - Œ∑) * x‚ÇÅ^(k)\")\n",
    "print(\"  x‚ÇÇ^(k+1) = x‚ÇÇ^(k) - Œ∑ * (x‚ÇÇ^(k) + Œ± + 1)\")\n",
    "print(f\"Thay Œ± = {alpha} v√† Œ∑ = {eta:.4f} ta ƒë∆∞·ª£c:\")\n",
    "print(f\"  x‚ÇÅ^(k+1) = (1 - {eta:.4f}) * x‚ÇÅ^(k) = {(1-eta):.4f} * x‚ÇÅ^(k)\")\n",
    "print(f\"  x‚ÇÇ^(k+1) = (1 - {eta:.4f}) * x‚ÇÇ^(k) - Œ∑ * ({alpha+1}) = {(1-eta):.4f} * x‚ÇÇ^(k) - 1\")\n",
    "\n",
    "# ====================================\n",
    "\n",
    "x = np.array([1.0, 0.0])  # x^(0)\n",
    "history = [x.copy()]\n",
    "for k in range(10):       # v√≠ d·ª• 10 v√≤ng\n",
    "    g = grad_f(x[0], x[1], alpha)\n",
    "    x = x - eta * g\n",
    "    history.append(x.copy())\n",
    "\n",
    "# ====================================\n",
    "# 4Ô∏è‚É£ In k·∫øt qu·∫£\n",
    "# ====================================\n",
    "x_star = np.array([0.0, -8.0])\n",
    "p_star = f(x_star[0], x_star[1], alpha, beta)\n",
    "print(f\"\\nƒêi·ªÉm t·ªëi ∆∞u x* = {x_star}\")\n",
    "print(f\"Gi√° tr·ªã t·ªëi ∆∞u p* = {p_star}\")\n",
    "\n",
    "print(\"\\n--- Qu√° tr√¨nh c·∫≠p nh·∫≠t x^(k) ---\")\n",
    "for i, xi in enumerate(history):\n",
    "    print(f\"k={i:2d}: x1={xi[0]:.6f}, x2={xi[1]:.6f}, f(x)={f(xi[0], xi[1], alpha, beta):.6f}\")\n",
    "\n",
    "# ====================================\n",
    "# 5Ô∏è‚É£ V·∫Ω qu·ªπ ƒë·∫°o h·ªôi t·ª•\n",
    "# ====================================\n",
    "history = np.array(history)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history[:,0], label='x1')\n",
    "plt.plot(history[:,1], label='x2')\n",
    "plt.title('Gi√° tr·ªã x1, x2 qua c√°c v√≤ng l·∫∑p (Gradient Descent)')\n",
    "plt.xlabel('S·ªë v√≤ng l·∫∑p k')\n",
    "plt.ylabel('Gi√° tr·ªã bi·∫øn')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ======================================\n",
    "# 1Ô∏è‚É£ Th√¥ng tin sinh vi√™n\n",
    "# ======================================\n",
    "gamma = [2, 3, 6, 3, 1, 9, 7, 1]\n",
    "alpha = gamma[6]           # Œ≥7 = 7\n",
    "beta = sum(gamma)          # T·ªïng 8 ch·ªØ s·ªë = 32\n",
    "\n",
    "print(f\"Œ± = {alpha}, Œ≤ = {beta}\")\n",
    "\n",
    "# ======================================\n",
    "# 2Ô∏è‚É£ ƒê·ªãnh nghƒ©a h√†m m·ª•c ti√™u v√† gradient\n",
    "# ======================================\n",
    "def f(x):\n",
    "    \"\"\"H√†m m·ª•c ti√™u f(x1, x2)\"\"\"\n",
    "    x1, x2 = x\n",
    "    return 0.5 * x1**2 + 0.5 * x2**2 + (alpha + 1)*x2 - beta\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"Gradient c·ªßa f(x1, x2)\"\"\"\n",
    "    x1, x2 = x\n",
    "    return np.array([x1, x2 + (alpha + 1)])\n",
    "\n",
    "# ======================================\n",
    "# 3Ô∏è‚É£ C√ÇU 1: T·ª± ƒë·ªông t√¨m nghi·ªám t·ªëi ∆∞u x* v√† gi√° tr·ªã t·ªëi ∆∞u p*\n",
    "# ======================================\n",
    "# Gi·∫£i ph∆∞∆°ng tr√¨nh ‚àáf(x) = 0\n",
    "# <=> x1 = 0 v√† x2 = -(Œ± + 1)\n",
    "def find_optimum(alpha, beta):\n",
    "    \"\"\"\n",
    "    Gi·∫£i nghi·ªám t·ªëi ∆∞u x* v√† p* cho h√†m f(x1,x2)\n",
    "    Returns:\n",
    "        x_star: vector nghi·ªám t·ªëi ∆∞u [x1*, x2*]\n",
    "        p_star: gi√° tr·ªã t·ªëi ∆∞u f(x*)\n",
    "    \"\"\"\n",
    "    # c√≥ th·ªÉ m·ªü r·ªông cho c√°c h√†m kh√°c b·∫±ng c√°ch d√πng scipy.optimize.solve, nh∆∞ng ·ªü ƒë√¢y gradient = 0 ƒë∆°n gi·∫£n\n",
    "    A = np.array([[1, 0], [0, 1]])  # H·ªá s·ªë gradient tuy·∫øn t√≠nh: [x1, x2 + (Œ±+1)]\n",
    "    b = np.array([0, -(alpha + 1)]) # nghi·ªám theo gradient = 0\n",
    "    x_star = np.linalg.solve(np.eye(2), b)  # gi·∫£i h·ªá ƒë∆°n gi·∫£n\n",
    "    p_star = f(x_star)\n",
    "    return x_star, p_star\n",
    "\n",
    "x_star, p_star = find_optimum(alpha, beta)\n",
    "print(\"\\n--- C√ÇU 1: K·∫æT QU·∫¢ T·ª∞ T√çNH ---\")\n",
    "print(f\"x* = {x_star}\")\n",
    "print(f\"p* = f(x*) = {p_star:.6f}\")\n",
    "\n",
    "# ======================================\n",
    "# 4Ô∏è‚É£ Thu·∫≠t to√°n Gradient Descent (C√¢u 2)\n",
    "# ======================================\n",
    "def gradient_descent_2d(f, grad_f, x_init, learning_rate, num_iterations, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Thu·∫≠t to√°n Gradient Descent t·ªïng qu√°t cho h√†m 2 bi·∫øn\n",
    "    \"\"\"\n",
    "    x = x_init\n",
    "    history = {'x': [x.copy()], 'f': [f(x)], 'gradient': [grad_f(x)], 'step_size': []}\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        g = grad_f(x)\n",
    "        if np.linalg.norm(g) < tolerance:\n",
    "            print(f\"H·ªôi t·ª• t·∫°i v√≤ng l·∫∑p {i+1}, ||‚àáf|| = {np.linalg.norm(g):.2e}\")\n",
    "            break\n",
    "        x_new = x - learning_rate * g\n",
    "        step_size = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(g)\n",
    "        history['step_size'].append(step_size)\n",
    "    return history\n",
    "\n",
    "# ======================================\n",
    "# 5Ô∏è‚É£ Ch·∫°y thu·∫≠t to√°n Gradient Descent\n",
    "# ======================================\n",
    "eta = 1 / (alpha + 1)\n",
    "x0 = np.array([1.0, 0.0])\n",
    "iterations = 50\n",
    "\n",
    "print(\"\\n--- C√îNG TH·ª®C C·∫¨P NH·∫¨T ---\")\n",
    "print(f\"Œ∑ = 1 / (Œ± + 1) = 1 / ({alpha + 1}) = {eta:.4f}\")\n",
    "print(\"C√¥ng th·ª©c: x^{(k+1)} = x^{(k)} - Œ∑ ‚àáf(x^{(k)})\")\n",
    "\n",
    "history = gradient_descent_2d(f, grad_f, x0, eta, iterations)\n",
    "\n",
    "x_opt = history['x'][-1]\n",
    "f_opt = f(x_opt)\n",
    "\n",
    "print(\"\\n--- C√ÇU 2: K·∫æT QU·∫¢ THU·∫¨T TO√ÅN ---\")\n",
    "print(f\"x t·ªëi ∆∞u (theo GD) ‚âà {x_opt}\")\n",
    "print(f\"f(x*) ‚âà {f_opt:.6f}\")\n",
    "\n",
    "# ======================================\n",
    "# 6Ô∏è‚É£ M·ªôt v√†i v√≤ng l·∫∑p ti√™u bi·ªÉu\n",
    "# ======================================\n",
    "print(\"\\n--- M·ªôt v√†i v√≤ng l·∫∑p ti√™u bi·ªÉu ---\")\n",
    "for i in [0, 1, 2, 5, 10, 20, len(history['x'])-1]:\n",
    "    x_i = history['x'][i]\n",
    "    print(f\"V√≤ng {i:2d}: x = {x_i}, f(x) = {f(x_i):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# . CHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G·ªçi $\\gamma=\\overline{\\gamma_1\\gamma_2\\gamma_3\\gamma_4\\gamma_5\\gamma_6\\gamma_7\\gamma_8}$ l√† m√£ s·ªë sinh vi√™n c·ªßa anh/ch·ªã. ƒê·∫∑t $\\alpha=\\gamma_7$ v√† $\\beta=\\sum_{i=1}^8\\gamma_i$, x√©t b√†i to√°n t·ªëi ∆∞u sau\n",
    "\\begin{align}\n",
    "\\min_{x=(x_1,x_2)\\in \\mathbb{R}^2} f(x)=\\dfrac{1}{2}x_1^2+\\dfrac{1}{2}x_2^2+(\\alpha+1) x_2-\\beta\\quad (1)\n",
    "\\end{align}\n",
    "\n",
    "1. H√£y t√¨m $\\alpha,\\beta$ v√† gi√° tr·ªã t·ªëi ∆∞u $p^*$ c·ªßa b√†i to√°n (1).\n",
    "\n",
    "2. 2. S·ª≠ d·ª•ng thu·∫≠t to√°n Gradient Descent cho b√†i to√°n (1) v·ªõi learning rate l√† $\\dfrac{1}{\\alpha+1}$ v√† ƒëi·ªÉm kh·ªüi t·∫°o $x^{(0)}=(1,0)$, h√£y t√¨m c√¥ng th·ª©c cho ƒëi·ªÉm c·∫≠p nh·∫≠t $x^{(k)}$.\n",
    "  \n",
    "3. G·ªçi $x^*$ l√† ƒëi·ªÉm t·ªëi ∆∞u c·ªßa (1), h√£y v·∫Ω ƒë·ªì th·ªã bi·ªÉu th·ªã cho sai s·ªë $\\text{err}_k=\\|\\nabla f(x^*)-\\nabla f(x^{(k)})\\|_2^2$. T·ª´ ƒë√≥ anh/ch·ªã r√∫t ra ƒë∆∞·ª£c k·∫øt lu·∫≠n g√¨?\n",
    "4. H√£y t√¨m learning rate trong thu·∫≠t to√°n Gradient Descent c·ªßa b√†i to√°n (1) theo ph∆∞∆°ng ph√°p exact line search. T·ª´ ƒë√≥ r√∫t ra nh·∫≠n x√©t cho c√°c ƒëi·ªÉm c·∫≠p nh·∫≠t $x^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import grad\n",
    "from autograd import numpy as anp\n",
    "from scipy.optimize import root\n",
    "\n",
    "# ======================================\n",
    "# 1Ô∏è‚É£ Th√¥ng tin sinh vi√™n\n",
    "# ======================================\n",
    "gamma = [2, 3, 6, 3, 1, 9, 7, 1]\n",
    "alpha = gamma[6]           # Œ≥7 = 7\n",
    "beta = sum(gamma)          # T·ªïng 8 ch·ªØ s·ªë = 32\n",
    "\n",
    "print(f\"Œ± = {alpha}, Œ≤ = {beta}\")\n",
    "\n",
    "# ======================================\n",
    "# 2Ô∏è‚É£ ƒê·ªãnh nghƒ©a h√†m m·ª•c ti√™u (d√πng autograd)\n",
    "# ======================================\n",
    "def f(x):\n",
    "    \"\"\"H√†m m·ª•c ti√™u f(x1, x2)\"\"\"\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return 0.5 * x1**2 + 0.5 * x2**2 + (alpha + 1)*x2 - beta\n",
    "\n",
    "# T·∫°o gradient t·ª± ƒë·ªông b·∫±ng autograd\n",
    "grad_f = grad(lambda x: f(x))  # ƒë·∫°o h√†m t·ª± ƒë·ªông c·ªßa f(x) theo vector x\n",
    "\n",
    "# ======================================\n",
    "# 3Ô∏è‚É£ C√ÇU 1: T·ª± ƒë·ªông t√¨m nghi·ªám t·ªëi ∆∞u x* v√† gi√° tr·ªã t·ªëi ∆∞u p*\n",
    "# ======================================\n",
    "#\"\"\"\n",
    "#def find_optimum(grad_f, f, x0):\n",
    "    \n",
    "    #Gi·∫£i nghi·ªám t·ªëi ∆∞u x* v√† p* b·∫±ng c√°ch t√¨m nghi·ªám c·ªßa ‚àáf(x) = 0\n",
    "    #Args:\n",
    "        #grad_f: h√†m gradient (autograd)\n",
    "       # f: h√†m m·ª•c ti√™u\n",
    "       # x0: ƒëi·ªÉm kh·ªüi t·∫°o ban ƒë·∫ßu\n",
    "    #Returns:\n",
    "       # x_star: vector nghi·ªám t·ªëi ∆∞u\n",
    "       # p_star: gi√° tr·ªã f(x*) t·∫°i nghi·ªám\n",
    "    \n",
    "    # D√πng scipy.optimize.root ƒë·ªÉ gi·∫£i h·ªá ‚àáf(x)=0\n",
    "    #sol = root(lambda x: grad_f(x), x0)\n",
    "    #x_star = sol.x\n",
    "    #p_star = f(x_star)\n",
    "    #return x_star, p_star\n",
    "#\"\"\"\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def find_optimum(grad_f, f):\n",
    "    \"\"\"\n",
    "    T√¨m x* v√† p* b·∫±ng c√°ch t·ªëi thi·ªÉu h√≥a ||‚àáf(x)||¬≤\n",
    "    \"\"\"\n",
    "    x0 = np.zeros(2)\n",
    "    # ƒë·ªãnh nghƒ©a h√†m ph·ª•\n",
    "    def g_norm2(x): \n",
    "        g = grad_f(x)\n",
    "        return np.sum(g**2)\n",
    "    \n",
    "    res = minimize(g_norm2, x0, method='BFGS')\n",
    "    x_star = res.x\n",
    "    p_star = f(x_star)\n",
    "    return x_star, p_star\n",
    "# G·ªçi h√†m t√¨m nghi·ªám\n",
    "x_star, p_star = find_optimum(grad_f, f)\n",
    "\n",
    "print(\"\\n--- C√ÇU 1: K·∫æT QU·∫¢ T·ª∞ T√çNH (d√πng autograd + scipy) ---\")\n",
    "print(f\"x* = {x_star}\")\n",
    "print(f\"p* = f(x*) = {p_star:.6f}\")\n",
    "\n",
    "# ======================================\n",
    "# 4Ô∏è‚É£ Thu·∫≠t to√°n Gradient Descent (C√¢u 2)\n",
    "# ======================================\n",
    "def gradient_descent_2d(f, grad_f, x_init, learning_rate, num_iterations, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Thu·∫≠t to√°n Gradient Descent t·ªïng qu√°t cho h√†m 2 bi·∫øn\n",
    "    \"\"\"\n",
    "    x = x_init\n",
    "    history = {'x': [x.copy()], 'f': [f(x)], 'gradient': [grad_f(x)], 'step_size': [], 'grad_norm2': [np.linalg.norm(grad_f(x))**2]}\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        g = grad_f(x)\n",
    "        if np.linalg.norm(g) < tolerance:\n",
    "            print(f\"H·ªôi t·ª• t·∫°i v√≤ng l·∫∑p {i+1}, ||‚àáf|| = {np.linalg.norm(g):.2e}\")\n",
    "            break\n",
    "        x_new = x - learning_rate * g\n",
    "        step_size = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(g)\n",
    "        history['step_size'].append(step_size)\n",
    "        history['grad_norm2'].append(np.linalg.norm(grad_f(x))**2)\n",
    "    return history\n",
    "\n",
    "# ======================================\n",
    "# 5Ô∏è‚É£ Ch·∫°y thu·∫≠t to√°n Gradient Descent\n",
    "# ======================================\n",
    "eta = 1 / (alpha + 1)\n",
    "x0 = np.array([1.0, 0.0])\n",
    "iterations = 50\n",
    "\n",
    "print(\"\\n--- C√îNG TH·ª®C C·∫¨P NH·∫¨T ---\")\n",
    "print(f\"Œ∑ = 1 / (Œ± + 1) = 1 / ({alpha + 1}) = {eta:.4f}\")\n",
    "print(\"C√¥ng th·ª©c: x^{(k+1)} = x^{(k)} - Œ∑ ‚àáf(x^{(k)})\")\n",
    "\n",
    "history = gradient_descent_2d(f, grad_f, x0, eta, iterations)\n",
    "\n",
    "x_opt = history['x'][-1]\n",
    "f_opt = f(x_opt)\n",
    "\n",
    "print(\"\\n--- C√ÇU 2: K·∫æT QU·∫¢ THU·∫¨T TO√ÅN ---\")\n",
    "print(f\"x t·ªëi ∆∞u (theo GD) ‚âà {x_opt}\")\n",
    "print(f\"f(x*) ‚âà {f_opt:.6f}\")\n",
    "\n",
    "# ======================================\n",
    "# 6Ô∏è‚É£ C√ÇU 3: V·∫Ω ƒë·ªì th·ªã sai s·ªë err_k = ||‚àáf(x*) - ‚àáf(xk)||¬≤\n",
    "# ======================================\n",
    "err_k = np.array(history['grad_norm2'])  # v√¨ ‚àáf(x*)=0\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(err_k, marker='o', color='tab:red')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"S·ªë v√≤ng l·∫∑p k\")\n",
    "plt.ylabel(r\"$\\mathrm{err}_k = ||\\nabla f(x^*) - \\nabla f(x^{(k)})||_2^2$\")\n",
    "plt.title(\"Bi·ªÉu ƒë·ªì h·ªôi t·ª• sai s·ªë gradient (theo log scale)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Sai s·ªë ban ƒë·∫ßu v√† cu·ªëi\n",
    "err_start = err_k[0]\n",
    "err_end = err_k[-1]\n",
    "\n",
    "# T·ª∑ l·ªá gi·∫£m sai s·ªë (log10)\n",
    "reduction_ratio = err_end / err_start\n",
    "log_drop = np.log10(err_start / err_end) if err_end > 0 else np.inf\n",
    "\n",
    "print(\"\\n--- C√ÇU 3: K·∫æT LU·∫¨N ---\")\n",
    "print(f\"Sai s·ªë ban ƒë·∫ßu: {err_start:.6e}\")\n",
    "print(f\"Sai s·ªë cu·ªëi c√πng: {err_end:.6e}\")\n",
    "print(f\"T·ª∑ l·ªá gi·∫£m sai s·ªë: {reduction_ratio:.6e}\")\n",
    "print(f\"M·ª©c gi·∫£m theo log10: {log_drop:.2f} b·∫≠c ƒë·ªô l·ªõn\")\n",
    "\n",
    "if err_end < 1e-6:\n",
    "    print(\"‚úÖ K·∫øt lu·∫≠n: Sai s·ªë gradient gi·∫£m r·∫•t m·∫°nh ‚Üí Thu·∫≠t to√°n h·ªôi t·ª• t·ªët v·ªÅ x*.\")\n",
    "elif err_end < 1e-3:\n",
    "    print(\"‚ö†Ô∏è K·∫øt lu·∫≠n: Sai s·ªë gi·∫£m ch·∫≠m nh∆∞ng v·∫´n h·ªôi t·ª• d·∫ßn ‚Üí C·∫ßn nhi·ªÅu v√≤ng l·∫∑p h∆°n ho·∫∑c learning rate l·ªõn h∆°n.\")\n",
    "else:\n",
    "    print(\"‚ùå K·∫øt lu·∫≠n: Sai s·ªë ch∆∞a gi·∫£m ƒë√°ng k·ªÉ ‚Üí C√≥ th·ªÉ learning rate qu√° nh·ªè ho·∫∑c ch∆∞a h·ªôi t·ª•.\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# ======================================\n",
    "# 6Ô∏è‚É£ M·ªôt v√†i v√≤ng l·∫∑p ti√™u bi·ªÉu\n",
    "# ======================================\n",
    "print(\"\\n--- M·ªôt v√†i v√≤ng l·∫∑p ti√™u bi·ªÉu ---\")\n",
    "for i in [0, 1, 2, 5, 10, 20, len(history['x'])-1]:\n",
    "    x_i = history['x'][i]\n",
    "    print(f\"V√≤ng {i:2d}: x = {x_i}, f(x) = {f(x_i):.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# ======================================\n",
    "# 7Ô∏è‚É£ C√ÇU 4: EXACT LINE SEARCH\n",
    "# ======================================\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def exact_line_search(f, grad_f, xk):\n",
    "    \"\"\"\n",
    "    Th·ª±c hi·ªán Exact Line Search ƒë·ªÉ t√¨m learning rate t·ªëi ∆∞u Œ∑*\n",
    "    cho h√†m b·∫•t k·ª≥ t·∫°i ƒëi·ªÉm xk.\n",
    "    \"\"\"\n",
    "    gk = grad_f(xk)\n",
    "    def phi(eta):\n",
    "        return f(xk - eta * gk)\n",
    "    res = minimize_scalar(phi, bounds=(0, 2), method='bounded')\n",
    "    return res.x\n",
    "\n",
    "# T√≠nh Œ∑_exact t·∫°i ƒëi·ªÉm kh·ªüi t·∫°o\n",
    "eta_exact = exact_line_search(f, grad_f, x0)\n",
    "\n",
    "print(\"\\n--- C√ÇU 4: EXACT LINE SEARCH ---\")\n",
    "print(f\"Œ∑ (Exact Line Search) = {eta_exact:.6f}\")\n",
    "print(f\"Œ∑ (C·ªë ƒë·ªãnh theo ƒë·ªÅ)   = {eta:.6f}\")\n",
    "\n",
    "# Nh·∫≠n x√©t:\n",
    "if abs(eta_exact - eta) < 1e-2:\n",
    "    print(\"‚úÖ Nh·∫≠n x√©t: Œ∑_exact g·∫ßn v·ªõi Œ∑ c·ªë ƒë·ªãnh ‚Üí t·ªëc ƒë·ªô h·ªôi t·ª• ·ªïn ƒë·ªãnh v√† h·ª£p l√Ω.\")\n",
    "elif eta_exact > eta:\n",
    "    print(\"‚ö†Ô∏è Nh·∫≠n x√©t: Œ∑_exact l·ªõn h∆°n Œ∑ c·ªë ƒë·ªãnh ‚Üí ta c√≥ th·ªÉ tƒÉng learning rate ƒë·ªÉ h·ªôi t·ª• nhanh h∆°n.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nh·∫≠n x√©t: Œ∑_exact nh·ªè h∆°n Œ∑ c·ªë ƒë·ªãnh ‚Üí learning rate hi·ªán t·∫°i h∆°i l·ªõn, d·ªÖ dao ƒë·ªông quanh nghi·ªám.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------ H·∫øt -----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L∆∞u √Ω: sinh vi√™n kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng internet. Gi√°m th·ªã kh√¥ng gi·∫£i th√≠ch g√¨ th√™m."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
