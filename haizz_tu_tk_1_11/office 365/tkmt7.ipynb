{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsccfSDERjBo"
   },
   "source": [
    "# ĐỀ KIỂM TRA THỰC HÀNH 1 \n",
    "## Môn: Tối ưu hoá cho Khoa học dữ liệu\n",
    "## Ngày: 09/10/2024\n",
    "## Lớp: DHKHDL18A. Nhóm thực hành 3.\n",
    "## Thời gian: 45 phút"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLqKbo8CeQy1"
   },
   "source": [
    "### Câu 1. (6 điểm) ### \n",
    "Cho ma trận $A=\\begin{bmatrix}3 & 0\\\\ 0 & 1 \\end{bmatrix}$ và vector $b=(2;-1)$. Xét bài toán tối ưu sau \n",
    "$$\\min_{x\\in\\mathbb{R}^2}f(x)=\\dfrac{1}{2}x^TAx-b^Tx.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W00lHNXQfg15"
   },
   "source": [
    "1. (2 điểm) Xác định điểm tối ưu $x^*$ và giá trị tối ưu $p^*$ của bài toán trên.\n",
    "\n",
    "\n",
    "2. (2 điểm) Sử dụng thuật toán Gradient Descent, với giá trị $x$ ban đầu là $x^{(0)}=(0;1)$, sử dụng learning rate là $0.9$ và thực hiện tối đa $20$ vòng lặp. In ra giá trị của $x^{(k)}$ và $f(x^{(k)})$ tương ứng sau mỗi vòng lặp $k$. Từ đó vẽ đồ thị biểu thị cho sai số $|f(x^{(k)})-f(p^*)|$.\n",
    "\n",
    "\n",
    "3. (2 điểm) Tìm learning rate theo phương pháp exact-line search trong thuật toán Gradient Descent của bài toán trên với điểm khởi tạo là $x^{(0)}=(0;1)$. Gọi điểm cập nhật trong trường hợp này là $x^{(k)}_{\\text{exact}}$, vẽ đồ thị mô tả cho sai số $|f(x^{(k)}_{\\text{exact}})-f(p^*)|$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# 1. Khai báo bài toán\n",
    "# =========================\n",
    "def create_problem(A, b):\n",
    "    \"\"\"\n",
    "    Tạo hàm f(x) = 1/2 x^T A x - b^T x\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        return 0.5 * x @ A @ x - b @ x\n",
    "    return f\n",
    "\n",
    "# =========================\n",
    "# 2. Gradient Descent (fixed lr)\n",
    "# =========================\n",
    "def gradient_descent(f, grad_f, x0, lr, max_iter):\n",
    "    x = x0.copy()\n",
    "    xs, fs = [x.copy()], [f(x)]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        x = x - lr * grad_f(x)\n",
    "        xs.append(x.copy())\n",
    "        fs.append(f(x))\n",
    "        \n",
    "        print(f\"Iter {k+1:2d}: x = {x}, f(x) = {fs[-1]:.6f}\")\n",
    "        \n",
    "    return np.array(xs), np.array(fs)\n",
    "\n",
    "# =========================\n",
    "# 3. Gradient Descent với Exact Line Search\n",
    "# =========================\n",
    "def gradient_descent_exact(A, f, grad_f, x0, max_iter):\n",
    "    x = x0.copy()\n",
    "    xs, fs = [x.copy()], [f(x)]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        alpha = (g @ g) / (g @ A @ g)\n",
    "        x = x - alpha * g\n",
    "        \n",
    "        xs.append(x.copy())\n",
    "        fs.append(f(x))\n",
    "        \n",
    "        print(f\"Iter {k+1:2d}: alpha = {alpha:.6f}, x = {x}, f(x) = {fs[-1]:.6f}\")\n",
    "        \n",
    "    return np.array(xs), np.array(fs)\n",
    "\n",
    "# =========================\n",
    "# 4. MAIN – chạy bài toán\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ----- INPUT (chỉ cần đổi chỗ này) -----\n",
    "    A = np.array([[3., 0.],\n",
    "                  [0., 1.]])\n",
    "    \n",
    "    b = np.array([2., -1.])\n",
    "    x0 = np.array([0., 1.])\n",
    "    \n",
    "    lr = 0.9\n",
    "    max_iter = 20\n",
    "    \n",
    "    # ----- Tạo hàm và gradient -----\n",
    "    f = create_problem(A, b)\n",
    "    grad_f = grad(f)\n",
    "    \n",
    "    # ----- Nghiệm tối ưu -----\n",
    "    x_star = np.linalg.solve(A, b)\n",
    "    p_star = f(x_star)\n",
    "    \n",
    "    print(\"===== NGHIỆM GIẢI TÍCH =====\")\n",
    "    print(\"x* =\", x_star)\n",
    "    print(\"p* =\", p_star)\n",
    "    \n",
    "    # ----- Gradient Descent thường -----\n",
    "    print(\"\\n===== GRADIENT DESCENT (FIXED LR) =====\")\n",
    "    xs_gd, fs_gd = gradient_descent(f, grad_f, x0, lr, max_iter)\n",
    "    \n",
    "    # ----- Exact Line Search -----\n",
    "    print(\"\\n===== GRADIENT DESCENT (EXACT LINE SEARCH) =====\")\n",
    "    xs_exact, fs_exact = gradient_descent_exact(A, f, grad_f, x0, max_iter)\n",
    "    \n",
    "    # =========================\n",
    "    # 5. Vẽ đồ thị sai số\n",
    "    # =========================\n",
    "    err_gd = np.abs(fs_gd - p_star)\n",
    "    err_exact = np.abs(fs_exact - p_star)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.semilogy(err_gd, 'o-', label='GD – fixed lr')\n",
    "    plt.semilogy(err_exact, 's-', label='GD – exact line search')\n",
    "    plt.xlabel(\"Iteration k\")\n",
    "    plt.ylabel(r\"$|f(x^{(k)}) - p^*|$\")\n",
    "    plt.title(\"So sánh tốc độ hội tụ\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "# =====================================================\n",
    "# 1. Định nghĩa bài toán f(x) = 1/2 x^T A x - b^T x\n",
    "# =====================================================\n",
    "def create_quadratic_problem(A, b):\n",
    "    def f(x):\n",
    "        return 0.5 * x @ A @ x - b @ x\n",
    "    return f\n",
    "\n",
    "# =====================================================\n",
    "# 2. Exact Line Search – in rõ từng bước\n",
    "# =====================================================\n",
    "def exact_line_search_steps(A, f, grad_f, x0, max_iter=1):\n",
    "    \"\"\"\n",
    "    Trình bày chi tiết từng bước tìm alpha theo exact line search\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "\n",
    "    print(\"========== EXACT LINE SEARCH ==========\")\n",
    "    print(f\"x^(0) = {x}\")\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        print(f\"\\n--- Bước lặp k = {k} ---\")\n",
    "\n",
    "        # Gradient\n",
    "        g = grad_f(x)\n",
    "        print(\"Gradient ∇f(x^k) = A x^k - b =\", g)\n",
    "\n",
    "        # Tử số\n",
    "        numerator = g @ g\n",
    "        print(\"Tử số  ∇f^T ∇f =\", numerator)\n",
    "\n",
    "        # Mẫu số\n",
    "        Ag = A @ g\n",
    "        denominator = g @ Ag\n",
    "        print(\"Mẫu số  ∇f^T A ∇f =\", denominator)\n",
    "\n",
    "        # Learning rate exact\n",
    "        alpha = numerator / denominator\n",
    "        print(\"Learning rate α_exact =\", alpha)\n",
    "\n",
    "        # Cập nhật x\n",
    "        x_new = x - alpha * g\n",
    "        print(\"x^(k+1) = x^k − α ∇f =\", x_new)\n",
    "\n",
    "        # Giá trị hàm\n",
    "        fx = f(x_new)\n",
    "        print(\"f(x^(k+1)) =\", fx)\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x\n",
    "\n",
    "# =====================================================\n",
    "# 3. MAIN – chỉ cần đổi dữ liệu đầu vào\n",
    "# =====================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ----- INPUT -----\n",
    "    A = np.array([[3., 0.],\n",
    "                  [0., 1.]])\n",
    "    b = np.array([2., -1.])\n",
    "    x0 = np.array([0., 1.])\n",
    "\n",
    "    # ----- Tạo hàm và gradient -----\n",
    "    f = create_quadratic_problem(A, b)\n",
    "    grad_f = grad(f)\n",
    "\n",
    "    # ----- Chạy exact line search (1 bước như đề) -----\n",
    "    exact_line_search_steps(A, f, grad_f, x0, max_iter=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (2 điểm) Xác định điểm tối ưu $x^*$ và giá trị tối ưu $p^*$ của bài toán trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Định nghĩa ma trận A và vector b\n",
    "A = np.array([[3,0],[0,1]])\n",
    "b = np.array([2,-1])\n",
    "\n",
    "# Hàm mục tiêu f(x)\n",
    "def f(x):\n",
    "    return 0.5 * x.T @ A @ x - b.T @ x\n",
    "\n",
    "# Gradient của f(x)\n",
    "def grad_f(x):\n",
    "    return A @ x - b\n",
    "\n",
    "# **Phần 1: Xác định điểm tối ưu x* và giá trị tối ưu p***\n",
    "\n",
    "# Giải phương trình A*x* = b để tìm x*\n",
    "x_star = np.linalg.solve(A, b)\n",
    "f_star = f(x_star)\n",
    "\n",
    "print(\"Phần 1:\")\n",
    "print(f\"Điểm tối ưu x* = {x_star}\")\n",
    "print(f\"Giá trị tối ưu p* = {f_star}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy điểm tối ưu của bài toán : $\\displaystyle x^* = (\\frac{2}{3}, -1)$ , giá trị tối ưu của bài toán là $p^* = \\frac{-7}{6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (2 điểm) Sử dụng thuật toán Gradient Descent, với giá trị $x$ ban đầu là $x^{(0)}=(0;1)$, sử dụng learning rate là $0.9$ và thực hiện tối đa $20$ vòng lặp. In ra giá trị của $x^{(k)}$ và $f(x^{(k)})$ tương ứng sau mỗi vòng lặp $k$. Từ đó vẽ đồ thị biểu thị cho sai số $|f(x^{(k)})-f(p^*)|$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo\n",
    "x_0 = np.array([0, 1])  # Giá trị ban đầu x(0)\n",
    "learning_rate = 0.9  # Learning rate cố định\n",
    "max_iter = 20  # Số vòng lặp tối đa\n",
    "\n",
    "# Danh sách lưu trữ các giá trị x(k) và f(x(k))\n",
    "x_vals = [x_0]\n",
    "f_vals = [f(x_0)]\n",
    "\n",
    "# Gradient Descent\n",
    "x = x_0\n",
    "for k in range(max_iter):\n",
    "    grad = grad_f(x)  # Tính gradient tại x(k)\n",
    "    x = x - learning_rate * grad  # Cập nhật giá trị của x(k)\n",
    "    x_vals.append(x)\n",
    "    f_vals.append(f(x))\n",
    "\n",
    "# In kết quả của từng vòng lặp\n",
    "print(\"Phần 2: Gradient Descent với learning rate = 1\")\n",
    "for k in range(max_iter + 1):\n",
    "    print(f\"Vòng lặp {k}: x = {x_vals[k]}, f(x) = {f_vals[k]}\")\n",
    "\n",
    "# Vẽ đồ thị sai số |f(x^k) - f(x*)| theo số vòng lặp\n",
    "errors = [abs(f_val - f_star) for f_val in f_vals]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(max_iter + 1), errors, marker='o')\n",
    "plt.title(\"Sai số |f(x^{(k)}) - f(x^*)| qua các vòng lặp\")\n",
    "plt.xlabel(\"Số vòng lặp k\")\n",
    "plt.ylabel(\"Sai số\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (2 điểm) Tìm learning rate theo phương pháp exact-line search trong thuật toán Gradient Descent của bài toán trên với điểm khởi tạo là $x^{(0)}=(0;1)$. Gọi điểm cập nhật trong trường hợp này là $x^{(k)}_{\\text{exact}}$, vẽ đồ thị mô tả cho sai số $|f(x^{(k)}_{\\text{exact}})-f(p^*)|$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, Matrix, solve, latex\n",
    "from IPython.display import display, Markdown\n",
    "# Khai báo các biến\n",
    "x1, x2 = symbols('x1 x2')\n",
    "\n",
    "# Ma trận A và vector b\n",
    "A = Matrix([[3,0],[0,1]])\n",
    "b = Matrix([2,-1])\n",
    "\n",
    "# Hàm số f(x) = 1/2 * x^T * A * x - b^T * x\n",
    "f = 1/2 * Matrix([x1, x2]).T * A * Matrix([x1, x2]) - b.T * Matrix([x1, x2])\n",
    "\n",
    "# Tính đạo hàm của f theo từng biến\n",
    "df_x1 = diff(f, x1)\n",
    "df_x2 = diff(f, x2)\n",
    "\n",
    "display(Markdown(f\"Đạo hàm : ${latex(df_x1)}$\"))\n",
    "display(Markdown(f\"Đạo hàm : ${latex(df_x2)}$\"))\n",
    "\n",
    "# Giải hệ phương trình đạo hàm bằng 0\n",
    "sol = solve([df_x1, df_x2], [x1, x2])\n",
    "sol, f.subs({x1: sol[x1], x2: sol[x2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = symbols('s')\n",
    "tk = Matrix([[x1],[x2]]) - s*Matrix([df_x1, df_x2])\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nghiem_s = solve(diff(f.subs({x1: tk[0], x2:tk[1]}),s))\n",
    "nghiem_s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabla = '\\u2207'\n",
    "display(Markdown(f\" Điểm s để f(x-s.{nabla}.f(x)) nhỏ nhất là : $\\displaystyle {latex(nghiem_s[0][s])}$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nghiem_s[0][s].subs({x1 : 1 , x2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f.subs({x1: tk[0], x2:tk[1]})).subs(*nghiem_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = nghiem_s[0][s].subs({x1:1,x2:0})\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code tính vòng lặp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0, 1])\n",
    "max_iter = 20  # Giới hạn số vòng lặp xuống còn 20\n",
    "\n",
    "# Hàm mục tiêu\n",
    "def f(x):\n",
    "    return 0.5 * x.T @ A @ x - b.T @ x\n",
    "\n",
    "# Tính gradient\n",
    "def grad_f(x):\n",
    "    return A @ x - b\n",
    "\n",
    "# Tìm điểm tối ưu p*\n",
    "p_star = np.linalg.solve(A, b)\n",
    "f_star = f(p_star)\n",
    "\n",
    "# Hàm exact-line search để tìm learning rate tối ưu\n",
    "def exact_line_search(grad):\n",
    "    return grad.T @ grad / (grad.T @ A @ grad)  # Công thức tìm learning rate tối ưu\n",
    "\n",
    "# Danh sách lưu trữ các giá trị x(k) và f(x(k)) với exact-line search\n",
    "x_vals_exact = [x_0]\n",
    "f_vals_exact = [f(x_0)]\n",
    "learning_rates = []  # Danh sách lưu learning rates\n",
    "\n",
    "x = x_0\n",
    "for k in range(max_iter):\n",
    "    grad = grad_f(x)  # Tính gradient tại x(k)\n",
    "    lr_exact = exact_line_search(grad)  # Tìm learning rate tối ưu\n",
    "    learning_rates.append(lr_exact)  # Lưu learning rate\n",
    "    x = x - lr_exact * grad  # Cập nhật giá trị của x(k) với learning rate tối ưu\n",
    "    x_vals_exact.append(x)\n",
    "    f_vals_exact.append(f(x))\n",
    "\n",
    "# In kết quả của từng vòng lặp với exact-line search\n",
    "print(\"\\nPhần 3: Exact-line Search\")\n",
    "for k in range(max_iter + 1):\n",
    "    print(f\"Vòng lặp {k}: x = {x_vals_exact[k]}, f(x) = {f_vals_exact[k]}\")\n",
    "\n",
    "# In ra bước nhảy tại mỗi vòng lặp\n",
    "print(\"\\nBước nhảy (learning rates) tại mỗi vòng lặp:\")\n",
    "for k in range(max_iter):\n",
    "    print(f\"Vòng lặp {k}: learning rate = {learning_rates[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ đồ thị sai số |f(x^k_exact) - f(x*)| theo số vòng lặp\n",
    "errors_exact = [abs(f_val - f_star) for f_val in f_vals_exact]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(max_iter + 1), errors_exact, marker='o')\n",
    "plt.title(\"Sai số |f(x^{(k)}_{exact}) - f(x^*)| qua các vòng lặp\")\n",
    "plt.xlabel(\"Số vòng lặp k\")\n",
    "plt.ylabel(\"Sai số\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 2. (4 điểm)\n",
    "Bộ dữ liệu **Labeled Faces in the Wild (LFW)**: Bộ dữ liệu là tập hợp các bức ảnh JPEG của những người nổi tiếng được thu thập trên internet. Sử dụng lệnh sau để load bộ dữ liệu\n",
    "```python\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfwPeople = fetch_lfw_people(data_home=\"./\",min_faces_per_person=70, resize=0.4)\n",
    "````\n",
    "1. Hiển thị thông tin kích thước ảnh, số lượng ảnh, và số lượng nhãn (danh tính) trong bộ dữ liệu.\n",
    "2. Chia dữ liệu thành 2 phần train/test (70/30) và chuẩn hóa dữ liệu\n",
    "3. Xây dựng mô hình SVM sử dụng kernel ```sigmoid``` ({‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’})\n",
    "4. Đánh giá mô hình: sử dụng accuracy, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hiển thị thông tin kích thước ảnh, số lượng ảnh, và số lượng nhãn (danh tính) trong bộ dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LFW dataset\n",
    "lfwPeople = fetch_lfw_people(\n",
    "    data_home=\"./\",\n",
    "    min_faces_per_person=70,\n",
    "    resize=0.4\n",
    ")\n",
    "\n",
    "# Dữ liệu\n",
    "X = lfwPeople.data          # (n_samples, n_features)\n",
    "y = lfwPeople.target        # nhãn\n",
    "target_names = lfwPeople.target_names\n",
    "images = lfwPeople.images  # ảnh gốc (h, w)\n",
    "\n",
    "# Thông tin\n",
    "n_samples, n_features = X.shape\n",
    "n_classes = len(target_names)\n",
    "h, w = images.shape[1], images.shape[2]\n",
    "\n",
    "print(\"===== THÔNG TIN BỘ DỮ LIỆU LFW =====\")\n",
    "print(f\"Kích thước ảnh: {h} x {w}\")\n",
    "print(f\"Số lượng ảnh: {n_samples}\")\n",
    "print(f\"Số lượng nhãn (danh tính): {n_classes}\")\n",
    "print(\"Danh sách nhãn:\", target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dữ liệu train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Chuẩn hóa dữ liệu\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n===== CHIA DỮ LIỆU =====\")\n",
    "print(\"Train size:\", X_train_scaled.shape)\n",
    "print(\"Test size :\", X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình SVM\n",
    "svm_model = SVC(\n",
    "    kernel=\"sigmoid\",\n",
    "    C=1.0,\n",
    "    gamma=\"scale\", # auto\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n===== HUẤN LUYỆN MÔ HÌNH SVM =====\")\n",
    "print(\"Kernel sử dụng:\", svm_model.kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Các chỉ số đánh giá\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"\\n===== ĐÁNH GIÁ MÔ HÌNH =====\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "print(\"\\n===== BÁO CÁO CHI TIẾT =====\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------- Hết --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lưu ý: sinh viên không được sử dụng internet. Giám thị không giải thích gì thêm."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
