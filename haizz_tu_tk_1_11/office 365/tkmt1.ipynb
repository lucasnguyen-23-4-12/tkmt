{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee4316a-5700-4b28-98ee-e35c7937c166",
   "metadata": {},
   "source": [
    "# TU LAM\n",
    "\n",
    "Câu 1: Trong năm nay, một cửa hàng kinh doanh xe máy dự định kinh doanh hai loại xe máy, xe máy Lead và xe Vision, với số vốn ban đầu không vượt quá 36 tỉ đồng.\n",
    "Giá nhập về 1 chiếc xe máy Lead là 40 triệu đồng, lợi nhuận dự kiến là 5 triệu đồng 1 chiếc.\n",
    "Giá nhập về 1 chiếc xe máy Vision là 30 triệu đồng, lợi nhuận dự kiến là 3,2 triệu đồng một chiếc.\n",
    "Cửa hàng ước tính rằng:\n",
    "Tổng nhu cầu thị trường không vượt quá 1100 chiếc xe cả hai loại\n",
    "Nhu cầu xe Lead không vượt quá 1,5 lần nhu cầu xe Vision.\n",
    "Bài toán đặt ra là xác định lợi nhuận có thể thu được lớn nhất của cửa hàng là bao nhiêu.\n",
    "\n",
    "a) Hãy lập mô hình tối ưu của bài toán trên theo dạng dưới đây bằng cách chỉ ra hàm mục tiêu $f_0$, ma trận $A$ và vector $b$:\n",
    "$$\\begin{align}\n",
    "&\\text{minimize } f_0(x) \\\\\n",
    "&\\text{subject to } Ax \\leq b\n",
    "\\end{align}$$\n",
    "\n",
    "b) Sử dụng thư viện thích hợp để giải bài toán trên."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba459f4a-565e-495a-b705-6aa377730c3b",
   "metadata": {},
   "source": [
    "a)\n",
    "Ham muc tieu: f(x) = 5*x + 3.2*y\n",
    "\n",
    "subject to:   x + y <= 1100\n",
    "\n",
    "            x - 1.5*y <= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c13825-2aa2-4a63-8b4a-a56d0e4c4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Khai báo các biến\n",
    "x = cp.Variable(2, nonneg=True)\n",
    "\n",
    "# Hàm mục tiêu (tối đa hóa doanh thu)\n",
    "objective = cp.Maximize(5*x[0] + 3.2*x[1] )\n",
    "\n",
    "# Các ràng buộc\n",
    "constraints = [\n",
    "    x[0] + x[1] <= 1100,\n",
    "    x[0] - 1.5*x[1] <= 0,\n",
    "    x[0]*40 + x[1]*30 <= 36000\n",
    "]\n",
    "\n",
    "# Tạo và giải bài toán\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "# In kết quả\n",
    "if problem.status == cp.OPTIMAL:\n",
    "    print(f\"  Xe lead (x1): {x.value[0]:.2f}\")\n",
    "    print(f\"  Xe vision (x2): {x.value[1]:.2f}\")\n",
    "    \n",
    "    print(f\"Loi nhuan tối đa: {problem.value:.2f} \")\n",
    "else:\n",
    "    print(\"Bài toán không tìm thấy lời giải tối ưu.\")\n",
    "    print(problem.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81628b-6371-4719-9c04-93b0497e0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# 1. Hệ số hàm mục tiêu (Vì linprog thực hiện minimize nên ta dùng số âm để tìm max)\n",
    "# Lợi nhuận: 5x1 + 3.2x2 -> f0 = -5x1 - 3.2x2\n",
    "c = [-5, -3.2]\n",
    "\n",
    "# 2. Ma trận A và vector b cho các ràng buộc Ax <= b\n",
    "A = [\n",
    "    [40, 30],      # 40x1 + 30x2 <= 36000 (Vốn)\n",
    "    [1, 1],        # x1 + x2 <= 1100 (Tổng nhu cầu)\n",
    "    [1, -1.5],     # x1 - 1.5x2 <= 0 (x1 <= 1.5x2)\n",
    "]\n",
    "\n",
    "b = [36000, 1100, 0]\n",
    "\n",
    "# 3. Giới hạn biến số (x1, x2 >= 0)\n",
    "x_bounds = (0, None)\n",
    "y_bounds = (0, None)\n",
    "\n",
    "# 4. Giải bài toán\n",
    "res = linprog(c, A_ub=A, b_ub=b, bounds=[x_bounds, y_bounds], method='highs')\n",
    "\n",
    "# 5. Hiển thị kết quả\n",
    "if res.success:\n",
    "    x1, x2 = res.x\n",
    "    max_profit = -res.fun\n",
    "    print(f\"--- KẾT QUẢ TỐI ƯU ---\")\n",
    "    print(f\"Số lượng xe Lead (x1): {round(x1, 2)} chiếc\")\n",
    "    print(f\"Số lượng xe Vision (x2): {round(x2, 2)} chiếc\")\n",
    "    print(f\"Lợi nhuận lớn nhất: {round(max_profit, 2)} triệu đồng\")\n",
    "else:\n",
    "    print(\"Không tìm được phương án tối ưu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a343a-eefe-4b46-921b-24ab449028fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "\n",
    "# Khởi tạo bài toán\n",
    "prob = pulp.LpProblem(\"Toi_uu_loi_nhuan\", pulp.LpMaximize)\n",
    "\n",
    "# Khai báo biến (là số nguyên vì xe máy không bán lẻ được)\n",
    "x1 = pulp.LpVariable('Lead', lowBound=0, cat='Integer')\n",
    "x2 = pulp.LpVariable('Vision', lowBound=0, cat='Integer')\n",
    "\n",
    "# Hàm mục tiêu\n",
    "prob += 5 * x1 + 3.2 * x2\n",
    "\n",
    "# Các ràng buộc\n",
    "prob += 40 * x1 + 30 * x2 <= 36000  # Vốn\n",
    "prob += x1 + x2 <= 1100            # Tổng cầu\n",
    "prob += x1 <= 1.5 * x2             # Tỉ lệ Lead/Vision\n",
    "\n",
    "# Giải\n",
    "prob.solve()\n",
    "\n",
    "print(f\"Kết quả: Lead = {pulp.value(x1)}, Vision = {pulp.value(x2)}\")\n",
    "print(f\"Lợi nhuận tối đa: {pulp.value(prob.objective)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f26d1-9d96-48d7-aa59-4e52fb979318",
   "metadata": {},
   "source": [
    "# Tu lam\n",
    "\n",
    "# Câu 2:  Cho ma trận $A$ và vector $b$ như sau:\n",
    "$A = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -2\n",
    "\\end{pmatrix}$\n",
    "$b = \\begin{pmatrix}\n",
    "-\\frac{12}{5} ,\n",
    "-2024\n",
    "\\end{pmatrix}$\n",
    "Xét bài toán tối ưu:\n",
    "$\\min_{x = (x_1, x_2) \\in \\mathbb{R}^2} f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2 - \\log(1+x_1^2)$\n",
    "\n",
    "## a) Xác định điểm tối ưu $x^\\ast$ và giá trị tối ưu $p^\\ast$\n",
    "\n",
    "## b) Sử dụng thuật toán Gradient Descent, với giá trị $x$ ban đầu là:\n",
    "$x^{(0)} = \\begin{pmatrix}\n",
    "-\\frac{23}{10} \\\\\n",
    "2024\n",
    "\\end{pmatrix}$\n",
    "sử dụng learning rate lần lượt là $\\frac{3}{10}$ và $\\frac{3}{5}$ và thực hiện tối đa 100 vòng lặp. In ra giá trị của $x^{(k)}$, $f(x^{(k)})$ tương ứng sau mỗi vòng lặp $k$ và vẽ đồ thị biểu thị cho sai số $|f(x^{(k)}) - p^\\ast|$ trong hai trường hợp của learning rate. Hãy đưa ra kết luận về sự hội tụ của thuật toán Gradient Descent trong từng trường hợp của learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72991d0c-a869-4357-b65f-a915ba6ee076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c744dcd-5bcd-4a48-97f8-f6825c67f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "\n",
    "from scipy import *\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from autograd import grad\n",
    "from autograd import numpy as anp\n",
    "from scipy.optimize import root\n",
    "def f(x):\n",
    "    \"\"\"Hàm mục tiêu f(x1, x2)\"\"\"\n",
    "    x1, x2 = x[0], x[1]\n",
    "    A = np.array([[1, 0], [0, -2]])\n",
    "    b = np.array([-12/5, -2024])\n",
    "    return 0.5 * ((x1 + 12/5)**2 + (-2*x2 + 2024)**2) - anp.log(1 + x1**2)\n",
    "grad_f = grad(lambda x: f(x))  # đạo hàm tự động của f(x) theo vector x\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def find_optimum(grad_f, f):\n",
    "    \"\"\"\n",
    "    Tìm x* và p* bằng cách tối thiểu hóa ||∇f(x)||²\n",
    "    \"\"\"\n",
    "    x0 = np.zeros(2)\n",
    "    # định nghĩa hàm phụ\n",
    "    def g_norm2(x): \n",
    "        g = grad_f(x)\n",
    "        return np.sum(g**2)\n",
    "    \n",
    "    res = minimize(g_norm2, x0, method='BFGS')\n",
    "    x_star = res.x\n",
    "    p_star = f(x_star)\n",
    "    return x_star, p_star\n",
    "# Gọi hàm tìm nghiệm\n",
    "x_star, p_star = find_optimum(grad_f, f)\n",
    "\n",
    "print(\"\\n--- CÂU 1: KẾT QUẢ TỰ TÍNH (dùng autograd + scipy) ---\")\n",
    "print(f\"x* = {x_star}\")\n",
    "print(f\"p* = f(x*) = {p_star:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9d7a0-96ce-4c1c-954c-7da8ba0409d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "import matplotlib.pyplot as plt\n",
    "def gradient_descent_2d(f, grad_f, x_init, learning_rate, num_iterations, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Thuật toán Gradient Descent tổng quát cho hàm 2 biến\n",
    "    \"\"\"\n",
    "    x = x_init\n",
    "    history = {'x': [x.copy()], 'f': [f(x)], 'gradient': [grad_f(x)], 'step_size': [], 'grad_norm2': [np.linalg.norm(grad_f(x))**2]}\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        g = grad_f(x)\n",
    "        if np.linalg.norm(g) < tolerance:\n",
    "            print(f\"Hội tụ tại vòng lặp {i+1}, ||∇f|| = {np.linalg.norm(g):.2e}\")\n",
    "            break\n",
    "        x_new = x - learning_rate * g\n",
    "        step_size = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(g)\n",
    "        history['step_size'].append(step_size)\n",
    "        history['grad_norm2'].append(np.linalg.norm(grad_f(x))**2)\n",
    "    return history\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def gradient_descent_exact_line_search(f, grad_f, x_init, num_iterations=100, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Gradient Descent 2D với Exact Line Search (tự động tìm learning rate tối ưu tại mỗi bước)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Hàm mục tiêu f(x), với x là vector numpy (2,)\n",
    "    grad_f : callable\n",
    "        Hàm gradient ∇f(x), trả về numpy vector (2,)\n",
    "    x_init : np.array\n",
    "        Điểm khởi tạo, dạng (2,)\n",
    "    num_iterations : int\n",
    "        Số vòng lặp tối đa\n",
    "    tolerance : float\n",
    "        Ngưỡng dừng khi ||grad|| < tolerance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history : dict\n",
    "        Ghi lại toàn bộ quá trình: x, f(x), gradient, step_size, grad_norm2, learning_rate\n",
    "    \"\"\"\n",
    "    x = x_init.copy()\n",
    "    history = {'x': [x.copy()],\n",
    "               'f': [f(x)],\n",
    "               'gradient': [grad_f(x)],\n",
    "               'step_size': [],\n",
    "               'grad_norm2': [np.linalg.norm(grad_f(x))**2],\n",
    "               'learning_rate': []}\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        g = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(g)\n",
    "\n",
    "        if grad_norm < tolerance:\n",
    "            print(f\"Hội tụ tại vòng {i+1}, ||∇f|| = {grad_norm:.2e}\")\n",
    "            break\n",
    "\n",
    "        # --- Exact Line Search ---\n",
    "        def phi(alpha):\n",
    "            return f(x - alpha * g)\n",
    "\n",
    "        res = minimize_scalar(phi, bounds=(0, 5), method='bounded')\n",
    "        alpha_star = res.x  # learning rate tối ưu\n",
    "\n",
    "        # --- Cập nhật ---\n",
    "        x_new = x - alpha_star * g\n",
    "        step_size = np.linalg.norm(x_new - x)\n",
    "\n",
    "        # --- Lưu lịch sử ---\n",
    "        x = x_new\n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(grad_f(x))\n",
    "        history['step_size'].append(step_size)\n",
    "        history['grad_norm2'].append(np.linalg.norm(grad_f(x))**2)\n",
    "        history['learning_rate'].append(alpha_star)\n",
    "\n",
    "    return history\n",
    "\n",
    "x0 = np.array([-23/10, 2024])\n",
    "eta = np.array([3/10, 3/5])\n",
    "iterations = 100\n",
    "history1 = gradient_descent_2d(f, grad_f, x0, eta[0], iterations)\n",
    "history2 = gradient_descent_2d(f, grad_f, x0, eta[1], iterations)\n",
    "print(\"\\n--- Quá trình cập nhật x^(k) cua eta0 ---\")\n",
    "for i, xi in enumerate(history1[\"x\"]):\n",
    "    print(f\"k={i:2d}: x1={xi[0]:.6f}, x2={xi[1]:.6f}, f(x)={f(xi)}\")\n",
    "err_k = np.array(history1['grad_norm2'])  # vì ∇f(x*)=0\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(err_k, marker='o', color='tab:red')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Số vòng lặp k\")\n",
    "plt.ylabel(r\"$\\mathrm{err}_k = ||\\nabla f(x^*) - \\nabla f(x^{(k)})||_2^2$\")\n",
    "plt.title(\"Biểu đồ hội tụ sai số gradient (theo log scale)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"\\n--- Quá trình cập nhật x^(k) cua eta1 ---\")\n",
    "for i, xi in enumerate(history2[\"x\"][:-1]):\n",
    "    print(f\"k={i:2d}: x1={xi[0]:.6f}, x2={xi[1]:.6f}, f(x)={f(xi)}\")\n",
    "err_k1 = np.array(history2['grad_norm2'])  # vì ∇f(x*)=0\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(err_k1, marker='o', color='tab:red')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Số vòng lặp k\")\n",
    "plt.ylabel(r\"$\\mathrm{err}_k = ||\\nabla f(x^*) - \\nabla f(x^{(k)})||_2^2$\")\n",
    "plt.title(\"Biểu đồ hội tụ sai số gradient (theo log scale)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995dcf88-0451-4853-bcdc-7c8191e26c0e",
   "metadata": {},
   "source": [
    "1. Phân tích kết quả với η \n",
    "1\n",
    "​\n",
    " =3/5\n",
    "Tính phân kỳ: Rõ ràng là thuật toán đã phân kỳ (diverge) thay vì hội tụ. Giá trị của hàm mục tiêu f(x) và tọa độ x \n",
    "2\n",
    "​\n",
    "  tăng lên rất nhanh sau mỗi bước lặp.\n",
    "\n",
    "Nguyên nhân: Tốc độ học η \n",
    "1\n",
    "​\n",
    " =3/5=0.6 là quá lớn. Mỗi bước cập nhật, thuật toán đã \"nhảy\" quá xa, vượt qua điểm tối ưu và đi vào một vùng có giá trị hàm mục tiêu lớn hơn, sau đó lại nhảy ngược trở lại và lặp lại quá trình đó. Cứ như vậy, các bước nhảy ngày càng lớn hơn, khiến thuật toán không bao giờ tìm được điểm tối ưu.\n",
    "\n",
    "2. Phân tích kết quả với η \n",
    "0\n",
    "​\n",
    " =3/10\n",
    "Tính hội tụ: Rõ ràng là thuật toán đã hội tụ (converge) về một giá trị tối ưu. Giá trị của hàm mục tiêu f(x) giảm dần và ổn định ở khoảng −2.122585.\n",
    "\n",
    "Tốc độ hội tụ: Thuật toán hội tụ khá nhanh. Chỉ sau 44 vòng lặp, giá trị của gradient norm (chuẩn của vector gradient) đã nhỏ hơn ngưỡng tolerance=1e-8 (8.07e−09), cho thấy thuật toán đã tìm thấy một điểm tối ưu cục bộ rất gần với điểm tối ưu thực sự.\n",
    "\n",
    "Kết luận tổng quát\n",
    "Lựa chọn tốc độ học (Learning Rate) rất quan trọng: Kết quả cho thấy việc chọn một tốc độ học phù hợp là yếu tố then chốt quyết định sự thành công của thuật toán Gradient Descent.\n",
    "\n",
    "Tốc độ học quá lớn: Một tốc độ học quá lớn (như η \n",
    "1\n",
    "​\n",
    " =3/5) sẽ khiến thuật toán không thể hội tụ và thay vào đó là phân kỳ. Nó sẽ \"nhảy\" qua lại trên \"đáy\" của hàm mục tiêu mà không bao giờ đạt tới nó, và thậm chí có thể khiến giá trị hàm mục tiêu tăng lên vô hạn.\n",
    "\n",
    "Tốc độ học phù hợp: Một tốc độ học nhỏ hơn và phù hợp (như η \n",
    "0\n",
    "​\n",
    " =3/10) sẽ giúp thuật toán hội tụ một cách ổn định. Mỗi bước cập nhật sẽ tiến gần hơn đến điểm tối ưu, và cuối cùng sẽ tìm thấy nó trong một số hữu hạn các bước lặp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c603cd2-5448-4f1a-bc26-777fa18567d3",
   "metadata": {},
   "source": [
    "# Câu 3: Cho các vector $ c, a_1, a_2,...,a_m \\in \\mathbb{R}^n $ và các số $ b_1, b_2,...,b_m \\in \\mathbb{R}^n $ (với $ m \\in \\mathbb{Z}^+ $), xét bài toán tối ưu:\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} f(x) = c^T x + \\sum_{i=1}^{m} e^{(b_i - a_i^T x)}   (2)\n",
    "$$\n",
    "a) Tính $(\\nabla f)$ và $(\\nabla^2 f)$ và chứng minh hàm f là hàm lồi trên tập xác định của nó.\n",
    "\n",
    "# b) Cho $m = 4$, $n = 3$ và ma trận $A \\in M_{3\\times4}(\\mathbb{R})$ được xác định như sau:\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0 & -1 & 2 & \\frac{1}{3} \\\\\n",
    "-2 & 4 & -3 & -\\frac{2}{7} \\\\\n",
    "0 & -2 & 5 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Gọi $a_i$ là cột thứ $i$ của $A$ $(i = 1,2,3,4)$,\n",
    "$b = (0, -1, 2, 2)^T$ và\n",
    "$c = (-1+\\frac{7e}{3}, 2-\\frac{23e}{7}, -2+5e)^T$\n",
    "\n",
    "Xác định điểm tối ưu $x^*$ của bài toán (2).\n",
    "\n",
    "## c) Với dữ kiện được cho ở ý b), sử dụng thuật toán Gradient Descent cho bài toán (2) với learning rate được tính theo phương pháp exact line search và điểm khởi tạo $x^{(0)} = (2, 0, 1)^T$, vẽ đồ thị biểu thị cho sai số $err_k = |f(x^{(k)}) - f(x^)|$. Từ đó anh/chị rút ra được kết luận gì?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364c178",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e10b9c-7047-401f-9e88-e11ee2ad31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import grad, hessian\n",
    "from autograd import numpy as anp\n",
    "\n",
    "def create_problem(n, m):\n",
    "    \"\"\"\n",
    "    Tạo ngẫu nhiên các vector và số cần thiết cho bài toán.\n",
    "    Args:\n",
    "        n (int): Kích thước của vector x.\n",
    "        m (int): Số lượng các vector a_i và b_i.\n",
    "    Returns:\n",
    "        tuple: (c, A, b)\n",
    "    \"\"\"\n",
    "    c = np.random.randn(n)\n",
    "    A = np.random.randn(m, n)  # Mỗi hàng là một vector a_i\n",
    "    b = np.random.randn(m)\n",
    "    return c, A, b\n",
    "\n",
    "# Khởi tạo các tham số của bài toán\n",
    "n_dim = 3    # Kích thước của vector x\n",
    "m_terms = 5  # Số lượng các hạng tử\n",
    "c, A, b = create_problem(n_dim, m_terms)\n",
    "\n",
    "# Định nghĩa hàm mục tiêu f(x)\n",
    "def f(x, c, A, b):\n",
    "    \"\"\"\n",
    "    Hàm mục tiêu f(x) = c^T x + sum(exp(b_i - a_i^T x))\n",
    "    \"\"\"\n",
    "    # Sử dụng numpy của autograd\n",
    "    term1 = anp.dot(c.T, x)\n",
    "    \n",
    "    # anp.dot(A, x) tương đương với a_i^T * x cho mỗi a_i trong A\n",
    "    exp_terms = anp.exp(b - anp.dot(A, x))\n",
    "    term2 = anp.sum(exp_terms)\n",
    "    \n",
    "    return term1 + term2\n",
    "\n",
    "# Tạo các hàm tính gradient và hessian tự động bằng autograd\n",
    "# Chúng ta cần gói hàm f để chỉ phụ thuộc vào x\n",
    "f_wrapped = lambda x: f(x, c, A, b)\n",
    "grad_f = grad(f_wrapped)\n",
    "hessian_f = hessian(f_wrapped)\n",
    "\n",
    "# Điểm để tính toán\n",
    "x_test = np.random.randn(n_dim)\n",
    "\n",
    "# --- Tính toán bằng autograd (số học) ---\n",
    "grad_autograd = grad_f(x_test)\n",
    "hessian_autograd = hessian_f(x_test)\n",
    "\n",
    "print(\"--- Kết quả từ Autograd (tính toán số) ---\")\n",
    "print(f\"Gradient (∇f) tại x = {x_test}:\\n{grad_autograd}\\n\")\n",
    "print(f\"Hessian (∇²f) tại x = {x_test}:\\n{hessian_autograd}\\n\")\n",
    "\n",
    "# --- Tính toán bằng công thức giải tích ---\n",
    "# Gradient\n",
    "grad_manual = c - np.dot(A.T, np.exp(b - np.dot(A, x_test)))\n",
    "\n",
    "# Hessian\n",
    "hessian_manual = np.zeros((n_dim, n_dim))\n",
    "for i in range(m_terms):\n",
    "    a_i = A[i, :]\n",
    "    exp_term = np.exp(b[i] - np.dot(a_i, x_test))\n",
    "    hessian_manual += exp_term * np.outer(a_i, a_i)\n",
    "\n",
    "print(\"--- Kết quả từ Công thức giải tích ---\")\n",
    "print(f\"Gradient (∇f) tại x = {x_test}:\\n{grad_manual}\\n\")\n",
    "print(f\"Hessian (∇²f) tại x = {x_test}:\\n{hessian_manual}\\n\")\n",
    "\n",
    "# Kiểm tra xem Hessian có nửa xác định dương hay không\n",
    "eigenvalues = np.linalg.eigvalsh(hessian_autograd)\n",
    "print(\"--- Kiểm tra tính lồi ---\")\n",
    "print(f\"Giá trị riêng của ma trận Hessian:\\n{eigenvalues}\")\n",
    "if np.all(eigenvalues >= 0):\n",
    "    print(\"-> Tất cả giá trị riêng đều không âm, do đó hàm f là lồi.\")\n",
    "else:\n",
    "    print(\"-> Có giá trị riêng âm, hàm f không lồi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6913398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import grad, hessian\n",
    "from autograd import numpy as anp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def solve_convex_problem(f, grad_f, hessian_f, x_init):\n",
    "    \"\"\"\n",
    "    Tìm điểm tối ưu x* của một hàm lồi bằng phương pháp Newton.\n",
    "\n",
    "    Args:\n",
    "        f (function): Hàm mục tiêu.\n",
    "        grad_f (function): Gradient của hàm mục tiêu (từ autograd).\n",
    "        hessian_f (function): Ma trận Hessian của hàm mục tiêu (từ autograd).\n",
    "        x_init (np.array): Điểm khởi tạo ban đầu.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Vector nghiệm tối ưu x*.\n",
    "    \"\"\"\n",
    "    # Sử dụng scipy.optimize.minimize với phương pháp Newton-CG\n",
    "    # Cung cấp gradient (jac) và ma trận Hessian (hess)\n",
    "    result = minimize(f, x_init, method='Newton-CG', jac=grad_f, hess=hessian_f)\n",
    "    \n",
    "    if result.success:\n",
    "        print(\"Tối ưu hóa thành công.\")\n",
    "        return result.x\n",
    "    else:\n",
    "        print(\"Tối ưu hóa thất bại.\")\n",
    "        print(f\"Lý do: {result.message}\")\n",
    "        return None\n",
    "\n",
    "# --- Định nghĩa hàm mục tiêu và các đạo hàm bằng autograd ---\n",
    "def create_problem_data():\n",
    "    \"\"\"\n",
    "    Tạo dữ liệu cho bài toán cụ thể dựa trên giả định:\n",
    "    n=3 (x_vector) và m=4 (số hạng tử exp).\n",
    "    \"\"\"\n",
    "    # Ma trận A phải là 4x3 (m x n) để anp.dot(A,x) hợp lệ\n",
    "    # Mỗi hàng của A là một vector a_i^T\n",
    "    # Đề bài cho a_i là CỘT, nên ta sẽ chuyển vị ma trận đề bài để mỗi HÀNG là a_i^T\n",
    "    A = np.array([\n",
    "        [0, -2, 0],\n",
    "        [-1, 4, -2],\n",
    "        [2, -3, 5],\n",
    "        [1/3, -2/7, 0]\n",
    "    ])\n",
    "    \n",
    "    b = np.array([0, -1, 2, 2])\n",
    "    \n",
    "    # c phải là vector 3 chiều (n)\n",
    "    c = np.array([-1 + 7*np.exp(1)/3, 2 - 23*np.exp(1)/7, -2 + 5*np.exp(1)])\n",
    "    \n",
    "    return A, b, c\n",
    "\n",
    "def f_objective(x, c, A, b):\n",
    "    \"\"\"\n",
    "    Hàm mục tiêu f(x) = c^T x + sum(exp(b_i - a_i^T x))\n",
    "    \"\"\"\n",
    "    term1 = anp.dot(c.T, x)\n",
    "    exp_terms = anp.exp(b - anp.dot(A, x))\n",
    "    term2 = anp.sum(exp_terms)\n",
    "    return term1 + term2\n",
    "\n",
    "# --- Bắt đầu giải bài toán ---\n",
    "print(\"--- ĐANG GIẢI BÀI TOÁN TỐI ƯU ---\")\n",
    "A, b, c = create_problem_data()\n",
    "\n",
    "# Gói hàm mục tiêu để sử dụng trong scipy.optimize\n",
    "f_wrapped = lambda x: f_objective(x, c, A, b)\n",
    "\n",
    "# Tạo các hàm đạo hàm tự động\n",
    "grad_f = grad(f_wrapped)\n",
    "hessian_f = hessian(f_wrapped)\n",
    "\n",
    "# Khởi tạo điểm ban đầu\n",
    "# x là vector 3 chiều, nên x_init có kích thước 3\n",
    "x_init = np.zeros(A.shape[1])  \n",
    "\n",
    "# Tìm điểm tối ưu\n",
    "x_star = solve_convex_problem(f_wrapped, grad_f, hessian_f, x_init)\n",
    "\n",
    "# In kết quả\n",
    "if x_star is not None:\n",
    "    print(\"\\n--- KẾT QUẢ ---\")\n",
    "    print(f\"Điểm tối ưu x* ≈ {x_star.round(6)}\")\n",
    "    print(f\"Giá trị tối ưu f(x*) ≈ {f_wrapped(x_star):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159df331-5e87-4c1a-9068-61044c747f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, minimize_scalar\n",
    "import matplotlib.pyplot as plt\n",
    "# --- Thuật toán Gradient Descent với Exact Line Search ---\n",
    "# ======================================================================\n",
    "# 2. Thuật toán Gradient Descent với Exact Line Search (ELS)\n",
    "# ======================================================================\n",
    "def gradient_descent_els(f, grad_f, x_init, num_iterations, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Thuật toán Gradient Descent với Exact Line Search.\n",
    "\n",
    "    Args:\n",
    "        f (function): Hàm mục tiêu.\n",
    "        grad_f (function): Gradient của hàm mục tiêu.\n",
    "        x_init (np.array): Điểm khởi tạo ban đầu.\n",
    "        num_iterations (int): Số vòng lặp tối đa.\n",
    "        tol (float): Ngưỡng sai số để dừng thuật toán.\n",
    "\n",
    "    Returns:\n",
    "        dict: Lịch sử hội tụ chứa các điểm x và giá trị hàm f(x).\n",
    "    \"\"\"\n",
    "    x = x_init.copy()\n",
    "    history = {'x': [x.copy()], 'f_val': [f(x)]}\n",
    "    \n",
    "    print(\"Bắt đầu thuật toán Gradient Descent với Exact Line Search...\")\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Tính gradient tại điểm hiện tại\n",
    "        g = grad_f(x)\n",
    "        \n",
    "        # Nếu gradient rất nhỏ, thuật toán đã hội tụ\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            print(f\"Hội tụ tại vòng lặp {i+1}. ||∇f|| = {np.linalg.norm(g):.2e}\")\n",
    "            break\n",
    "\n",
    "        # Định nghĩa hàm một biến cho Exact Line Search\n",
    "        # Mục tiêu: tìm eta để tối thiểu hóa f(x - eta * g)\n",
    "        def g_eta(eta):\n",
    "            return f(x - eta * g)\n",
    "\n",
    "        # Sử dụng minimize_scalar để tìm eta tối ưu\n",
    "        # Phương pháp 'bounded' yêu cầu một khoảng tìm kiếm\n",
    "        res = minimize_scalar(g_eta, bounds=(0, 00.1), method='bounded')\n",
    "        eta_k = res.x\n",
    "\n",
    "        # Cập nhật điểm x\n",
    "        x = x - eta_k * g\n",
    "        \n",
    "        # Lưu lịch sử\n",
    "        history['x'].append(x.copy())\n",
    "        history['f_val'].append(f(x))\n",
    "        \n",
    "    print(\"Thuật toán kết thúc.\")\n",
    "    return history\n",
    "\n",
    "# ======================================================================\n",
    "# 3. Chạy thuật toán và vẽ đồ thị\n",
    "# ======================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Lấy dữ liệu bài toán từ câu b)\n",
    "    A, b, c = create_problem_data()\n",
    "    f_wrapped = lambda x: f_objective(x, c, A, b)\n",
    "    grad_f = grad(f_wrapped)\n",
    "    hessian_f = hessian(f_wrapped)\n",
    "\n",
    "    # Bước 1: Tìm điểm tối ưu x* để so sánh và tính sai số\n",
    "    print(\"--- Tìm điểm tối ưu x* bằng phương pháp Newton-CG để so sánh ---\")\n",
    "    x_star_ref = minimize(f_wrapped, np.zeros(A.shape[1]), method='Newton-CG', jac=grad_f, hess=hessian_f).x\n",
    "    f_star_ref = f_wrapped(x_star_ref)\n",
    "    \n",
    "    print(f\"Giá trị tối ưu f(x*) ≈ {f_star_ref:.6f}\")\n",
    "    \n",
    "    # Bước 2: Cấu hình và chạy thuật toán Gradient Descent với ELS\n",
    "    x0 = np.array([2.0, 0.0, 1.0])\n",
    "    iterations = 100\n",
    "    \n",
    "    history_els = gradient_descent_els(f_wrapped, grad_f, x0, iterations)\n",
    "    # --- HIỂN THỊ KẾT QUẢ TẠI MỖI VÒNG LẶP ---\n",
    "    print(\"\\n--- Quá trình cập nhật x^(k) và f(x^(k)) ---\")\n",
    "    \n",
    "    # history_els['x'] có 101 phần tử (điểm khởi tạo + 100 lần cập nhật)\n",
    "    # Lặp qua tất cả các phần tử, bắt đầu từ k=0\n",
    "    for k in range(len(history_els['x'])):\n",
    "        xk = history_els['x'][k]\n",
    "        fxk = history_els['f_val'][k]\n",
    "        \n",
    "        # In ra thông tin của vòng lặp k\n",
    "        print(f\"k={k:2d}: xk = [{xk[0]:.6f}, {xk[1]:.6f}, {xk[2]:.6f}], f(xk) = {fxk:.6f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Bước 3: Tính sai số và vẽ đồ thị\n",
    "    # err_k = |f(x^(k)) - f(x*)|\n",
    "    errors = [abs(f_val - f_star_ref) for f_val in history_els['f_val']]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(errors)), errors, marker='o', linestyle='-')\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Biểu đồ hội tụ của Gradient Descent (ELS)\")\n",
    "    plt.xlabel(\"Số vòng lặp k\")\n",
    "    plt.ylabel(r\"Sai số $err_k = |f(x^{(k)}) - f(x^*)|$ (log scale)\")\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# 4. Rút ra kết luận\n",
    "# ======================================================================\n",
    "\"\"\"\n",
    "Dựa trên đồ thị và quá trình chạy thuật toán, ta có thể rút ra một số kết luận:\n",
    "\n",
    "1.  Hội tụ ổn định và nhanh chóng: Đồ thị cho thấy sai số giảm đều đặn và nhanh chóng theo cấp số nhân (đường thẳng trên đồ thị log). Điều này chứng tỏ thuật toán Gradient Descent với Exact Line Search hoạt động rất hiệu quả và đáng tin cậy cho bài toán tối ưu lồi này.\n",
    "\n",
    "2.  Hiệu quả của Exact Line Search: Bằng cách tìm được tốc độ học tối ưu tại mỗi bước, thuật toán luôn đảm bảo tiến về phía nghiệm tối ưu một cách nhanh nhất có thể theo hướng gradient. Điều này giúp tránh được các vấn đề như phân kỳ (khi learning rate quá lớn) hay hội tụ chậm (khi learning rate quá nhỏ) như khi sử dụng một tốc độ học cố định.\n",
    "\n",
    "3.  Độ chính xác cao: Sai số giảm đến mức rất nhỏ sau một số ít vòng lặp, cho thấy thuật toán đã tìm được một nghiệm rất gần với nghiệm tối ưu thực sự.\n",
    "\n",
    "Tóm lại, đối với các hàm lồi như bài toán này, việc sử dụng Gradient Descent kết hợp với phương pháp Exact Line Search là một chiến lược tối ưu hiệu quả và mạnh mẽ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd551b7b-3b88-42f0-88ab-4cc1160cee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# BÀI (2): f(x) = c^T x + sum_i exp(b_i - a_i^T x)\n",
    "# Giải các yêu cầu (a), (b), (c)\n",
    "# ===============================================================\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# (a) Hàm mục tiêu, Gradient, Hessian (tổng quát)\n",
    "# ---------------------------------------------------------------\n",
    "def f_objective(x, c, A, b):\n",
    "    \"\"\"\n",
    "    f(x) = c^T x + sum_i exp(b_i - a_i^T x)\n",
    "    A: shape (n, m) chứa các cột a_i (m cột, mỗi cột dài n)\n",
    "    b: shape (m,)\n",
    "    c: shape (n,)\n",
    "    x: shape (n,)\n",
    "    \"\"\"\n",
    "    Ax = A.T @ x                # shape (m,)\n",
    "    s = np.exp(b - Ax)          # shape (m,)\n",
    "    return float(c @ x + np.sum(s))\n",
    "\n",
    "def grad_f(x, c, A, b):\n",
    "    \"\"\"\n",
    "    ∇f(x) = c - A * s, với s_i = exp(b_i - a_i^T x)\n",
    "    \"\"\"\n",
    "    Ax = A.T @ x\n",
    "    s = np.exp(b - Ax)          # (m,)\n",
    "    return c - A @ s            # (n,)\n",
    "\n",
    "def hess_f(x, A, b):\n",
    "    \"\"\"\n",
    "    ∇²f(x) = A * diag(s) * A^T, với s_i = exp(b_i - a_i^T x)\n",
    "    \"\"\"\n",
    "    Ax = A.T @ x\n",
    "    s = np.exp(b - Ax)          # (m,)\n",
    "    # A @ diag(s) @ A^T == A @ (s[:,None] * A^T)^T == A @ (s[:,None] * A.T).T\n",
    "    # Cách gọn: A @ (diag(s)) @ A^T = A @ (s[:, None] * A.T).T = A @ (A * s)  (nhưng rõ ràng nhất vẫn là nhân trực tiếp)\n",
    "    return A @ (s[:, None] * A.T)\n",
    "\n",
    "def check_convexity_by_sampling(c, A, b, n_checks=40, tol=1e-10, seed=0):\n",
    "    \"\"\"\n",
    "    Kiểm tra tính lồi bằng cách lấy nhiều điểm ngẫu nhiên x và\n",
    "    xác minh eigenvalues(Hessian) >= -tol\n",
    "    (tổng quát cho n chiều).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = A.shape[0]\n",
    "    min_eig_global = +np.inf\n",
    "    for _ in range(n_checks):\n",
    "        x = rng.normal(size=n)\n",
    "        H = hess_f(x, A, b)\n",
    "        # Đối xứng hoá số học (đề phòng sai số số học)\n",
    "        H = 0.5 * (H + H.T)\n",
    "        eigvals = np.linalg.eigvalsh(H)\n",
    "        min_eig_global = min(min_eig_global, eigvals.min())\n",
    "        if eigvals.min() < -tol:\n",
    "            return False, min_eig_global\n",
    "    return True, min_eig_global\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# (b) Dữ liệu cụ thể, tìm x*\n",
    "# ---------------------------------------------------------------\n",
    "def create_problem_data():\n",
    "    \"\"\"\n",
    "    Tạo dữ liệu phần (b) theo đề: m=4, n=3.\n",
    "    Ma trận A (3x4) với các cột a_i, b in R^4, c in R^3.\n",
    "    \"\"\"\n",
    "    A = np.array([\n",
    "        [ 0.0,   -1.0,   2.0,  1.0/3.0],\n",
    "        [-2.0,    4.0,  -3.0, -2.0/7.0],\n",
    "        [ 0.0,   -2.0,   5.0,  0.0     ]\n",
    "    ])  # shape (3,4)  (n=3, m=4)\n",
    "\n",
    "    b = np.array([0.0, -1.0, 2.0, 2.0])   # (m,)\n",
    "    c = np.array([\n",
    "        -1.0 + 7.0*np.e/3.0,\n",
    "         2.0 - 23.0*np.e/7.0,\n",
    "        -2.0 + 5.0*np.e\n",
    "    ])  # (n,)\n",
    "\n",
    "    return A, b, c\n",
    "\n",
    "def solve_global_minimizer(c, A, b, x0=None):\n",
    "    \"\"\"\n",
    "    Tìm nghiệm tối ưu x* (toàn cục vì f lồi).\n",
    "    Dùng BFGS (có cung cấp gradient).\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(n)\n",
    "\n",
    "    res = minimize(lambda x: f_objective(x, c, A, b),\n",
    "                   x0,\n",
    "                   jac=lambda x: grad_f(x, c, A, b),\n",
    "                   method=\"BFGS\")\n",
    "\n",
    "    if not res.success:\n",
    "        print(\"⚠️ Cảnh báo solver:\", res.message)\n",
    "    return res.x, res.fun\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# (c) Gradient Descent với Exact Line Search + đồ thị sai số\n",
    "# ---------------------------------------------------------------\n",
    "def gradient_descent_exact_line_search(f, grad, x_init, max_iter, c, A, b,\n",
    "                                       bounds=(0.0, 1.0), tol=1e-8, verbose=False):\n",
    "    \"\"\"\n",
    "    GD với exact line search:\n",
    "    Ở bước k, tìm eta_k = argmin_{eta in bounds} f(x_k - eta * grad(x_k))\n",
    "    rồi x_{k+1} = x_k - eta_k * grad(x_k)\n",
    "\n",
    "    f, grad: callable f(x,c,A,b), grad(x,c,A,b)\n",
    "    bounds: khoảng tìm kiếm cho eta (nên vừa phải để tránh overflow exp)\n",
    "    \"\"\"\n",
    "    x = x_init.copy()\n",
    "    history = {\"x\": [x.copy()], \"f\": [f(x, c, A, b)], \"eta\": []}\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        g = grad(x, c, A, b)\n",
    "        g_norm = np.linalg.norm(g)\n",
    "        if g_norm < tol:\n",
    "            if verbose:\n",
    "                print(f\"Hội tụ tại vòng {k}: ||∇f|| = {g_norm:.2e}\")\n",
    "            break\n",
    "\n",
    "        def phi(eta):\n",
    "            return f(x - eta*g, c, A, b)\n",
    "\n",
    "        # exact line search theo khoảng bounds\n",
    "        res = minimize_scalar(phi, bounds=bounds, method=\"bounded\")\n",
    "        if not res.success:\n",
    "            # fallback nhẹ: giảm bounds và thử lại 1 lần\n",
    "            res = minimize_scalar(phi, bounds=(0.0, 0.2), method=\"bounded\")\n",
    "        eta_k = float(res.x)\n",
    "\n",
    "        x = x - eta_k * g\n",
    "        history[\"x\"].append(x.copy())\n",
    "        history[\"f\"].append(f(x, c, A, b))\n",
    "        history[\"eta\"].append(eta_k)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"k={k:2d}: eta={eta_k:.4e}, f={history['f'][-1]:.6f}, ||g||={g_norm:.3e}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# ===============================================================\n",
    "# Chạy toàn bộ (a), (b), (c)\n",
    "# ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ---------------- (a) Công thức & kiểm tra lồi ----------------\n",
    "    print(\"=== (a) Gradient, Hessian & tính lồi (tổng quát) ===\")\n",
    "    # Lý thuyết:\n",
    "    # ∇f(x)   = c - A s,  s_i = exp(b_i - a_i^T x)\n",
    "    # ∇²f(x)  = A diag(s) A^T  (PSD vì s_i > 0 và a_i a_i^T là PSD)\n",
    "    # ⇒ f lồi trên R^n.\n",
    "\n",
    "    # Kiểm tra số ngẫu nhiên với dữ liệu bất kỳ\n",
    "    A_tmp, b_tmp, c_tmp = create_problem_data()\n",
    "    is_psd, min_eig = check_convexity_by_sampling(c_tmp, A_tmp, b_tmp, n_checks=40)\n",
    "    print(f\"Kiểm tra số (random sampling): PSD? {'YES' if is_psd else 'NO'}; min eigen ≈ {min_eig:.4e}\\n\")\n",
    "\n",
    "    # ---------------- (b) Tìm x* với dữ liệu đề bài ----------------\n",
    "    print(\"=== (b) Tìm nghiệm tối ưu x* ===\")\n",
    "    A, b, c = create_problem_data()\n",
    "    x_star, f_star = solve_global_minimizer(c, A, b, x0=np.zeros(A.shape[0]))\n",
    "    print(f\"x* = {x_star}\")\n",
    "    print(f\"f(x*) = {f_star:.6f}\\n\")\n",
    "\n",
    "    # ---------------- (c) GD + Exact Line Search & đồ thị sai số ---------------\n",
    "    print(\"=== (c) GD với Exact Line Search, x0 = (2,0,1)^T, vẽ err_k ===\")\n",
    "    x0 = np.array([2.0, 0.0, 1.0])\n",
    "    max_iter = 30\n",
    "\n",
    "    # Chạy GD(ELS)\n",
    "    hist = gradient_descent_exact_line_search(\n",
    "        f=f_objective,\n",
    "        grad=grad_f,\n",
    "        x_init=x0,\n",
    "        max_iter=max_iter,\n",
    "        c=c, A=A, b=b,\n",
    "        bounds=(0.0, 1.0),   # khoảng tìm eta; có thể siết nhỏ nếu overflow\n",
    "        tol=1e-10,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Tính sai số |f(x^k) - f(x*)|\n",
    "    f_vals = np.array(hist[\"f\"])\n",
    "    err = np.abs(f_vals - f_star)\n",
    "\n",
    "    # In vài dòng quá trình\n",
    "    print(\"\\nMột vài vòng lặp tiêu biểu:\")\n",
    "    for k in [0, 1, 2, 5, 10, len(f_vals)-1]:\n",
    "        if 0 <= k < len(f_vals):\n",
    "            print(f\"k={k:2d}: f(x^k)={f_vals[k]:.6f}, err_k={err[k]:.6e}\")\n",
    "\n",
    "    # Vẽ đồ thị lỗi (log-scale)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(range(len(err)), err, marker='o')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Số vòng lặp k\")\n",
    "    plt.ylabel(r\"$\\mathrm{err}_k = |f(x^{(k)}) - f(x^*)|$\")\n",
    "    plt.title(\"Hội tụ của GD với Exact Line Search\")\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Nhận xét tự động\n",
    "    drop = err[0]/max(err[-1], 1e-300)\n",
    "    print(\"\\n=== Nhận xét ===\")\n",
    "    print(f\"Giảm sai số tổng: ~{drop:.2e} lần.\")\n",
    "    if err[-1] < 1e-6:\n",
    "        print(\"✔ Thuật toán hội tụ rất tốt với Exact Line Search (đường lỗi giảm đều trên thang log).\")\n",
    "    elif err[-1] < 1e-3:\n",
    "        print(\"✔ Hội tụ ổn; có thể tăng số vòng lặp để đạt chính xác hơn.\")\n",
    "    else:\n",
    "        print(\"⚠ Hội tụ còn chậm / dừng sớm; thử siết bounds của eta nhỏ hơn (ví dụ (0, 0.3)) hoặc tăng max_iter.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77af28-3d18-4ec5-8313-dddadb82fad4",
   "metadata": {},
   "source": [
    "### Ví dụ 1: Hàm một biến ###\n",
    "Xét hàm số:\n",
    "$$\n",
    "f(x) = e^{2x} + 3x^2.\n",
    "$$\n",
    "\n",
    "**Bước 1.** Tính đạo hàm bậc nhất và bậc hai:\n",
    "$$\n",
    "f'(x) = 2e^{2x} + 6x, \\qquad f''(x) = 4e^{2x} + 6.\n",
    "$$\n",
    "\n",
    "**Bước 2.** Kiểm tra dấu của đạo hàm bậc hai:\n",
    "Vì $e^{2x} > 0$ với mọi $x \\in \\mathbb{R}$, ta có:\n",
    "$$\n",
    "f''(x) = 4e^{2x} + 6 > 0, \\quad \\forall x \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "**Kết luận:** $f''(x) > 0$ nên $f(x)$ là **hàm lồi** trên toàn bộ $\\mathbb{R}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa01e0-4c40-4be9-8250-022782fd1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_problem(n, m):\n",
    "    \"\"\"\n",
    "    Tạo ngẫu nhiên các vector và số cần thiết cho bài toán.\n",
    "    Args:\n",
    "        n (int): Kích thước của vector x.\n",
    "        m (int): Số lượng các vector a_i và b_i.\n",
    "    Returns:\n",
    "        tuple: (c, A, b)\n",
    "    \"\"\"\n",
    "    c = np.random.randn(n)\n",
    "    A = np.random.randn(m, n)  # Mỗi hàng là một vector a_i\n",
    "    b = np.random.randn(m)\n",
    "    return c, A, b\n",
    "\n",
    "# Khởi tạo các tham số của bài toán\n",
    "n_dim = 1    # Kích thước của vector x\n",
    "m_terms = 5  # Số lượng các hạng tử\n",
    "c, A, b = create_problem(n_dim, m_terms)\n",
    "\n",
    "# Định nghĩa hàm mục tiêu f(x)\n",
    "def f(x, c, A, b):\n",
    "    \"\"\"\n",
    "    Hàm mục tiêu f(x) = c^T x + sum(exp(b_i - a_i^T x))\n",
    "    \"\"\"\n",
    "    # Sử dụng numpy của autograd\n",
    "    term1 = anp.dot(c.T, x)\n",
    "    \n",
    "    # anp.dot(A, x) tương đương với a_i^T * x cho mỗi a_i trong A\n",
    "    exp_terms = anp.exp(b - anp.dot(A, x))\n",
    "    term2 = anp.sum(exp_terms)\n",
    "    \n",
    "    return term1 + term2\n",
    " \n",
    "# Tạo các hàm tính gradient và hessian tự động bằng autograd\n",
    "# Chúng ta cần gói hàm f để chỉ phụ thuộc vào x\n",
    "f_wrapped = lambda x: f(x, c, A, b)\n",
    "grad_f = grad(f_wrapped)\n",
    "hessian_f = hessian(f_wrapped)\n",
    "\n",
    "# Điểm để tính toán\n",
    "x_test = np.random.randn(n_dim)\n",
    "\n",
    "# --- Tính toán bằng autograd (số học) ---\n",
    "grad_autograd = grad_f(x_test)\n",
    "hessian_autograd = hessian_f(x_test)\n",
    "\n",
    "print(\"--- Kết quả từ Autograd (tính toán số) ---\")\n",
    "print(f\"Gradient (∇f) tại x = {x_test}:\\n{grad_autograd}\\n\")\n",
    "print(f\"Hessian (∇²f) tại x = {x_test}:\\n{hessian_autograd}\\n\")\n",
    "\n",
    "# --- Tính toán bằng công thức giải tích ---\n",
    "# Gradient\n",
    "grad_manual = c - np.dot(A.T, np.exp(b - np.dot(A, x_test)))\n",
    "\n",
    "# Hessian\n",
    "hessian_manual = np.zeros((n_dim, n_dim))\n",
    "for i in range(m_terms):\n",
    "    a_i = A[i, :]\n",
    "    exp_term = np.exp(b[i] - np.dot(a_i, x_test))\n",
    "    hessian_manual += exp_term * np.outer(a_i, a_i)\n",
    "\n",
    "print(\"--- Kết quả từ Công thức giải tích ---\")\n",
    "print(f\"Gradient (∇f) tại x = {x_test}:\\n{grad_manual}\\n\")\n",
    "print(f\"Hessian (∇²f) tại x = {x_test}:\\n{hessian_manual}\\n\")\n",
    "\n",
    "# Kiểm tra xem Hessian có nửa xác định dương hay không\n",
    "eigenvalues = np.linalg.eigvalsh(hessian_autograd)\n",
    "print(\"--- Kiểm tra tính lồi ---\")\n",
    "print(f\"Giá trị riêng của ma trận Hessian:\\n{eigenvalues}\")\n",
    "if np.all(eigenvalues >= 0):\n",
    "    print(\"-> Tất cả giá trị riêng đều không âm, do đó hàm f là lồi.\")\n",
    "else:\n",
    "    print(\"-> Có giá trị riêng âm, hàm f không lồi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538df0d7",
   "metadata": {},
   "source": [
    "### Ví dụ 2: Hàm hai biến ###\n",
    "Xét hàm:\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 3x_2^2 + 2x_1x_2 + 5x_1 - 4x_2.\n",
    "$$\n",
    "\n",
    "**Bước 1.** Viết lại dưới dạng ma trận:\n",
    "$$\n",
    "f(x) = \\frac{1}{2} x^T Q x + b^T x + c,\n",
    "$$\n",
    "với\n",
    "$$\n",
    "Q = \\begin{bmatrix} 2 & 2 \\\\ 2 & 6 \\end{bmatrix}, \\quad\n",
    "b = \\begin{bmatrix} 5 \\\\ -4 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**Bước 2.** Hessian của $f$ là:\n",
    "$$\n",
    "\\nabla^2 f(x) = Q = \n",
    "\\begin{bmatrix}\n",
    "2 & 2 \\\\\n",
    "2 & 6\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**Bước 3.** Kiểm tra xác định dương của $Q$:\n",
    "- Các định thức con chính:\n",
    "$$\n",
    "\\det(Q_1) = 2 > 0, \\quad\n",
    "\\det(Q_2) = \n",
    "\\begin{vmatrix}\n",
    "2 & 2 \\\\ 2 & 6\n",
    "\\end{vmatrix} = 8 > 0.\n",
    "$$\n",
    "Vì các định thức con chính đều dương, nên $Q \\succ 0$ (xác định dương).\n",
    "\n",
    "**Kết luận:** Hessian $\\nabla^2 f(x) = Q \\succ 0$ nên $f(x_1, x_2)$ là **hàm lồi** trên $\\mathbb{R}^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hessian matrix Q\n",
    "Q = np.array([\n",
    "    [2, 2],\n",
    "    [2, 6]\n",
    "], dtype=float)\n",
    "\n",
    "# ---- Kiểm tra xác định dương ----\n",
    "\n",
    "# điều kiện 1: eigenvalues > 0\n",
    "eigvals = np.linalg.eigvals(Q)\n",
    "\n",
    "print(\"Eigenvalues of Q:\", eigvals)\n",
    "\n",
    "if np.all(eigvals > 0):\n",
    "    print(\"→ Tất cả eigenvalues > 0 → Q là xác định dương → f lồi.\")\n",
    "else:\n",
    "    print(\"→ Tồn tại eigenvalue ≤ 0 → không xác định dương.\")\n",
    "\n",
    "# điều kiện 2: check leading principal minors\n",
    "det1 = np.linalg.det(Q[:1, :1])\n",
    "det2 = np.linalg.det(Q)\n",
    "\n",
    "print(\"\\nDeterminants of leading principal minors:\")\n",
    "print(\"det(Q1) =\", det1)\n",
    "print(\"det(Q2) =\", det2)\n",
    "\n",
    "if det1 > 0 and det2 > 0:\n",
    "    print(\"→ Hai định thức con chính dương → Q xác định dương → hàm lồi.\")\n",
    "else:\n",
    "    print(\"→ Điều kiện định thức con chính không thỏa → không xác định dương.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0756c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5f753d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5286dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2ee83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dec9eb60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559bdb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
