{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BÀI TẬP THỰC HÀNH: GRADIENT DESCENT VỚI CÁC HÀM SỐ KHÁC NHAU\n",
    "\n",
    "## Mục tiêu\n",
    "- Hiểu sâu về thuật toán Gradient Descent\n",
    "- Thực hành triển khai thuật toán từ đầu\n",
    "- So sánh hiệu quả của các learning rate khác nhau\n",
    "- Phân tích quá trình hội tụ trên các hàm số khác nhau\n",
    "\n",
    "## Lý thuyết cơ bản\n",
    "\n",
    "Gradient Descent là thuật toán tối ưu hóa cơ bản nhất, sử dụng công thức cập nhật:\n",
    "\n",
    "$$x_{new} = x_{old} - \\alpha \\nabla f(x_{old})$$\n",
    "\n",
    "Trong đó:\n",
    "- $\\alpha$ là learning rate (tốc độ học)\n",
    "- $\\nabla f(x)$ là gradient (đạo hàm) của hàm số tại điểm $x$\n",
    "\n",
    "### Các yếu tố ảnh hưởng đến hiệu quả:\n",
    "1. **Learning rate ($\\alpha$)**: Quá lớn → nhảy dốc, quá nhỏ → hội tụ chậm\n",
    "2. **Điểm khởi tạo**: Ảnh hưởng đến cực trị địa phương tìm được\n",
    "3. **Dạng hàm số**: Lồi vs không lồi, smooth vs non-smooth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cấu hình matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phần 1: Triển khai Gradient Descent cơ bản\n",
    "\n",
    "### Bài tập 1.1: Hàm bậc hai đơn giản\n",
    "Cho hàm số $f(x) = (x-3)^2 + 2$\n",
    "\n",
    "**Yêu cầu:**\n",
    "1. Tính đạo hàm của hàm số\n",
    "2. Triển khai thuật toán Gradient Descent\n",
    "3. Tìm điểm cực tiểu với các learning rate khác nhau\n",
    "\n",
    "**Lý thuyết:**\n",
    "Đây là hàm lồi đơn giản có cực tiểu toàn cục tại $x^* = 3$ với $f(x^*) = 2$.\n",
    "Đạo hàm: $f'(x) = 2(x-3)$\n",
    "\n",
    "Điều kiện hội tụ: $0 < \\alpha < \\frac{2}{L}$ với $L$ là Lipschitz constant của gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    \"\"\"Hàm số f(x) = (x-3)^2 + 2\"\"\"\n",
    "    return (x - 3)**2 + 2\n",
    "\n",
    "def df1_dx(x):\n",
    "    \"\"\"Đạo hàm f'(x) = 2(x-3)\"\"\"\n",
    "    # TODO: Đạo hàm của (x-3)^2 + 2 là \n",
    "    return 2 * (x-3)\n",
    "\n",
    "def gradient_descent_1d(f, df_dx, x_init, learning_rate, num_iterations, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Thuật toán Gradient Descent cho hàm 1 biến\n",
    "    \n",
    "    Args:\n",
    "        f: Hàm mục tiêu\n",
    "        df_dx: Đạo hàm của hàm mục tiêu\n",
    "        x_init: Điểm khởi tạo\n",
    "        learning_rate: Tốc độ học\n",
    "        num_iterations: Số vòng lặp tối đa\n",
    "        tolerance: Ngưỡng dừng (khi gradient đủ nhỏ)\n",
    "    \n",
    "    Returns:\n",
    "        history: Lịch sử các điểm x, giá trị hàm số và gradient\n",
    "    \"\"\"\n",
    "    x = x_init\n",
    "    history = {\n",
    "        'x': [x], \n",
    "        'f': [f(x)], \n",
    "        'gradient': [df_dx(x)],\n",
    "        'step_size': []\n",
    "    }\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # TODO: Tính gradient tại điểm hiện tại\n",
    "        gradient = df_dx(x)\n",
    "        \n",
    "        # Kiểm tra điều kiện dừng\n",
    "        if abs(gradient) < tolerance:\n",
    "            print(f\"Hội tụ tại iteration {i+1}, gradient = {gradient:.2e}\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Cập nhật x theo công thức: x_new = x_old - learning_rate * gradient\n",
    "        x_new = x - learning_rate * gradient\n",
    "        step_size = abs(x_new - x)\n",
    "        \n",
    "        x = x_new\n",
    "        \n",
    "        # Lưu lịch sử\n",
    "        history['x'].append(x)\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(gradient)\n",
    "        history['step_size'].append(step_size)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Test với hàm đơn giản\n",
    "print(\"Test Gradient Descent cơ bản:\")\n",
    "history_test = gradient_descent_1d(f1, df1_dx, x_init=0, learning_rate=0.1, num_iterations=20)\n",
    "print(f\"Điểm cuối: x = {history_test['x'][-1]:.6f}\")\n",
    "print(f\"Giá trị hàm: f(x) = {history_test['f'][-1]:.6f}\")\n",
    "print(f\"Gradient cuối: {history_test['gradient'][-1]:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài tập 1.2: Phân tích chi tiết các Learning Rate\n",
    "\n",
    "**Mục tiêu:** Hiểu sâu về ảnh hưởng của learning rate đến quá trình hội tụ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử nghiệm với các learning rate khác nhau\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.5, 0.8, 1.0, 1.2]\n",
    "x_init = 0\n",
    "num_iterations = 50\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Vẽ hàm số gốc\n",
    "x_range = np.linspace(-2, 8, 1000)\n",
    "y_range = f1(x_range)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x)', alpha=0.7)\n",
    "    \n",
    "    # Chạy gradient descent\n",
    "    history = gradient_descent_1d(f1, df1_dx, x_init, lr, num_iterations)\n",
    "    \n",
    "    # Vẽ quá trình hội tụ\n",
    "    plt.plot(history['x'], history['f'], 'ro-', markersize=4, linewidth=1, label='GD path')\n",
    "    plt.plot(history['x'][0], history['f'][0], 'go', markersize=8, label='Start')\n",
    "    plt.plot(history['x'][-1], history['f'][-1], 'rs', markersize=8, label='End')\n",
    "    \n",
    "    # Vẽ điểm tối ưu thực\n",
    "    plt.plot(3, 2, 'k*', markersize=12, label='True optimum')\n",
    "    \n",
    "    plt.title(f'Learning Rate = {lr}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(1.5, max(15, max(history['f'])))\n",
    "    \n",
    "    # Lưu kết quả để phân tích\n",
    "    final_error = abs(history['x'][-1] - 3)\n",
    "    convergence_rate = len(history['x']) - 1\n",
    "    results_summary.append({\n",
    "        'lr': lr,\n",
    "        'final_x': history['x'][-1],\n",
    "        'final_error': final_error,\n",
    "        'iterations': convergence_rate,\n",
    "        'converged': final_error < 1e-6\n",
    "    })\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Phân tích kết quả\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHÂN TÍCH KẾT QUẢ LEARNING RATE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'LR':<6} {'Final X':<12} {'Error':<12} {'Iterations':<12} {'Converged':<12}\")\n",
    "print(\"-\"*80)\n",
    "for result in results_summary:\n",
    "    print(f\"{result['lr']:<6} {result['final_x']:<12.6f} {result['final_error']:<12.2e} \"\n",
    "          f\"{result['iterations']:<12} {result['converged']:<12}\")\n",
    "print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phần 2: Gradient Descent cho Hàm Đa Biến\n",
    "\n",
    "### Bài tập 2.1: Hàm bậc hai đa biến (Quadratic Function)\n",
    "\n",
    "**Lý thuyết:**\n",
    "Hàm bậc hai đa biến có dạng tổng quát:\n",
    "$$f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T A \\mathbf{x} + \\mathbf{b}^T\\mathbf{x} + c$$\n",
    "\n",
    "Gradient: $\\nabla f(\\mathbf{x}) = A\\mathbf{x} + \\mathbf{b}$\n",
    "\n",
    "**Ví dụ cụ thể:** $f(x_1, x_2) = x_1^2 + 2x_2^2 + x_1x_2 - 4x_1 - 6x_2 + 5$\n",
    "\n",
    "Gradient: \n",
    "- $\\frac{\\partial f}{\\partial x_1} = 2x_1 + x_2 - 4$\n",
    "- $\\frac{\\partial f}{\\partial x_2} = 4x_2 + x_1 - 6$\n",
    "\n",
    "**Mục tiêu:** Tìm điểm cực tiểu và phân tích ảnh hưởng của condition number của ma trận Hessian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    \"\"\"Hàm bậc hai đa biến: f(x1,x2) = x1^2 + 2*x2^2 + x1*x2 - 4*x1 - 6*x2 + 5\"\"\"\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return x1**2 + 2*x2**2 + x1*x2 - 4*x1 - 6*x2 + 5\n",
    "\n",
    "def grad_f2(x):\n",
    "    \"\"\"Gradient của f2\"\"\"\n",
    "    x1, x2 = x[0], x[1]\n",
    "    # TODO: Gradient của f(x1,x2) = x1^2 + 2*x2^2 + x1*x2 - 4*x1 - 6*x2 + 5\n",
    "    # ∂f/∂x1 = 2*x1 + x2 - 4\n",
    "    # ∂f/∂x2 = 4*x2 + x1 - 6\n",
    "    grad_x1 = 2*x1 + x2 - 4\n",
    "    grad_x2 = 4*x2 + x1 - 6\n",
    "    return np.array([grad_x1, grad_x2])\n",
    "\n",
    "def gradient_descent_2d(f, grad_f, x_init, learning_rate, num_iterations, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Gradient Descent cho hàm 2 biến\n",
    "    \n",
    "    Args:\n",
    "        f: Hàm mục tiêu\n",
    "        grad_f: Gradient của hàm mục tiêu\n",
    "        x_init: Điểm khởi tạo [x1, x2]\n",
    "        learning_rate: Tốc độ học\n",
    "        num_iterations: Số vòng lặp tối đa\n",
    "        tolerance: Ngưỡng dừng\n",
    "    \n",
    "    Returns:\n",
    "        history: Lịch sử quá trình tối ưu\n",
    "    \"\"\"\n",
    "    x = np.array(x_init, dtype=float)\n",
    "    history = {\n",
    "        'x': [x.copy()],\n",
    "        'f': [f(x)],\n",
    "        'gradient': [grad_f(x).copy()],\n",
    "        'gradient_norm': [np.linalg.norm(grad_f(x))]\n",
    "    }\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # TODO: Tính gradient tại điểm hiện tại\n",
    "        gradient = grad_f(x)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        \n",
    "        # Kiểm tra điều kiện dừng\n",
    "        if gradient_norm < tolerance:\n",
    "            print(f\"Hội tụ tại iteration {i+1}, ||gradient|| = {gradient_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # TODO: Cập nhật x theo công thức: x_new = x_old - learning_rate * gradient\n",
    "        x_new = x - learning_rate * gradient\n",
    "        x = x_new\n",
    "        \n",
    "        # Lưu lịch sử\n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "        history['gradient'].append(gradient.copy())\n",
    "        history['gradient_norm'].append(gradient_norm)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Test hàm 2D\n",
    "print(\"Test Gradient Descent 2D:\")\n",
    "x_init = [0, 0]\n",
    "history_2d = gradient_descent_2d(f2, grad_f2, x_init, learning_rate=0.1, num_iterations=100)\n",
    "print(f\"Điểm cuối: x = [{history_2d['x'][-1][0]:.6f}, {history_2d['x'][-1][1]:.6f}]\")\n",
    "print(f\"Giá trị hàm: f(x) = {history_2d['f'][-1]:.6f}\")\n",
    "print(f\"Norm gradient: ||∇f|| = {history_2d['gradient_norm'][-1]:.2e}\")\n",
    "\n",
    "# Tính nghiệm chính xác bằng giải tích\n",
    "# Giải hệ phương trình ∇f = 0\n",
    "A = np.array([[2, 1], [1, 4]])\n",
    "b = np.array([-4, -6])\n",
    "x_optimal = np.linalg.solve(A, -b)\n",
    "f_optimal = f2(x_optimal)\n",
    "print(f\"\\nNghiệm chính xác: x* = [{x_optimal[0]:.6f}, {x_optimal[1]:.6f}]\")\n",
    "print(f\"Giá trị tối ưu: f(x*) = {f_optimal:.6f}\")\n",
    "print(f\"Sai số: ||x - x*|| = {np.linalg.norm(history_2d['x'][-1] - x_optimal):.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài tập 2.2: Trực quan hóa quá trình hội tụ 2D\n",
    "\n",
    "**Mục tiêu:** Tạo contour plot để hiển thị đường đi của thuật toán trên bề mặt hàm số\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_2d(f, grad_f, x_init_list, learning_rates, num_iterations=50):\n",
    "    \"\"\"\n",
    "    Vẽ quá trình hội tụ của Gradient Descent trên contour plot\n",
    "    \"\"\"\n",
    "    # Tạo lưới điểm để vẽ contour\n",
    "    x1_range = np.linspace(-2, 4, 100)\n",
    "    x2_range = np.linspace(-1, 3, 100)\n",
    "    X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "    Z = np.zeros_like(X1)\n",
    "    \n",
    "    for i in range(X1.shape[0]):\n",
    "        for j in range(X1.shape[1]):\n",
    "            Z[i, j] = f([X1[i, j], X2[i, j]])\n",
    "    \n",
    "    # Tạo subplot cho các learning rate khác nhau\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for idx, lr in enumerate(learning_rates[:4]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Vẽ contour\n",
    "        contour = ax.contour(X1, X2, Z, levels=20, alpha=0.6, colors='gray')\n",
    "        ax.clabel(contour, inline=True, fontsize=8)\n",
    "        \n",
    "        # Vẽ đường đi cho các điểm khởi tạo khác nhau\n",
    "        for i, x_init in enumerate(x_init_list):\n",
    "            history = gradient_descent_2d(f, grad_f, x_init, lr, num_iterations)\n",
    "            \n",
    "            # Chuyển đổi lịch sử thành arrays\n",
    "            x_path = np.array(history['x'])\n",
    "            \n",
    "            # Vẽ đường đi\n",
    "            ax.plot(x_path[:, 0], x_path[:, 1], 'o-', \n",
    "                   color=colors[i % len(colors)], linewidth=2, markersize=4,\n",
    "                   label=f'Start: ({x_init[0]}, {x_init[1]})')\n",
    "            \n",
    "            # Đánh dấu điểm bắt đầu và kết thúc\n",
    "            ax.plot(x_init[0], x_init[1], 's', color=colors[i % len(colors)], \n",
    "                   markersize=8, markeredgecolor='black')\n",
    "            ax.plot(x_path[-1, 0], x_path[-1, 1], '*', color=colors[i % len(colors)], \n",
    "                   markersize=12, markeredgecolor='black')\n",
    "        \n",
    "        # Đánh dấu điểm tối ưu thực\n",
    "        A = np.array([[2, 1], [1, 4]])\n",
    "        b = np.array([-4, -6])\n",
    "        x_optimal = np.linalg.solve(A, -b)\n",
    "        ax.plot(x_optimal[0], x_optimal[1], 'k*', markersize=15, \n",
    "               markeredgecolor='white', label='True optimum')\n",
    "        \n",
    "        ax.set_title(f'Learning Rate = {lr}')\n",
    "        ax.set_xlabel('x₁')\n",
    "        ax.set_ylabel('x₂')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(-2, 4)\n",
    "        ax.set_ylim(-1, 3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Thử nghiệm với các điểm khởi tạo và learning rate khác nhau\n",
    "x_init_list = [[0, 0], [-1, 2], [3, -0.5], [2, 2]]\n",
    "learning_rates = [0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "print(\"Trực quan hóa quá trình hội tụ với các learning rate khác nhau:\")\n",
    "plot_convergence_2d(f2, grad_f2, x_init_list, learning_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Câu 2 — LFW + SVM (sigmoid)\n",
    "# ==============================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1) Load dữ liệu & hiển thị thông tin cơ bản\n",
    "lfwPeople = fetch_lfw_people(data_home=\"./\", min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "X = lfwPeople.data            # (n_samples, n_features), mỗi ảnh đã được flatten\n",
    "y = lfwPeople.target          # nhãn (chỉ số)\n",
    "target_names = lfwPeople.target_names\n",
    "images = lfwPeople.images     # (n_samples, h, w)\n",
    "h, w = images.shape[1], images.shape[2]\n",
    "\n",
    "print(\"=== THÔNG TIN BỘ DỮ LIỆU LFW ===\")\n",
    "print(f\"Kích thước ảnh: {h} x {w} pixels\")\n",
    "print(f\"Số lượng ảnh (n_samples): {X.shape[0]}\")\n",
    "print(f\"Số chiều đặc trưng (n_features): {X.shape[1]}\")\n",
    "print(f\"Số lượng nhãn (danh tính): {len(target_names)}\")\n",
    "\n",
    "# 2) Chia train/test (70/30) và chuẩn hóa dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Dùng Pipeline: StandardScaler -> SVC(sigmoid)\n",
    "clf = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"svm\", SVC(kernel=\"sigmoid\", C=1.0, gamma=\"scale\", random_state=42))\n",
    "])\n",
    "\n",
    "# 3) Huấn luyện mô hình\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4) Đánh giá mô hình\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\n=== ĐÁNH GIÁ MÔ HÌNH SVM (kernel='sigmoid') ===\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# classification_report: in đầy đủ precision, recall, F1 theo từng lớp và macro/weighted\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
