{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Đối ngẫu và Phương pháp Newton – Bài thực hành đơn giản\n",
    "\n",
    "Trong lab ngắn này, bạn sẽ ôn lại khái niệm đối ngẫu lồi qua một ví dụ nhỏ đủ tính toán được bằng tay và mã Python, sau đó triển khai phương pháp Newton với backtracking line search. Mục tiêu là hiểu công thức, tự viết mã, và trực quan hoá đường đi của thuật toán.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mục tiêu và tiền đề\n",
    "Trong lab này, bạn sẽ: \n",
    "- (i) củng cố khái niệm đối ngẫu lồi qua một ví dụ nhỏ có thể tính được tay và bằng Python, \n",
    "- (ii) tự cài đặt thuật toán Newton với backtracking line search cho bài toán tối ưu hoá không ràng buộc và \n",
    "- (iii) trực quan hoá quỹ đạo hội tụ so với gradient descent để cảm nhận lợi ích của thông tin Hessian\n",
    "\n",
    "## Thiết lập\n",
    "Chạy ô sau để import và cấu hình cơ bản.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "np.random.seed(42)\n",
    "\n",
    "from typing import Callable, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phần A — Ôn tập đối ngẫu qua ví dụ đơn giản\n",
    "Xét bài toán primal lồi dạng bình phương tối ưu hoá có ràng buộc tuyến tính nhỏ:\n",
    "\n",
    "$$\n",
    "\\min_{x\\in\\mathbb{R}^n} \\; f(x)=\\tfrac{1}{2}\\lVert x\\rVert^2 + c^\\top x\\quad \\text{s.t.}\\quad a^\\top x = b.\n",
    "$$\n",
    "\n",
    "Hàm Lagrange: $$\\mathcal{L}(x,\\lambda) = \\tfrac{1}{2}\\lVert x\\rVert^2 + c^\\top x + \\lambda(a^\\top x - b)$$. Với $\\lambda\\in\\mathbb{R}$, đối ngẫu là $g(\\lambda)=\\inf_x \\mathcal{L}(x,\\lambda)$. \n",
    "\n",
    "Ta tìm $x(\\lambda)$ thoả $\\nabla_x\\mathcal{L}=0\\Rightarrow x(\\lambda)=-(c+\\lambda a)$. Thay vào thu được hàm đối ngẫu lồi-concave theo $\\lambda$, và bài toán dual: $\\max_\\lambda g(\\lambda)$.\n",
    "\n",
    "Chúng ta sẽ cụ thể hoá với $n=2$, chọn $c, a, b$ và so sánh nghiệm primal-dual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ đối ngẫu: n=2\n",
    "c = np.array([1.0, -2.0])\n",
    "a = np.array([2.0, 1.0])\n",
    "b = 1.0\n",
    "\n",
    "# x(lambda) = -(c + lambda a)\n",
    "def x_of_lambda(lmbd: float) -> np.ndarray:\n",
    "    return -(c + lmbd * a)\n",
    "\n",
    "# g(lambda) = L(x(lambda), lambda) = 1/2||x||^2 + c^T x + lambda(a^T x - b)\n",
    "# với x = x(lambda)\n",
    "def g_dual(lmbd: float) -> float:\n",
    "    x = x_of_lambda(lmbd)\n",
    "    return 0.5 * np.dot(x, x) + np.dot(c, x) + lmbd * (np.dot(a, x) - b)\n",
    "\n",
    "# maximize g(lambda) (concave) -> ta có thể tìm đạo hàm g'(lambda)=0\n",
    "# Tính analytic: a^T x(lambda) - b = 0 => a^T (-(c + lambda a)) - b = 0\n",
    "# => -(a^T c) - lambda * (a^T a) - b = 0 => lambda* = -(a^T c + b)/(a^T a)\n",
    "lam_star = -(np.dot(a, c) + b) / np.dot(a, a)\n",
    "x_star_dual = x_of_lambda(lam_star)\n",
    "\n",
    "print('lambda* =', lam_star)\n",
    "print('x*(dual-induced) =', x_star_dual)\n",
    "\n",
    "# Kiểm tra ràng buộc a^T x = b\n",
    "print('a^T x =', np.dot(a, x_star_dual), '(mong đợi ~ b =', b, ')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ đồ thị g(lambda)\n",
    "lmbd_grid = np.linspace(lam_star - 5, lam_star + 5, 400)\n",
    "vals = np.array([g_dual(l) for l in lmbd_grid])\n",
    "\n",
    "plt.figure(figsize=(6,3.8))\n",
    "plt.plot(lmbd_grid, vals, label='g(lambda)')\n",
    "plt.axvline(lam_star, color='r', linestyle='--', label=f'lambda*={lam_star:.3f}')\n",
    "plt.xlabel('lambda'); plt.ylabel('g(lambda)')\n",
    "plt.title('Đồ thị hàm đối ngẫu g(lambda)')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1️⃣ HÀM GIẢI PRIMAL + DUAL CHO BÀI TOÁN TỔNG QUÁT\n",
    "# ======================================================\n",
    "\n",
    "def solve_primal_dual_quadratic(c, a, b):\n",
    "    \"\"\"\n",
    "    Bài toán:\n",
    "        minimize  f(x) = 1/2 ||x||^2 + c^T x\n",
    "        subject to a^T x = b\n",
    "\n",
    "    Input:\n",
    "        c: array-like, shape (n,)\n",
    "        a: array-like, shape (n,)\n",
    "        b: scalar\n",
    "\n",
    "    Output:\n",
    "        x_star : nghiệm primal tối ưu\n",
    "        lambda_star : nghiệm dual tối ưu\n",
    "        p_star : giá trị tối ưu primal f(x*)\n",
    "        d_star : giá trị tối ưu dual g(lambda*)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- ép về numpy vector 1D ----\n",
    "    c = np.atleast_1d(np.array(c, dtype=float))\n",
    "    a = np.atleast_1d(np.array(a, dtype=float))\n",
    "    b = float(b)\n",
    "\n",
    "    if c.shape != a.shape:\n",
    "        raise ValueError(\"c và a phải cùng kích thước\")\n",
    "\n",
    "    ata = a @ a\n",
    "    if abs(ata) < 1e-12:\n",
    "        raise ValueError(\"a phải khác 0 để ràng buộc a^T x = b có ý nghĩa\")\n",
    "\n",
    "    # ---- nghiệm dual λ* từ điều kiện feasibility a^T x(λ) = b ----\n",
    "    # a^T(-c - λ a) = b  ⇒  -(a^T c) - λ (a^T a) = b\n",
    "    # ⇒ λ* = -(b + a^T c) / (a^T a)\n",
    "    lambda_star = -(b + a @ c) / ata\n",
    "\n",
    "    # ---- nghiệm primal x* ----\n",
    "    x_star = -c - lambda_star * a\n",
    "\n",
    "    # ---- giá trị primal tối ưu ----\n",
    "    p_star = 0.5 * (x_star @ x_star) + c @ x_star\n",
    "\n",
    "    # ---- hàm dual g(λ) = L(x(λ), λ) ----\n",
    "    def dual_value(lmbd):\n",
    "        x_l = -c - lmbd * a\n",
    "        L = 0.5 * (x_l @ x_l) + c @ x_l + lmbd * (a @ x_l - b)\n",
    "        return L\n",
    "\n",
    "    d_star = dual_value(lambda_star)\n",
    "\n",
    "    return x_star, lambda_star, p_star, d_star, dual_value\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2️⃣ VÍ DỤ CỤ THỂ n = 2 (bạn có thể đổi tự do)\n",
    "# ======================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ví dụ: n = 2\n",
    "    # f(x) = 1/2 ||x||^2 + c^T x,  x = (x1,x2)\n",
    "    # c = (1, -2),  a = (2, 1),   a^T x = b = 3\n",
    "    c = [1, -2]\n",
    "    a = [2, 1]\n",
    "    b = 3\n",
    "\n",
    "    x_star, lambda_star, p_star, d_star, dual_value = solve_primal_dual_quadratic(c, a, b)\n",
    "\n",
    "    print(\"===== KẾT QUẢ PRIMAL - DUAL =====\")\n",
    "    print(\"c =\", c)\n",
    "    print(\"a =\", a)\n",
    "    print(\"b =\", b)\n",
    "    print(\"\\nNghiệm primal x* =\", x_star)\n",
    "    print(\"Nghiệm dual   λ* =\", lambda_star)\n",
    "    print(\"\\nGiá trị primal tối ưu  p* = f(x*) =\", p_star)\n",
    "    print(\"Giá trị dual tối ưu    d* = g(λ*) =\", d_star)\n",
    "    print(\"\\nKiểm tra strong duality (p* ?= d*):\")\n",
    "    print(\"p* - d* =\", p_star - d_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def primal_dual_solver(f_expr, x_symbols, eq_constraints=None):\n",
    "    \"\"\"\n",
    "    Solve:\n",
    "        minimize f(x)\n",
    "        subject to h_i(x) = 0\n",
    "        \n",
    "    Automatic:\n",
    "        - build Lagrangian\n",
    "        - compute dual function\n",
    "        - find optimal x*, λ*\n",
    "        - check strong duality\n",
    "        \n",
    "    INPUT:\n",
    "        f_expr: sympy expression of f(x)\n",
    "        x_symbols: list [x1, x2, ..., xn]\n",
    "        eq_constraints: list [h1(x), h2(x), ...]\n",
    "        \n",
    "    WORKS for nD automatically\n",
    "    \"\"\"\n",
    "\n",
    "    if eq_constraints is None:\n",
    "        eq_constraints = []\n",
    "\n",
    "    # --------------------\n",
    "    # PREPARE SYMBOLS\n",
    "    # --------------------\n",
    "\n",
    "    x = sp.Matrix(x_symbols)\n",
    "    n = len(x_symbols)\n",
    "\n",
    "    m = len(eq_constraints)\n",
    "    λ = sp.Matrix(sp.symbols(f\"λ1:{m+1}\", real=True))\n",
    "\n",
    "    # --------------------\n",
    "    # BUILD LAGRANGIAN\n",
    "    # --------------------\n",
    "    L = f_expr\n",
    "    for i in range(m):\n",
    "        L += λ[i] * eq_constraints[i]\n",
    "\n",
    "    print(\"\\n===== LAGRANGIAN =====\")\n",
    "    print(\"L(x, λ) =\")\n",
    "    print(sp.simplify(L))\n",
    "\n",
    "    # --------------------\n",
    "    # STATIONARITY ∂L/∂x=0\n",
    "    # --------------------\n",
    "    grad_L = sp.Matrix([sp.diff(L, var) for var in x_symbols])\n",
    "\n",
    "    print(\"\\n===== KKT STATIONARITY =====\")\n",
    "    for i, gi in enumerate(grad_L):\n",
    "        print(f\"∂L/∂x{i+1} = {sp.simplify(gi)} = 0\")\n",
    "\n",
    "    # --------------------\n",
    "    # SOLVE KKT SYSTEM\n",
    "    # --------------------\n",
    "    equations = list(grad_L)\n",
    "\n",
    "    # add primal feasibility\n",
    "    for h in eq_constraints:\n",
    "        equations.append(h)\n",
    "\n",
    "    unknowns = list(x_symbols) + list(λ)\n",
    "\n",
    "    sol = sp.solve(equations, unknowns, dict=True)\n",
    "\n",
    "    if not sol:\n",
    "        print(\"\\n⚠ No KKT solution found\")\n",
    "        return\n",
    "    \n",
    "    sol = sol[0]\n",
    "\n",
    "    # --------------------\n",
    "    # EXTRACT PRIMAL x*\n",
    "    # --------------------\n",
    "    x_star = sp.Matrix([sol[v] for v in x_symbols])\n",
    "\n",
    "    # --------------------\n",
    "    # DUAL λ*\n",
    "    # --------------------\n",
    "    λ_star = sp.Matrix([sol[v] for v in λ]) if m>0 else sp.Matrix([])\n",
    "\n",
    "    print(\"\\n===== OPTIMAL SOLUTION =====\")\n",
    "    print(\"x* =\", x_star)\n",
    "    print(\"λ* =\", λ_star)\n",
    "\n",
    "    # --------------------\n",
    "    # PRIMAL VALUE\n",
    "    # --------------------\n",
    "    p_star = f_expr.subs([(x_symbols[i], x_star[i]) for i in range(n)])\n",
    "    p_star = sp.simplify(p_star)\n",
    "\n",
    "    print(\"\\n===== PRIMAL VALUE =====\")\n",
    "    print(\"p* = f(x*) =\", p_star)\n",
    "\n",
    "    # --------------------\n",
    "    # BUILD DUAL FUNCTION\n",
    "    # --------------------\n",
    "    x_lambda = sp.solve(list(grad_L), list(x_symbols), dict=True)\n",
    "\n",
    "    if x_lambda:\n",
    "\n",
    "        xλ = x_lambda[0]\n",
    "\n",
    "        L_lambda = L.subs([(x_symbols[i], xλ[x_symbols[i]]) \n",
    "                                  for i in range(n)])\n",
    "\n",
    "        g_lambda = sp.simplify(L_lambda)\n",
    "\n",
    "        if m > 0:\n",
    "            d_star = g_lambda.subs([(λ[i], λ_star[i]) for i in range(m)])\n",
    "            d_star = sp.simplify(d_star)\n",
    "\n",
    "            print(\"\\n===== DUAL FUNCTION g(λ) =====\")\n",
    "            print(\"g(λ) =\", g_lambda)\n",
    "\n",
    "            print(\"\\n===== DUAL VALUE =====\")\n",
    "            print(\"g(λ*) =\", d_star)\n",
    "\n",
    "            print(\"\\n===== STRONG DUALITY CHECK =====\")\n",
    "            print(\"p* - g(λ*) =\", sp.simplify(p_star - d_star))\n",
    "\n",
    "    print(\"\\nDONE ✓\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sj x1 - 2*x2 = 0\n",
    "x1, x2 = sp.symbols('x1 x2', real=True)\n",
    "\n",
    "f = 0.5*(x1**2 + 4*x2**2) - x1 + 4*x2 + 2.5\n",
    "\n",
    "h = x1 - 2*x2 - 2\n",
    "\n",
    "primal_dual_solver(f, [x1, x2], [h])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2,x3,x4 = sp.symbols('x1 x2 x3 x4')\n",
    "f = 0.5*(x1**2+x2**2+x3**2+x4**2)\n",
    "\n",
    "h1 = x1 + x2 - 3\n",
    "h2 = x3 - x4\n",
    "\n",
    "primal_dual_solver(f, [x1,x2,x3,x4], [h1, h2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def primal_dual_solver_general(f_expr, x_symbols, constraints):\n",
    "\n",
    "    x = sp.Matrix(x_symbols)\n",
    "    n = len(x)\n",
    "\n",
    "    eq_cons = []\n",
    "    ineq_cons = []\n",
    "    ineq_sign = []  # store signs\n",
    "\n",
    "    # ---------------------------\n",
    "    # PREPROCESS CONSTRAINT INPUT\n",
    "    # ---------------------------\n",
    "    for ctype, expr in constraints:\n",
    "\n",
    "        if ctype in [\"=\"]:\n",
    "            eq_cons.append(expr)\n",
    "\n",
    "        elif ctype in [\"<\",\"<=\"]:  \n",
    "            # convert to g(x) ≤ 0\n",
    "            ineq_cons.append(expr)\n",
    "            ineq_sign.append(+1)   # λ ≥ 0\n",
    "\n",
    "        elif ctype in [\">\",\">=\"]:\n",
    "            # convert to (-expr ≤ 0)\n",
    "            ineq_cons.append(-expr)\n",
    "            ineq_sign.append(-1)   # λ ≤ 0\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown constraint symbol\")\n",
    "\n",
    "    # ---------------------------------\n",
    "    # VARIABLES FOR MULTIPLIERS\n",
    "    # ---------------------------------\n",
    "    λ = sp.Matrix(sp.symbols(f\"lam1:{len(ineq_cons)+1}\", real=True))\n",
    "    ν = sp.Matrix(sp.symbols(f\"nu1:{len(eq_cons)+1}\", real=True))\n",
    "\n",
    "    # --------------------\n",
    "    # BUILD LAGRANGIAN\n",
    "    # --------------------\n",
    "    L = f_expr\n",
    "\n",
    "    for i,h in enumerate(eq_cons):\n",
    "        L += ν[i]*h\n",
    "\n",
    "    for j,g in enumerate(ineq_cons):\n",
    "        L += λ[j]*g\n",
    "\n",
    "    print(\"\\n===== LAGRANGIAN =====\")\n",
    "    print(L)\n",
    "\n",
    "    # --------------------\n",
    "    # STATIONARITY\n",
    "    # --------------------\n",
    "    grad_L = sp.Matrix([sp.diff(L, xi) for xi in x_symbols])\n",
    "\n",
    "    print(\"\\n===== ∂L/∂x = 0 =====\")\n",
    "    for eq in grad_L:\n",
    "        print(eq,\"= 0\")\n",
    "\n",
    "    # --------------------\n",
    "    # BUILD KKT EQUATIONS\n",
    "    # --------------------\n",
    "    eqs = list(grad_L)\n",
    "\n",
    "    # primal eq feasibility\n",
    "    for h in eq_cons:\n",
    "        eqs.append(h)\n",
    "\n",
    "    # complementary slackness\n",
    "    for j,g in enumerate(ineq_cons):\n",
    "        eqs.append(λ[j]*g)\n",
    "\n",
    "    # variable list\n",
    "    unknowns = list(x_symbols) + list(λ) + list(ν)\n",
    "\n",
    "    # solve\n",
    "    sol = sp.solve(eqs, unknowns, dict=True)\n",
    "\n",
    "    if not sol:\n",
    "        print(\"\\n❌ No symbolic KKT solution found.\")\n",
    "        return\n",
    "\n",
    "    sol = sol[0]\n",
    "\n",
    "    # ---------------------------------\n",
    "    # FILTER DUAL FEASIBILITY\n",
    "    # ---------------------------------\n",
    "    feasible_solutions = []\n",
    "\n",
    "    for key,value in sol.items():\n",
    "        pass\n",
    "\n",
    "    for var,val in sol.items():\n",
    "        pass\n",
    "\n",
    "    # only primal-dual output\n",
    "    x_star = sp.Matrix([sol[v] for v in x_symbols])\n",
    "\n",
    "    print(\"\\n===== x* =====\")\n",
    "    print(x_star)\n",
    "\n",
    "    # dual multipliers\n",
    "    lam_star = sp.Matrix([sol[v] for v in λ]) if len(λ)>0 else None\n",
    "    nu_star  = sp.Matrix([sol[v] for v in ν]) if len(ν)>0 else None\n",
    "\n",
    "    print(\"\\n===== λ* =====\")\n",
    "    print(lam_star)\n",
    "\n",
    "    print(\"\\n===== ν* =====\")\n",
    "    print(nu_star)\n",
    "\n",
    "    # primal value\n",
    "    f_star = f_expr.subs([(x_symbols[i],x_star[i]) for i in range(n)])\n",
    "    print(\"\\n===== f(x*) =====\")\n",
    "    print(sp.simplify(f_star))\n",
    "\n",
    "    # strong duality check only if convex + equality constraint only\n",
    "    if len(ineq_cons)==0:\n",
    "        # dual function\n",
    "        xλ_sol = sp.solve(list(grad_L), list(x_symbols), dict=True)\n",
    "        if xλ_sol:\n",
    "            xλ = xλ_sol[0]\n",
    "            Lλ = L.subs([(x_symbols[i],xλ[x_symbols[i]]) for i in range(n)])\n",
    "            gλ = sp.simplify(Lλ)\n",
    "            dual_val = gλ.subs([(ν[i],nu_star[i]) for i in range(len(nu_star))])\n",
    "            print(\"\\n===== strong duality =====\")\n",
    "            print(\"f* - g* =\", sp.simplify(f_star-dual_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = sp.symbols('x y', real=True)\n",
    "f = -x**2 - y**2 + 4*x +6*y\n",
    "cons = [(\"<=\", x + y -5),\n",
    "        (\">=\", x),\n",
    "         (\">=\", y)\n",
    "        ]\n",
    "\n",
    "primal_dual_solver_general(f, [x,y], cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def solve_optimization(f_expr, x_symbols, constraints):\n",
    "    \"\"\"\n",
    "    Giải bài toán tối ưu có ràng buộc bằng điều kiện KKT\n",
    "    \n",
    "    Input:\n",
    "        f_expr: hàm mục tiêu (sympy expression)\n",
    "        x_symbols: list các biến [x, y, z, ...]\n",
    "        constraints: list các ràng buộc [(\"=\", expr), (\"<=\", expr), (\">=\", expr), ...]\n",
    "    \n",
    "    Output: In ra màn hình nghiệm tối ưu x*, λ*, ν*, f(x*)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = sp.Matrix(x_symbols)\n",
    "    n = len(x)\n",
    "    eq_cons, ineq_cons, ineq_sign = [], [], []\n",
    "    \n",
    "    # Phân loại ràng buộc\n",
    "    for ctype, expr in constraints:\n",
    "        if ctype == \"=\":\n",
    "            eq_cons.append(expr)\n",
    "        elif ctype in [\"<\", \"<=\"]:\n",
    "            ineq_cons.append(expr)\n",
    "            ineq_sign.append(+1)  # λ ≥ 0\n",
    "        elif ctype in [\">\", \">=\"]:\n",
    "            ineq_cons.append(-expr)\n",
    "            ineq_sign.append(-1)  # λ ≤ 0\n",
    "    \n",
    "    # Tạo biến nhân tử Lagrange\n",
    "    m_ineq, m_eq = len(ineq_cons), len(eq_cons)\n",
    "    λ = sp.Matrix(sp.symbols(f\"lam1:{m_ineq+1}\", real=True)) if m_ineq > 0 else sp.Matrix([])\n",
    "    ν = sp.Matrix(sp.symbols(f\"nu1:{m_eq+1}\", real=True)) if m_eq > 0 else sp.Matrix([])\n",
    "    \n",
    "    # Xây dựng Lagrangian\n",
    "    L = f_expr\n",
    "    for i, h in enumerate(eq_cons):\n",
    "        L += ν[i] * h\n",
    "    for j, g in enumerate(ineq_cons):\n",
    "        L += λ[j] * g\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LAGRANGIAN\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"L = {L}\\n\")\n",
    "    \n",
    "    # Điều kiện stationarity\n",
    "    grad_L = sp.Matrix([sp.diff(L, xi) for xi in x_symbols])\n",
    "    print(\"=\"*70)\n",
    "    print(\"STATIONARITY: ∇L = 0\")\n",
    "    print(\"=\"*70)\n",
    "    for i, eq in enumerate(grad_L):\n",
    "        print(f\"∂L/∂{x_symbols[i]} = {eq} = 0\")\n",
    "    \n",
    "    # Hệ phương trình KKT\n",
    "    eqs = list(grad_L)\n",
    "    for h in eq_cons:\n",
    "        eqs.append(h)\n",
    "    for j, g in enumerate(ineq_cons):\n",
    "        eqs.append(λ[j] * g)\n",
    "    \n",
    "    unknowns = list(x_symbols) + list(λ) + list(ν)\n",
    "    \n",
    "    # Giải hệ KKT\n",
    "    try:\n",
    "        solutions = sp.solve(eqs, unknowns, dict=True)\n",
    "    except:\n",
    "        print(\"\\n❌ Không giải được hệ KKT\")\n",
    "        return\n",
    "    \n",
    "    if not solutions:\n",
    "        print(\"\\n❌ Không tìm thấy nghiệm\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n✓ Tìm thấy {len(solutions)} nghiệm ứng viên\\n\")\n",
    "    \n",
    "    # Lọc nghiệm khả thi\n",
    "    feasible = []\n",
    "    for idx, sol in enumerate(solutions):\n",
    "        is_feasible = True\n",
    "        \n",
    "        # Kiểm tra dual feasibility (dấu của λ)\n",
    "        for j in range(m_ineq):\n",
    "            lam_val = sol.get(λ[j], 0)\n",
    "            try:\n",
    "                lam_numeric = complex(lam_val)\n",
    "                if lam_numeric.imag != 0:\n",
    "                    is_feasible = False\n",
    "                    break\n",
    "                lam_real = lam_numeric.real\n",
    "                if ineq_sign[j] == +1 and lam_real < -1e-10:\n",
    "                    is_feasible = False\n",
    "                    break\n",
    "                if ineq_sign[j] == -1 and lam_real > 1e-10:\n",
    "                    is_feasible = False\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Kiểm tra primal feasibility (g(x) ≤ 0)\n",
    "        if is_feasible:\n",
    "            for j, g in enumerate(ineq_cons):\n",
    "                g_val = g.subs([(x_symbols[i], sol.get(x_symbols[i], 0)) for i in range(n)])\n",
    "                try:\n",
    "                    if float(g_val) > 1e-8:\n",
    "                        is_feasible = False\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if is_feasible:\n",
    "            feasible.append(sol)\n",
    "    \n",
    "    if not feasible:\n",
    "        print(\"❌ Không có nghiệm KKT khả thi\")\n",
    "        return\n",
    "    \n",
    "    # Chọn nghiệm tốt nhất (f nhỏ nhất)\n",
    "    best_sol = feasible[0]\n",
    "    best_f = None\n",
    "    for sol in feasible:\n",
    "        x_val = sp.Matrix([sol.get(x_symbols[i], 0) for i in range(n)])\n",
    "        f_val = f_expr.subs([(x_symbols[i], x_val[i]) for i in range(n)])\n",
    "        try:\n",
    "            f_numeric = float(sp.re(f_val))\n",
    "            if best_f is None or f_numeric < best_f:\n",
    "                best_f = f_numeric\n",
    "                best_sol = sol\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Kết quả\n",
    "    x_star = sp.Matrix([best_sol.get(v, 0) for v in x_symbols])\n",
    "    lam_star = sp.Matrix([best_sol.get(v, 0) for v in λ]) if m_ineq > 0 else None\n",
    "    nu_star = sp.Matrix([best_sol.get(v, 0) for v in ν]) if m_eq > 0 else None\n",
    "    f_star = f_expr.subs([(x_symbols[i], x_star[i]) for i in range(n)])\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"KẾT QUẢ\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nx* = {x_star}\")\n",
    "    if lam_star is not None:\n",
    "        print(f\"\\nλ* = {lam_star}\")\n",
    "    if nu_star is not None:\n",
    "        print(f\"\\nν* = {nu_star}\")\n",
    "    print(f\"\\nf(x*) = {sp.simplify(f_star)}\")\n",
    "    \n",
    "    # Kiểm tra đối ngẫu mạnh (nếu chỉ có ràng buộc đẳng thức)\n",
    "    if m_ineq == 0 and m_eq > 0:\n",
    "        try:\n",
    "            xν_sol = sp.solve(list(grad_L), list(x_symbols), dict=True)\n",
    "            if xν_sol:\n",
    "                xν = xν_sol[0]\n",
    "                Lν = L.subs([(x_symbols[i], xν[x_symbols[i]]) for i in range(n)])\n",
    "                gν = sp.simplify(Lν)\n",
    "                dual_val = gν.subs([(ν[i], nu_star[i]) for i in range(len(nu_star))])\n",
    "                gap = sp.simplify(f_star - dual_val)\n",
    "                print(f\"\\nDuality gap: f* - g* = {gap}\")\n",
    "                try:\n",
    "                    if abs(float(sp.re(gap))) < 1e-8:\n",
    "                        print(\"✓ Đối ngẫu mạnh được thỏa mãn\")\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SỬ DỤNG\n",
    "# ============================================\n",
    "# x² - y² + 4x + 6y  s.t. x+y≤5, x≥0, y≥0\n",
    "\n",
    "x, y = sp.symbols('x y', real=True)\n",
    "\n",
    "f = -x**2 - y**2 + 4*x +6*y\n",
    "cons = [(\"<=\", x + y -5),\n",
    "        (\">=\", x),\n",
    "         (\">=\", y)\n",
    "        ]\n",
    "\n",
    "solve_optimization(f, [x, y], cons)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phần B — Phương pháp Newton với backtracking\n",
    "Ta xét hàm Rosenbrock kinh điển $f(x,y) = (1-x)^2 + 100(y - x^2)^2$. Viết gradient, Hessian, sau đó cài đặt Newton với điều kiện Armijo (backtracking). So sánh quỹ đạo với Gradient Descent để cảm nhận cải thiện điều kiện hoá.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking Line Search\n",
    "\n",
    "Mục tiêu: tìm hệ số bước \\( \\alpha > 0 \\) sao cho điều kiện Armijo được thỏa mãn.\n",
    "\n",
    "$$\n",
    "\\alpha_{k+1} = \\rho \\, \\alpha_k, \\quad 0 < \\rho < 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(x + \\alpha d) \\le f(x) + c \\alpha \\nabla f(x)^\\top d, \\quad 0 < c < 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Newton Method\n",
    "\n",
    "Tại bước \\( k \\):\n",
    "\n",
    "$$\n",
    "\\nabla f(x_k) = g_k, \\quad \\nabla^2 f(x_k) = H_k\n",
    "$$\n",
    "\n",
    "Phương trình Newton:\n",
    "\n",
    "$$\n",
    "H_k p_k = -g_k \\quad \\Rightarrow \\quad p_k = -H_k^{-1} g_k\n",
    "$$\n",
    "\n",
    "Cập nhật nghiệm:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_k p_k\n",
    "$$\n",
    "\n",
    "Nếu \\( H_k \\) suy biến, dùng nghịch đảo giả:\n",
    "\n",
    "$$\n",
    "p_k = -H_k^{+} g_k\n",
    "$$\n",
    "\n",
    "Tiêu chí dừng:\n",
    "\n",
    "$$\n",
    "\\|\\nabla f(x_k)\\| < \\text{tol}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Descent (Hạ dốc)\n",
    "\n",
    "Cập nhật theo hướng âm gradient:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\eta \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "Tiêu chí dừng:\n",
    "\n",
    "$$\n",
    "\\|\\nabla f(x_k)\\| < \\text{tol}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x: np.ndarray) -> float:\n",
    "    X, Y = x[0], x[1]\n",
    "    return (1 - X)**2 + 100.0 * (Y - X**2)**2\n",
    "\n",
    "def grad_rosenbrock(x: np.ndarray) -> np.ndarray:\n",
    "    X, Y = x[0], x[1]\n",
    "    dfdx = -2*(1 - X) - 400*X*(Y - X**2)\n",
    "    dfdy = 200*(Y - X**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "def hess_rosenbrock(x: np.ndarray) -> np.ndarray:\n",
    "    X, Y = x[0], x[1]\n",
    "    dxx = 2 - 400*(Y - 3*X**2)\n",
    "    dxy = -400*X\n",
    "    dyx = -400*X\n",
    "    dyy = 200\n",
    "    return np.array([[dxx, dxy], [dyx, dyy]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f: Callable[[np.ndarray], float],\n",
    "                             grad: Callable[[np.ndarray], np.ndarray],\n",
    "                             x: np.ndarray,\n",
    "                             direction: np.ndarray,\n",
    "                             alpha_init: float = 1.0,\n",
    "                             rho: float = 0.5,\n",
    "                             c: float = 1e-4) -> float:\n",
    "    f_x = f(x)\n",
    "    grad_x = grad(x)\n",
    "    alpha = alpha_init\n",
    "    while f(x + alpha * direction) > f_x + c * alpha * np.dot(grad_x, direction):\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-12:\n",
    "            break\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def newton_method(f: Callable[[np.ndarray], float],\n",
    "                  grad: Callable[[np.ndarray], np.ndarray],\n",
    "                  hess: Callable[[np.ndarray], np.ndarray],\n",
    "                  x0: np.ndarray,\n",
    "                  tol: float = 1e-8,\n",
    "                  max_iter: int = 50,\n",
    "                  line_search: bool = True) -> Tuple[np.ndarray, list]:\n",
    "    x = x0.astype(float).copy()\n",
    "    hist = [x.copy()]\n",
    "    for _ in range(max_iter):\n",
    "        g = grad(x)\n",
    "        H = hess(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "        try:\n",
    "            p = -np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Nếu Hessian suy biến, dùng pseudo-inverse\n",
    "            p = -np.linalg.pinv(H) @ g\n",
    "        step = backtracking_line_search(f, grad, x, p) if line_search else 1.0\n",
    "        x = x + step * p\n",
    "        hist.append(x.copy())\n",
    "    return x, hist\n",
    "\n",
    "\n",
    "def gradient_descent(f: Callable[[np.ndarray], float],\n",
    "                     grad: Callable[[np.ndarray], np.ndarray],\n",
    "                     x0: np.ndarray,\n",
    "                     lr: float = 1e-3,\n",
    "                     max_iter: int = 5000,\n",
    "                     tol: float = 1e-8) -> Tuple[np.ndarray, list]:\n",
    "    x = x0.astype(float).copy()\n",
    "    hist = [x.copy()]\n",
    "    for _ in range(max_iter):\n",
    "        g = grad(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "        x = x - lr * g\n",
    "        hist.append(x.copy())\n",
    "    return x, hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy thử Newton vs GD trên Rosenbrock\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "x_newton, hist_newton = newton_method(rosenbrock, grad_rosenbrock, hess_rosenbrock, x0, max_iter=30)\n",
    "x_gd, hist_gd = gradient_descent(rosenbrock, grad_rosenbrock, x0, lr=1e-3, max_iter=50000)\n",
    "\n",
    "print('Newton x* ~', x_newton, 'f* ~', rosenbrock(x_newton))\n",
    "print('GD     x* ~', x_gd, 'f* ~', rosenbrock(x_gd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ quỹ đạo trên contour f\n",
    "hist_newton_arr = np.array(hist_newton)\n",
    "hist_gd_arr = np.array(hist_gd)\n",
    "\n",
    "# Lưới để vẽ contour\n",
    "xv = np.linspace(-2, 2, 400)\n",
    "yv = np.linspace(-1, 2, 400)\n",
    "XX, YY = np.meshgrid(xv, yv)\n",
    "ZZ = (1-XX)**2 + 100*(YY-XX**2)**2\n",
    "\n",
    "plt.figure(figsize=(6.5,5))\n",
    "cs = plt.contour(XX, YY, np.log10(ZZ+1e-6), levels=30, cmap='viridis')\n",
    "plt.clabel(cs, inline=True, fontsize=8, fmt='%.1f')\n",
    "plt.plot(hist_gd_arr[:,0], hist_gd_arr[:,1], '-o', ms=2, lw=1, label='GD', alpha=0.8)\n",
    "plt.plot(hist_newton_arr[:,0], hist_newton_arr[:,1], '-s', ms=4, lw=1.5, label='Newton', alpha=0.9)\n",
    "plt.scatter([1],[1], c='red', s=50, label='Min (1,1)')\n",
    "plt.legend(); plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.title('Quỹ đạo GD vs Newton trên Rosenbrock (contour log10(f))')\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BUILDING GENERAL OPTIMIZATION MODEL (1D → nD)\n",
    "# =========================================================\n",
    "def build_model(f_expr, variables):\n",
    "\n",
    "    # symbolic → numeric lambdify\n",
    "    f = sp.lambdify(variables, f_expr, \"numpy\")\n",
    "\n",
    "    # gradient\n",
    "    grad_syms = [sp.diff(f_expr, v) for v in variables]\n",
    "    grad = sp.lambdify(variables, grad_syms, \"numpy\")\n",
    "\n",
    "    # Hessian\n",
    "    H_syms = sp.hessian(f_expr, variables)\n",
    "    hess = sp.lambdify(variables, H_syms, \"numpy\")\n",
    "\n",
    "    return f, grad, hess\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# NEWTON with BACKTRACKING ARMIJO\n",
    "# =========================================================\n",
    "def newton_armijo(f, grad, hess, x0, alpha=0.4, beta=0.8,\n",
    "                  max_iter=200, tol=1e-6):\n",
    "\n",
    "    x = np.array(x0, dtype=float)\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        g = np.array(grad(*x), dtype=float)\n",
    "        H = np.array(hess(*x), dtype=float)\n",
    "\n",
    "        # stopping\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        # Newton step\n",
    "        try:\n",
    "            p = np.linalg.solve(H, -g)\n",
    "        except np.linalg.LinAlgError:       # fallback GD direction\n",
    "            p = -g  \n",
    "\n",
    "        # Armijo line search\n",
    "        t = 1\n",
    "        fx = f(*x)\n",
    "        while f(*(x + t*p)) > fx + alpha*t*np.dot(g, p):\n",
    "            t *= beta\n",
    "\n",
    "        x = x + t*p\n",
    "        hist.append(x.copy())\n",
    "\n",
    "    return np.array(hist)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# GRADIENT DESCENT + BACKTRACKING ARMIJO\n",
    "# =========================================================\n",
    "def gd_armijo(f, grad, x0, alpha=0.4, beta=0.8,\n",
    "              max_iter=200, tol=1e-6):\n",
    "\n",
    "    x = np.array(x0, dtype=float)\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        g = np.array(grad(*x), dtype=float)\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        p = -g\n",
    "\n",
    "        t = 1\n",
    "        fx = f(*x)\n",
    "        while f(*(x + t*p)) > fx + alpha*t*np.dot(g, p):\n",
    "            t *= beta\n",
    "\n",
    "        x = x + t*p\n",
    "        hist.append(x.copy())\n",
    "\n",
    "    return np.array(hist)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOTTING IN 2D CASE ONLY\n",
    "# =========================================================\n",
    "def plot_2D(f, newton_path, gd_path):\n",
    "    xs = np.linspace(-2,2,400)\n",
    "    ys = np.linspace(-1,3,400)\n",
    "    X,Y = np.meshgrid(xs,ys)\n",
    "    Z = f(X,Y)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.contour(X,Y,Z,levels=40)\n",
    "\n",
    "    plt.plot(newton_path[:,0],newton_path[:,1],'ro--',label=\"Newton\")\n",
    "    plt.plot(gd_path[:,0],gd_path[:,1],'go--',label=\"Gradient Descent\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Path comparison\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# GENERAL RUN FUNCTION: Just change f_expr and variables\n",
    "# =========================================================\n",
    "def run(f_expr, variables, x0):\n",
    "\n",
    "    f, grad, hess = build_model(f_expr, variables)\n",
    "\n",
    "    newton_hist = newton_armijo(f, grad, hess, x0)\n",
    "    gd_hist     = gd_armijo(f, grad, x0)\n",
    "\n",
    "    print(\"\\nFinal Newton point =\", newton_hist[-1])\n",
    "    print(\"Final GD point     =\", gd_hist[-1])\n",
    "\n",
    "    # only visualizable when dim = 2\n",
    "    if len(variables)==2:\n",
    "        plot_2D(f,newton_hist,gd_hist)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# DEFAULT TEST: ROSENBROCK\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x, y = sp.symbols(\"x y\", real=True)\n",
    "\n",
    "    f_expr = (1-x)**2 + 100*(y - x**2)**2\n",
    "\n",
    "    run(f_expr, [x,y], x0=[-1.2, 1.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phần B — Ví dụ bổ sung: Newton có/không backtracking trên Rosenbrock\n",
    "Để minh hoạ vai trò của backtracking (điều kiện Armijo) trong phương pháp Newton, ta so sánh Newton với backtracking và Newton full-step (không backtracking) từ một điểm khởi tạo khác trên hàm Rosenbrock, đồng thời hiển thị quỹ đạo và giá trị hàm mục tiêu để thấy sự khác biệt về độ ổn định và tốc độ hội tụ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ bổ sung Phần B: Newton có/không backtracking vs GD trên Rosenbrock\n",
    "# Dùng khởi tạo khác để quan sát khác biệt ổn định\n",
    "x0_alt = np.array([-1.8, 1.8])\n",
    "\n",
    "# Newton với backtracking (đã có hàm newton_method)\n",
    "x_newton_bt, hist_newton_bt = newton_method(rosenbrock, grad_rosenbrock, hess_rosenbrock,\n",
    "                                            x0_alt, max_iter=40, line_search=True)\n",
    "\n",
    "# Newton full-step (không backtracking): tận dụng newton_method với line_search=False\n",
    "x_newton_fs, hist_newton_fs = newton_method(rosenbrock, grad_rosenbrock, hess_rosenbrock,\n",
    "                                            x0_alt, max_iter=40, line_search=False)\n",
    "\n",
    "# Gradient Descent giữ nguyên lr như trước để tham chiếu\n",
    "x_gd_alt, hist_gd_alt = gradient_descent(rosenbrock, grad_rosenbrock, x0_alt, lr=1e-3, max_iter=80000)\n",
    "\n",
    "print('Newton BT  x* ~', x_newton_bt, 'f* ~', rosenbrock(x_newton_bt))\n",
    "print('Newton FS  x* ~', x_newton_fs, 'f* ~', rosenbrock(x_newton_fs))\n",
    "print('GD (alt)   x* ~', x_gd_alt,    'f* ~', rosenbrock(x_gd_alt))\n",
    "\n",
    "# Vẽ quỹ đạo\n",
    "h_bt = np.array(hist_newton_bt)\n",
    "h_fs = np.array(hist_newton_fs)\n",
    "h_gd = np.array(hist_gd_alt)\n",
    "\n",
    "xv = np.linspace(-2.2, 2.2, 450)\n",
    "yv = np.linspace(-1.2, 2.2, 450)\n",
    "XX, YY = np.meshgrid(xv, yv)\n",
    "ZZ = (1-XX)**2 + 100*(YY-XX**2)**2\n",
    "\n",
    "plt.figure(figsize=(6.8,5.4))\n",
    "cs = plt.contour(XX, YY, np.log10(ZZ+1e-6), levels=35, cmap='viridis')\n",
    "plt.clabel(cs, inline=True, fontsize=8, fmt='%.1f')\n",
    "plt.plot(h_gd[:,0], h_gd[:,1], '-o', ms=2, lw=1, label='GD (alt)', alpha=0.75)\n",
    "plt.plot(h_bt[:,0], h_bt[:,1], '-s', ms=4, lw=1.4, label='Newton + backtracking', alpha=0.9)\n",
    "plt.plot(h_fs[:,0], h_fs[:,1], '-^', ms=4, lw=1.2, label='Newton full-step', alpha=0.9)\n",
    "plt.scatter([1],[1], c='red', s=50, label='Min (1,1)')\n",
    "plt.legend(); plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.title('Phần B (bổ sung): Quỹ đạo Newton có/không backtracking vs GD trên Rosenbrock')\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# Vẽ tiến triển giá trị hàm mục tiêu theo iteration\n",
    "plt.figure(figsize=(6.2,3.6))\n",
    "f_bt = [rosenbrock(p) for p in h_bt]\n",
    "f_fs = [rosenbrock(p) for p in h_fs]\n",
    "f_gd = [rosenbrock(p) for p in h_gd]\n",
    "plt.semilogy(f_bt, label='Newton + backtracking')\n",
    "plt.semilogy(f_fs, label='Newton full-step')\n",
    "plt.semilogy(f_gd, label='GD (alt)')\n",
    "plt.xlabel('Iteration'); plt.ylabel('f(x) (log scale)')\n",
    "plt.title('Phần B (bổ sung): Tiến triển f(x)')\n",
    "plt.legend(); plt.grid(True, which='both', alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# BUILD GENERAL MODEL FROM SYMBOLIC: f, grad f, Hessian f\n",
    "# =================================================================\n",
    "def build_function(f_expr, variables):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        f_expr: sympy objective function\n",
    "        variables: list of symbolic variables [x1,x2,...,xn]\n",
    "\n",
    "    Output:\n",
    "        f(x), grad(x), Hessian(x) (numpy-callable)\n",
    "    \"\"\"\n",
    "\n",
    "    f   = sp.lambdify(variables, f_expr, \"numpy\")\n",
    "\n",
    "    grad_expr = [sp.diff(f_expr, v) for v in variables]\n",
    "    grad = sp.lambdify(variables, grad_expr, \"numpy\")\n",
    "\n",
    "    hess_expr = sp.hessian(f_expr, variables)\n",
    "    hess = sp.lambdify(variables, hess_expr, \"numpy\")\n",
    "\n",
    "    return f, grad, hess\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# NEWTON WITH BACKTRACKING\n",
    "# =================================================================\n",
    "def newton_backtracking(\n",
    "    f, grad, hess,\n",
    "    x0,\n",
    "    alpha = 0.4,\n",
    "    beta  = 0.8,\n",
    "    tol   = 1e-6,\n",
    "    max_iter = 200,\n",
    "    verbose = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Newton method with Armijo line search.\n",
    "    \n",
    "    Input:\n",
    "        f, grad, hess : callables\n",
    "        x0   : initial guess (scalar or vector)\n",
    "        alpha, beta: Armijo params\n",
    "        tol  : tolerance\n",
    "        max_iter: max loops\n",
    "    \n",
    "    Output:\n",
    "        history: list of x_k\n",
    "    \"\"\"\n",
    "\n",
    "    # ensure vector format\n",
    "    x = np.array(x0, dtype=float)\n",
    "    if x.ndim == 0:\n",
    "        x = x.reshape(1)\n",
    "\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for k in range(max_iter):\n",
    "\n",
    "        g = np.asarray(grad(*x), dtype=float)\n",
    "\n",
    "        # stopping condition\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            if verbose:\n",
    "                print(f\"STOP: gradient norm small at iter {k}\")\n",
    "            break\n",
    "\n",
    "        H = np.asarray(hess(*x), dtype=float)\n",
    "\n",
    "        # Newton direction\n",
    "        try:\n",
    "            p = np.linalg.solve(H, -g)\n",
    "        except np.linalg.LinAlgError:   # if Hessian singular\n",
    "            p = -g                      # fallback gradient descent\n",
    "\n",
    "        # Armijo backtracking\n",
    "        t = 1.0\n",
    "        fx = f(*x)\n",
    "\n",
    "        while True:\n",
    "            x_trial = x + t*p\n",
    "            f_trial = f(*x_trial)\n",
    "\n",
    "            if f_trial <= fx + alpha*t*np.dot(g,p):\n",
    "                break\n",
    "\n",
    "            t *= beta\n",
    "\n",
    "            if t < 1e-12:\n",
    "                break\n",
    "\n",
    "        x = x + t*p\n",
    "        hist.append(x.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iter {k:3d} | f={fx:.6f} | step={t:.3e} | x={x}\")\n",
    "\n",
    "    return np.array(hist)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# UNIVERSAL RUN FUNCTION — CHANGE ONLY f_expr + x0\n",
    "# =================================================================\n",
    "def run_newton(f_expr, variables, x0, **kwargs):\n",
    "    \"\"\"\n",
    "    Build model & run Newton easily:\n",
    "    \"\"\"\n",
    "\n",
    "    f, grad, hess = build_function(f_expr, variables)\n",
    "\n",
    "    hist = newton_backtracking(f, grad, hess, x0, **kwargs)\n",
    "\n",
    "    print(\"\\nFinal x* =\", hist[-1])\n",
    "    print(\"Final f* =\", f(*hist[-1]))\n",
    "    print(\"Iterations =\", len(hist)-1)\n",
    "\n",
    "    return hist, f, grad, hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sp.symbols(\"x y\", real=True)\n",
    "f_expr = (1-x)**2 + 100*(y - x**2)**2\n",
    "\n",
    "run_newton(f_expr, [x,y], x0=[-1.2, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2,x3 = sp.symbols(\"x1 x2 x3\")\n",
    "\n",
    "f_expr = x1**2 + x2**2 + 5*x3**2 - 3*x2 + x1*x3\n",
    "\n",
    "run_newton(f_expr, [x1,x2,x3], x0=[1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.Symbol(\"x\", real=True)\n",
    "\n",
    "f_expr = sp.sin(x) + x**4\n",
    "\n",
    "run_newton(f_expr, [x], x0=[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phần C — Bài tập hàm đơn giản\n",
    "Ba bài này giúp luyện Newton trên các hàm cơ bản: 1D bậc bốn (lồi), quadratic trục chuẩn 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton 1D với backtracking\n",
    "from typing import Callable\n",
    "\n",
    "def backtracking_1d(f: Callable[[float], float], df: Callable[[float], float], x: float,\n",
    "                    p: float, alpha_init: float = 1.0, rho: float = 0.5, c: float = 1e-4) -> float:\n",
    "    fx = f(x)\n",
    "    g = df(x)\n",
    "    alpha = alpha_init\n",
    "    while f(x + alpha * p) > fx + c * alpha * g * p:\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-12:\n",
    "            break\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def newton_1d(f: Callable[[float], float], df: Callable[[float], float], d2f: Callable[[float], float],\n",
    "              x0: float, max_iter: int = 50, tol: float = 1e-8) -> tuple[float, list]:\n",
    "    x = float(x0)\n",
    "    hist = [x]\n",
    "    for _ in range(max_iter):\n",
    "        g = df(x)\n",
    "        if abs(g) < tol:\n",
    "            break\n",
    "        H = d2f(x)\n",
    "        # tránh chia cho 0, thêm đệm nhỏ nếu cần\n",
    "        if abs(H) < 1e-12:\n",
    "            H = 1e-12 if H >= 0 else -1e-12\n",
    "        p = - g / H\n",
    "        alpha = backtracking_1d(f, df, x, p)\n",
    "        x = x + alpha * p\n",
    "        hist.append(x)\n",
    "    return x, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài: Newton 1D trên hàm bậc bốn\n",
    "Xét $f(x) = (x-2)^4 + 0.5\\,(x+1)^2$. Viết đạo hàm bậc nhất, bậc hai và chạy Newton từ nhiều điểm khởi tạo. Vẽ đồ thị 1D kèm các điểm lặp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bài 4\n",
    "f4 = lambda x: (x-2.0)**4 + 0.5*(x+1.0)**2\n",
    "df4 = lambda x: 4*(x-2.0)**3 + (x+1.0)\n",
    "d2f4 = lambda x: 12*(x-2.0)**2 + 1.0\n",
    "\n",
    "inits = [-4.0, 0.0, 4.0]\n",
    "traj_4 = []\n",
    "for x0 in inits:\n",
    "    x_star, hist = newton_1d(f4, df4, d2f4, x0)\n",
    "    traj_4.append((x0, x_star, hist))\n",
    "    print(f'Init {x0:5.1f} -> x*={x_star:.6f}, f*={f4(x_star):.6e}, iters={len(hist)-1}')\n",
    "\n",
    "# Vẽ\n",
    "xs = np.linspace(-5, 6, 600)\n",
    "ys = np.array([f4(x) for x in xs])\n",
    "plt.figure(figsize=(6.5,3.8))\n",
    "plt.plot(xs, ys, label='f4(x)')\n",
    "for x0, x_star, hist in traj_4:\n",
    "    pts_x = np.array(hist)\n",
    "    pts_y = np.array([f4(x) for x in pts_x])\n",
    "    plt.plot(pts_x, pts_y, '-o', ms=3, lw=1, label=f'Newton from {x0}')\n",
    "plt.xlabel('x'); plt.ylabel('f(x)'); plt.title('Bài 4: Newton 1D trên hàm bậc bốn')\n",
    "plt.legend(); plt.grid(True, alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BUILD FUNCTION MODEL (GENERAL 1D → nD)\n",
    "# ============================================================\n",
    "def build_function(f_expr, variables):\n",
    "\n",
    "    f = sp.lambdify(variables, f_expr, \"numpy\")\n",
    "\n",
    "    grad_expr = [sp.diff(f_expr, v) for v in variables]\n",
    "    grad = sp.lambdify(variables, grad_expr, \"numpy\")\n",
    "\n",
    "    hess_expr = sp.hessian(f_expr, variables)\n",
    "    hess = sp.lambdify(variables, hess_expr, \"numpy\")\n",
    "\n",
    "    return f, grad, hess, grad_expr, hess_expr\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEWTON METHOD FOR GENERAL DIMENSION\n",
    "# ============================================================\n",
    "def newton(f, grad, hess, x0, tol=1e-8, max_iter=50):\n",
    "\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    # unify to vector form\n",
    "    if x.ndim == 0:\n",
    "        x = x.reshape(1)\n",
    "\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        g = np.asarray(grad(*x), dtype=float)\n",
    "        H = np.asarray(hess(*x), dtype=float)\n",
    "\n",
    "        if g.ndim == 0:\n",
    "            g = np.array([g])\n",
    "        if H.ndim == 0:\n",
    "            H = np.array([[H]])\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            p = np.linalg.solve(H, -g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            p = -g\n",
    "\n",
    "        x = x + p\n",
    "        hist.append(x.copy())\n",
    "\n",
    "    # return list of scalars for 1D, vectors for nD\n",
    "    if len(x) == 1:\n",
    "        return [float(v[0]) for v in hist]  \n",
    "    else:\n",
    "        return np.array(hist)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PLOT FOR 1D CASE ONLY\n",
    "# ============================================================\n",
    "def plot_1D(f, trajectories, x_range=(-4,4)):\n",
    "\n",
    "    xs = np.linspace(x_range[0], x_range[1], 500)\n",
    "    ys = f(xs)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(xs, ys, label=\"f(x)\", color=\"black\")\n",
    "\n",
    "    colors = [\"red\",\"green\",\"blue\",\"purple\"]\n",
    "\n",
    "    for i,(x0,hist) in enumerate(trajectories.items()):\n",
    "        c = colors[i%len(colors)]\n",
    "\n",
    "        plt.plot(hist, f(np.array(hist)), \"o--\", color=c,\n",
    "                 label=f\"x0={x0} → {hist[-1]:.4f}\")\n",
    "\n",
    "    plt.title(\"Newton iteration paths 1D\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN DRIVER FUNCTION\n",
    "# ============================================================\n",
    "def run_newton(f_expr, variables, x0_list):\n",
    "\n",
    "    f, grad, hess, grad_expr, hess_expr = build_function(f_expr, variables)\n",
    "\n",
    "    print(\"\\n==== First derivative ====\")\n",
    "    for g in grad_expr:\n",
    "        print(g)\n",
    "\n",
    "    print(\"\\n==== Second derivative ====\")\n",
    "    print(hess_expr)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for x0 in x0_list:\n",
    "\n",
    "        hist = newton(f, grad, hess, x0)\n",
    "\n",
    "        results[x0] = hist\n",
    "\n",
    "        print(f\"\\nInit x0={x0}  →  x*={hist[-1]}   iter={len(hist)-1}\")\n",
    "\n",
    "    # plot only if 1D\n",
    "    if len(variables)==1:\n",
    "        plot_1D(f, results)\n",
    "\n",
    "    return results\n",
    "x = sp.Symbol(\"x\")\n",
    "f_expr = (x-2)**4 + 0.5*(x+1)**2\n",
    "\n",
    "run_newton(\n",
    "    f_expr,\n",
    "    [x],\n",
    "    x0_list=[-4, 0, 4]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# BUILD FUNCTION MODEL (GENERAL 1D → nD)\n",
    "# ===============================================================\n",
    "def build_function(f_expr, variables):\n",
    "\n",
    "    f = sp.lambdify(variables, f_expr, \"numpy\")\n",
    "\n",
    "    grad_expr = [sp.diff(f_expr, v) for v in variables]\n",
    "    grad = sp.lambdify(variables, grad_expr, \"numpy\")\n",
    "\n",
    "    hess_expr = sp.hessian(f_expr, variables)\n",
    "    hess = sp.lambdify(variables, hess_expr, \"numpy\")\n",
    "\n",
    "    return f, grad, hess, grad_expr, hess_expr\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# BACKTRACKING LINE SEARCH (GENERAL nD)\n",
    "# ===============================================================\n",
    "def backtracking(f, grad, x, p, alpha0=1.0, rho=0.5, c=1e-4):\n",
    "\n",
    "    fx = f(*x) if len(x)>1 else f(x[0])\n",
    "    g  = grad(*x) if len(x)>1 else np.array([grad(x[0])])\n",
    "\n",
    "    alpha = alpha0\n",
    "\n",
    "    while True:\n",
    "        x_new = x + alpha * p\n",
    "\n",
    "        f_new = f(*x_new) if len(x)>1 else f(x_new[0])\n",
    "\n",
    "        if f_new <= fx + c * alpha * np.dot(g, p):\n",
    "            break\n",
    "\n",
    "        alpha *= rho\n",
    "\n",
    "        if alpha < 1e-12:\n",
    "            break\n",
    "\n",
    "    return alpha\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# NEWTON + BACKTRACKING\n",
    "# ===============================================================\n",
    "def newton_backtracking(f, grad, hess, x0, tol=1e-8, max_iter=50):\n",
    "\n",
    "    x = np.array(x0, dtype=float)\n",
    "    x = x.reshape(-1)   # force vector format\n",
    "\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        g = np.asarray(grad(*x), dtype=float)\n",
    "        H = np.asarray(hess(*x), dtype=float)\n",
    "\n",
    "        # 1D special handling\n",
    "        if g.ndim == 0:\n",
    "            g = np.array([g])\n",
    "        if H.ndim == 0:\n",
    "            H = np.array([[H]])\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            p = -np.linalg.solve(H, g)\n",
    "        except:\n",
    "            p = -g\n",
    "\n",
    "        alpha = backtracking(f, grad, x, p)\n",
    "\n",
    "        x = x + alpha*p\n",
    "\n",
    "        hist.append(x.copy())\n",
    "\n",
    "    return np.array(hist)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# PLOTTING FOR 1D\n",
    "# ===============================================================\n",
    "def plot_1D(f, results, x_range=(-4,4)):\n",
    "\n",
    "    xs = np.linspace(x_range[0], x_range[1], 400)\n",
    "    ys = f(xs)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(xs, ys, label=\"f(x)\", color=\"black\")\n",
    "\n",
    "    colors = [\"red\",\"green\",\"blue\",\"purple\"]\n",
    "\n",
    "    for i,(x0,hist) in enumerate(results.items()):\n",
    "\n",
    "        c = colors[i%len(colors)]\n",
    "\n",
    "        xs_hist = hist.flatten()\n",
    "        plt.plot(xs_hist, f(xs_hist), \"o--\", color=c,\n",
    "                 label=f\"x0={x0} → {hist[-1][0]:.4f}\")\n",
    "\n",
    "    plt.title(\"Newton + Backtracking Path (1D)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MAIN WRAPPER\n",
    "# ===============================================================\n",
    "def run_newton_backtracking(f_expr, variables, x0_list):\n",
    "\n",
    "    f, grad, hess, grad_expr, hess_expr = build_function(f_expr, variables)\n",
    "\n",
    "    print(\"\\n==== First derivative ====\")\n",
    "    for g in grad_expr:\n",
    "        print(g)\n",
    "\n",
    "    print(\"\\n==== Hessian ====\")\n",
    "    print(hess_expr)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for x0 in x0_list:\n",
    "\n",
    "        hist = newton_backtracking(f, grad, hess, x0)\n",
    "\n",
    "                # build a usable dictionary key\n",
    "        if np.isscalar(x0):\n",
    "            key = x0                      # use raw number\n",
    "        else:\n",
    "            key = tuple(x0)              # use tuple\n",
    "        \n",
    "        results[key] = hist\n",
    "\n",
    "\n",
    "        print(f\"\\nInit x0={x0}  →  x*={hist[-1]}   iter={len(hist)-1}\")\n",
    "\n",
    "    if len(variables)==1:\n",
    "        plot_1D(f, results)\n",
    "\n",
    "    return results\n",
    "x = sp.Symbol(\"x\")\n",
    "\n",
    "f_expr = (x-2)**4 + 0.5*(x+1)**2\n",
    "\n",
    "run_newton_backtracking(\n",
    "    f_expr,\n",
    "    [x],\n",
    "    x0_list=[-4, 0, 4]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_1, x_2, \\dots, x_6) = \\sum_{i=1}^{5} \\left[ 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 6-D ROSENBROCK EXAMPLE\n",
    "# ==========================\n",
    "\n",
    "# 6 symbolic variables\n",
    "x1,x2,x3,x4,x5,x6 = sp.symbols(\"x1 x2 x3 x4 x5 x6\", real=True)\n",
    "\n",
    "variables = [x1,x2,x3,x4,x5,x6]\n",
    "\n",
    "# generalized rosenbrock\n",
    "f_expr = 0\n",
    "for i in range(5):\n",
    "    xi   = variables[i]\n",
    "    xip1 = variables[i+1]\n",
    "    f_expr += 100*(xip1 - xi**2)**2 + (1 - xi)**2\n",
    "\n",
    "# initial guesses\n",
    "x0_list = [\n",
    "    [-1.5,  1.5, -1,  2, -2,  1],   # difficult start\n",
    "    [0.5,   0.5, 0.5, 0.5, 0.5, 0.5], \n",
    "    [3,    -2,   1,  -1,   2, -1],\n",
    "]\n",
    "\n",
    "results = run_newton_backtracking(\n",
    "    f_expr,\n",
    "    variables,\n",
    "    x0_list=x0_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 5: Quadratic trục chuẩn 2D\n",
    "Xét $f(x)=\\tfrac{1}{2}x^\\top Q x + q^\\top x$ với $Q=\\operatorname{diag}(1, 50)$ và $q=(-2,1)$. So sánh Newton (H=Q) và GD, vẽ contour và quỹ đạo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bài 5\n",
    "Qd = np.diag([1.0, 50.0])\n",
    "qd = np.array([-2.0, 1.0])\n",
    "fd = lambda x: 0.5 * x @ Qd @ x + qd @ x\n",
    "gd = lambda x: Qd @ x + qd\n",
    "Hd = lambda x: Qd\n",
    "\n",
    "x0 = np.array([3.0, -3.0])\n",
    "xN, hN = newton_method(fd, gd, Hd, x0, max_iter=10)\n",
    "xG, hG = gradient_descent(fd, gd, x0, lr=5e-3, max_iter=20000)\n",
    "print('Diag Quad: Newton x* ~', xN, 'f* ~', fd(xN))\n",
    "print('Diag Quad: GD     x* ~', xG, 'f* ~', fd(xG))\n",
    "\n",
    "hN = np.array(hN); \n",
    "hG = np.array(hG)\n",
    "xv = np.linspace(-4, 4, 300)\n",
    "yv = np.linspace(-4, 4, 300)\n",
    "XX, YY = np.meshgrid(xv, yv)\n",
    "ZZ = 0.5*(Qd[0,0]*XX**2 + 2*Qd[0,1]*XX*YY + Qd[1,1]*YY**2) + qd[0]*XX + qd[1]*YY\n",
    "\n",
    "plt.figure(figsize=(6.2,5))\n",
    "cs = plt.contour(XX, YY, ZZ, levels=30, cmap='viridis')\n",
    "plt.clabel(cs, inline=True, fontsize=8)\n",
    "plt.plot(hG[:,0], hG[:,1], '-o', ms=2, lw=1, label='GD')\n",
    "plt.plot(hN[:,0], hN[:,1], '-s', ms=4, lw=1.5, label='Newton')\n",
    "plt.legend(); plt.xlabel('x'); plt.ylabel('y'); plt.title('Bài 5: Quadratic trục chuẩn — GD vs Newton')\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================================\n",
    "# 1. ĐỊNH NGHĨA HÀM QUADRATIC TỔNG QUÁT\n",
    "#     f(x) = 1/2 x^T Q x + q^T x\n",
    "# ==========================================================\n",
    "\n",
    "def quad_f(Q: np.ndarray, q: np.ndarray, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return 0.5 * x @ Q @ x + q @ x\n",
    "\n",
    "def quad_grad(Q: np.ndarray, q: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return Q @ x + q\n",
    "\n",
    "def quad_hess(Q: np.ndarray) -> np.ndarray:\n",
    "    # Hessian của quadratic là Q (hằng)\n",
    "    return Q\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2. NEWTON (H = Q CỐ ĐỊNH)\n",
    "# ==========================================================\n",
    "\n",
    "def newton_quadratic(Q, q, x0, tol=1e-8, max_iter=20):\n",
    "    \"\"\"\n",
    "    Newton cho hàm quadratic f(x) = 1/2 x^T Q x + q^T x.\n",
    "    Với Q đối xứng xác định dương → hội tụ 1 bước từ mọi x0.\n",
    "    Nhưng ta vẫn lặp để xem quỹ đạo.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x0, dtype=float)\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "\n",
    "    H = quad_hess(Q)\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        g = quad_grad(Q, q, x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        # Bước Newton: Q p = -g  →  p = -Q^{-1} g\n",
    "        p = -np.linalg.solve(H, g)\n",
    "\n",
    "        x = x + p\n",
    "        hist.append(x.copy())\n",
    "\n",
    "    return np.array(hist)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3. GRADIENT DESCENT (CỐ ĐỊNH LEARNING RATE)\n",
    "# ==========================================================\n",
    "\n",
    "def gradient_descent_quadratic(Q, q, x0, learning_rate=0.03, tol=1e-8, max_iter=200000):\n",
    "    \"\"\"\n",
    "    GD cho quadratic. \n",
    "    Với step size α < 2 / L (L = eigenvalue max của Q) thì hội tụ.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x0, dtype=float)\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "\n",
    "    hist = [x.copy()]\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        g = quad_grad(Q, q, x)\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        x = x - learning_rate * g\n",
    "        hist.append(x.copy())\n",
    "\n",
    "    return np.array(hist)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4. VẼ CONTOUR VÀ QUỸ ĐẠO (CHỈ DÙNG KHI dim = 2)\n",
    "# ==========================================================\n",
    "\n",
    "def plot_quadratic_2d(Q, q, newton_path, gd_path):\n",
    "    assert Q.shape == (2,2), \"Contour 2D chỉ hỗ trợ Q 2x2\"\n",
    "    assert q.shape == (2,), \"q phải có chiều 2 cho plot 2D\"\n",
    "\n",
    "    # Lấy tất cả điểm để chọn vùng vẽ\n",
    "    all_points = np.vstack([newton_path, gd_path])\n",
    "    x_min, x_max = all_points[:,0].min() - 1, all_points[:,0].max() + 1\n",
    "    y_min, y_max = all_points[:,1].min() - 1, all_points[:,1].max() + 1\n",
    "\n",
    "    xs = np.linspace(x_min, x_max, 200)\n",
    "    ys = np.linspace(y_min, y_max, 200)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "\n",
    "    # f(x) = 1/2(x^T Q x) + q^T x, với x = (X, Y)\n",
    "    Z = (\n",
    "        0.5 * (\n",
    "            Q[0,0]*X**2 + 2*Q[0,1]*X*Y + Q[1,1]*Y**2\n",
    "        )\n",
    "        + q[0]*X + q[1]*Y\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    cs = plt.contour(X, Y, Z, levels=30)\n",
    "    plt.clabel(cs, inline=True, fontsize=8, fmt=\"%.1f\")\n",
    "\n",
    "    # Path Newton\n",
    "    plt.plot(newton_path[:,0], newton_path[:,1], 'ro--', label='Newton')\n",
    "\n",
    "    # Path GD\n",
    "    plt.plot(gd_path[:,0], gd_path[:,1], 'go--', label='Gradient Descent')\n",
    "\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Contour f(x) và quỹ đạo Newton vs GD')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5. HÀM ĐIỀU PHỐI TỔNG QUÁT\n",
    "# ==========================================================\n",
    "\n",
    "def run_quadratic_demo(Q, q, x0, gd_lr=0.03, gd_max_iter=20000):\n",
    "    \"\"\"\n",
    "    Chạy full:\n",
    "      - tính nghiệm tối ưu x*\n",
    "      - Newton path (H=Q)\n",
    "      - GD path\n",
    "      - nếu dim=2 → vẽ contour + path\n",
    "      - nếu dim>2 → vẽ f(x_k) theo iteration\n",
    "    \"\"\"\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    x0 = np.asarray(x0, dtype=float)\n",
    "\n",
    "    dim = Q.shape[0]\n",
    "    assert Q.shape == (dim, dim)\n",
    "    assert q.shape == (dim,)\n",
    "\n",
    "    # nghiệm tối ưu analytic: Q x* + q = 0 → x* = -Q^{-1} q\n",
    "    x_star = -np.linalg.solve(Q, q)\n",
    "    print(\"x* (analytic) =\", x_star)\n",
    "    print(\"f(x*) =\", quad_f(Q, q, x_star))\n",
    "\n",
    "    # Newton\n",
    "    newton_path = newton_quadratic(Q, q, x0)\n",
    "    print(f\"\\nNewton: iters = {len(newton_path)-1}\")\n",
    "    print(\"  last x =\", newton_path[-1])\n",
    "    print(\"  f(last) =\", quad_f(Q, q, newton_path[-1]))\n",
    "\n",
    "    # GD\n",
    "    gd_path = gradient_descent_quadratic(Q, q, x0, learning_rate=gd_lr, max_iter=gd_max_iter)\n",
    "    print(f\"\\nGD: iters = {len(gd_path)-1}\")\n",
    "    print(\"  last x =\", gd_path[-1])\n",
    "    print(\"  f(last) =\", quad_f(Q, q, gd_path[-1]))\n",
    "\n",
    "    # Plot\n",
    "    if dim == 2:\n",
    "        plot_quadratic_2d(Q, q, newton_path, gd_path)\n",
    "    else:\n",
    "        # vẽ f(x_k) theo iteration cho nD\n",
    "        f_newton = [quad_f(Q,q,x) for x in newton_path]\n",
    "        f_gd     = [quad_f(Q,q,x) for x in gd_path]\n",
    "\n",
    "        plt.figure(figsize=(7,4))\n",
    "        plt.semilogy(f_newton, 'ro--', label='Newton')\n",
    "        plt.semilogy(f_gd, 'go--', label='GD')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('f(x_k) (log scale)')\n",
    "        plt.title('Convergence on quadratic (nD)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return newton_path, gd_path, x_star\n",
    "# ===== BÀI 5: QUADRATIC TRỤC CHUẨN 2D =====\n",
    "Q = np.diag([1.0, 50.0])\n",
    "q = np.array([-2.0, 1.0])\n",
    "\n",
    "x0 = np.array([3.0, -3.0])   # bạn đổi thoải mái để xem quỹ đạo khác\n",
    "gd_lr = 5e-3             # < 2/L = 2/50 = 0.04 → hội tụ\n",
    "\n",
    "newton_path, gd_path, x_star = run_quadratic_demo(Q, q, x0, gd_lr=gd_lr, gd_max_iter=20000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\mathbf{x}) = 10n + \\sum_{i=1}^{n} \\left[ x_i^2 - 10\\cos(2\\pi x_i) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    f(x) = sum_{i=1}^{n-1} [ (1-x_i)^2 + 100(x_{i+1}-x_i^2)^2 ]\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    return np.sum((1 - x[:-1])**2 + 100*(x[1:] - x[:-1]**2)**2)\n",
    "\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    g = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        if i > 0:\n",
    "            g[i] += 200*(x[i] - x[i-1]**2)\n",
    "        if i < n-1:\n",
    "            g[i] += -2*(1-x[i]) - 400*x[i]*(x[i+1]-x[i]**2)\n",
    "    return g\n",
    "\n",
    "\n",
    "def hess_rosenbrock(x):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    H = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        if i < n-1:\n",
    "            H[i,i] += 2 - 400*(x[i+1]-x[i]**2) + 800*x[i]**2\n",
    "            H[i,i+1] += -400*x[i]\n",
    "        if i > 0:\n",
    "            H[i,i] += 200\n",
    "            H[i,i-1] += -400*x[i-1]\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(f, grad, x, p, alpha0=1.0, rho=0.5, c=1e-4):\n",
    "    alpha = alpha0\n",
    "    fx = f(x)\n",
    "    g = grad(x)\n",
    "\n",
    "    while f(x + alpha*p) > fx + c*alpha*np.dot(g, p):\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-12:\n",
    "            break\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, grad, hess, x0, tol=1e-6, max_iter=50, backtrack=True):\n",
    "    x = x0.copy()\n",
    "    hist_x = [x.copy()]\n",
    "    hist_f = [f(x)]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        g = grad(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        H = hess(x)\n",
    "        try:\n",
    "            p = -np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            p = -np.linalg.pinv(H) @ g\n",
    "\n",
    "        alpha = backtracking(f, grad, x, p) if backtrack else 1.0\n",
    "        x = x + alpha*p\n",
    "\n",
    "        hist_x.append(x.copy())\n",
    "        hist_f.append(f(x))\n",
    "\n",
    "    return np.array(hist_x), np.array(hist_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad, x0, lr=1e-3, tol=1e-6, max_iter=20000):\n",
    "    x = x0.copy()\n",
    "    hist_x = [x.copy()]\n",
    "    hist_f = [f(x)]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        g = grad(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "        x = x - lr*g\n",
    "        hist_x.append(x.copy())\n",
    "        hist_f.append(f(x))\n",
    "\n",
    "    return np.array(hist_x), np.array(hist_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== INPUT ======\n",
    "n = 2                      # đổi n tùy ý\n",
    "x0 = np.array([-1.2, 1.0]) # điểm khởi tạo chuẩn Rosenbrock\n",
    "\n",
    "# Newton có backtracking\n",
    "xn_bt, fn_bt = newton_method(\n",
    "    rosenbrock, grad_rosenbrock, hess_rosenbrock,\n",
    "    x0, backtrack=True\n",
    ")\n",
    "\n",
    "# Newton full-step (không backtracking)\n",
    "xn_full, fn_full = newton_method(\n",
    "    rosenbrock, grad_rosenbrock, hess_rosenbrock,\n",
    "    x0, backtrack=False\n",
    ")\n",
    "\n",
    "# Gradient Descent\n",
    "xg, fg = gradient_descent(\n",
    "    rosenbrock, grad_rosenbrock, x0, lr=1e-3\n",
    ")\n",
    "\n",
    "print(\"Newton + BT iters:\", len(fn_bt)-1)\n",
    "print(\"Newton full iters:\", len(fn_full)-1)\n",
    "print(\"GD iters:\", len(fg)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.semilogy(fn_bt, label=\"Newton + Backtracking\")\n",
    "plt.semilogy(fn_full, label=\"Newton full-step\")\n",
    "plt.semilogy(fg, label=\"Gradient Descent\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.title(\"Convergence on Rosenbrock\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n == 2:\n",
    "    x = np.linspace(-2, 2, 400)\n",
    "    y = np.linspace(-1, 3, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = (1-X)**2 + 100*(Y-X**2)**2\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contour(X, Y, Z, levels=30)\n",
    "    plt.plot(xn_bt[:,0], xn_bt[:,1], 'o-', label=\"Newton + BT\")\n",
    "    plt.plot(xn_full[:,0], xn_full[:,1], 's--', label=\"Newton full\")\n",
    "    plt.plot(xg[:,0], xg[:,1], 'x-', label=\"GD\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Trajectories on Rosenbrock\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ĐỔI HÀM Ở ĐÂY =====\n",
    "def f(x):\n",
    "    return (x[0] - 3)**2 + 2\n",
    "\n",
    "x0 = np.array([0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sum((x - 3)**2) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = grad(lambda x: f(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, grad_f, x, p, alpha0=1.0, rho=0.5, c=1e-4):\n",
    "    alpha = alpha0\n",
    "    fx = f(x)\n",
    "    g = grad_f(x)\n",
    "\n",
    "    while f(x + alpha * p) > fx + c * alpha * np.dot(g, p):\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-12:\n",
    "            break\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_line_search_quadratic(A, g):\n",
    "    return (g @ g) / (g @ A @ g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    f,\n",
    "    grad_f,\n",
    "    x0,\n",
    "    lr=0.1,\n",
    "    max_iter=100,\n",
    "    tol=1e-8,\n",
    "    use_backtracking=False,\n",
    "    use_exact_line_search=False,\n",
    "    A=None,          # chỉ dùng cho exact line search\n",
    "    verbose=True\n",
    "):\n",
    "    x = x0.copy()\n",
    "    xs = [x.copy()]\n",
    "    fs = [f(x)]\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Hội tụ tại iter {k}, ||grad|| = {np.linalg.norm(g):.2e}\")\n",
    "            break\n",
    "\n",
    "        p = -g\n",
    "\n",
    "        # chọn step size\n",
    "        if use_exact_line_search:\n",
    "            if A is None:\n",
    "                raise ValueError(\"Exact line search cần ma trận A\")\n",
    "            alpha = exact_line_search_quadratic(A, g)\n",
    "        elif use_backtracking:\n",
    "            alpha = backtracking_line_search(f, grad_f, x, p)\n",
    "        else:\n",
    "            alpha = lr\n",
    "\n",
    "        x = x + alpha * p\n",
    "        xs.append(x.copy())\n",
    "        fs.append(f(x))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iter {k+1:3d}: x = {x}, f(x) = {fs[-1]:.6f}, alpha = {alpha:.3e}\")\n",
    "\n",
    "    return np.array(xs), np.array(fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_convergence_condition(P):\n",
    "    eigvals = np.linalg.eigvals(P)\n",
    "    L = np.max(np.real(eigvals))\n",
    "    print(\"\\n===== ĐIỀU KIỆN HỘI TỤ GD =====\")\n",
    "    print(\"lambda_max(P) =\", L)\n",
    "    print(\"Hội tụ nếu: 0 < lr < 2 / L =\", 2/L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n===== GD với lr = {lr} =====\")\n",
    "    xs, fs = gradient_descent(\n",
    "        f, grad_f, x0,\n",
    "        lr=lr,\n",
    "        max_iter=50,\n",
    "        use_backtracking=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    histories[lr] = fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "for lr, fs in histories.items():\n",
    "    plt.semilogy(np.abs(fs - 2), label=f\"lr={lr}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"|f(x) - f(x*)|\")\n",
    "plt.title(\"Gradient Descent – Bài 1.1\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_bt, fs_bt = gradient_descent(\n",
    "    f, grad_f, x0,\n",
    "    lr=1.0,\n",
    "    max_iter=50,\n",
    "    use_backtracking=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2.0]])  # Hessian của (x-3)^2\n",
    "\n",
    "xs_exact, fs_exact = gradient_descent(\n",
    "    f, grad_f, x0,\n",
    "    max_iter=20,\n",
    "    use_exact_line_search=True,\n",
    "    A=A,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_convergence_condition(\n",
    "    f,\n",
    "    dim,\n",
    "    sample_points=None,\n",
    "    assume_quadratic=False\n",
    "):\n",
    "    \"\"\"\n",
    "    In điều kiện hội tụ Gradient Descent cho hàm f(x)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Hàm mục tiêu f(x), x là vector R^n\n",
    "    dim : int\n",
    "        Số chiều của x\n",
    "    sample_points : list[np.ndarray], optional\n",
    "        Danh sách điểm để ước lượng Hessian (nếu không phải quadratic)\n",
    "    assume_quadratic : bool\n",
    "        Nếu biết chắc f là quadratic → Hessian hằng\n",
    "    \"\"\"\n",
    "\n",
    "    grad_f = grad(f)\n",
    "    hess_f = hessian(f)\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"ĐIỀU KIỆN HỘI TỤ GRADIENT DESCENT\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    if assume_quadratic:\n",
    "        x0 = np.zeros(dim)\n",
    "        H = hess_f(x0)\n",
    "        eigvals = np.linalg.eigvals(H)\n",
    "        L = np.max(np.real(eigvals))\n",
    "\n",
    "        print(\"→ Hàm được giả sử là quadratic\")\n",
    "        print(\"Hessian =\\n\", H)\n",
    "        print(\"Eigenvalues =\", eigvals)\n",
    "        print(f\"L = lambda_max(H) = {L:.6f}\")\n",
    "\n",
    "    else:\n",
    "        if sample_points is None:\n",
    "            raise ValueError(\"Cần sample_points để ước lượng L\")\n",
    "\n",
    "        L_list = []\n",
    "        for x in sample_points:\n",
    "            H = hess_f(x)\n",
    "            eigvals = np.linalg.eigvals(H)\n",
    "            L_list.append(np.max(np.real(eigvals)))\n",
    "\n",
    "        L = max(L_list)\n",
    "\n",
    "        print(\"→ Hàm tổng quát (ước lượng Lipschitz constant)\")\n",
    "        print(\"Các giá trị L ước lượng:\", L_list)\n",
    "        print(f\"L (max) ≈ {L:.6f}\")\n",
    "\n",
    "    print(\"\\n👉 Điều kiện hội tụ Gradient Descent:\")\n",
    "    print(f\"    0 < learning_rate < 2 / L = {2/L:.6f}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, hessian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return (x[0] - 3)**2 + 2\n",
    "\n",
    "L = print_convergence_condition(\n",
    "    f=f1,\n",
    "    dim=1,\n",
    "    assume_quadratic=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return np.sum((x - 3)**2)\n",
    "\n",
    "L = print_convergence_condition(\n",
    "    f=f2,\n",
    "    dim=5,\n",
    "    assume_quadratic=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return (1-x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "sample_points = [\n",
    "    np.array([-1.2, 1.0]),\n",
    "    np.array([0.0, 0.0]),\n",
    "    np.array([1.0, 1.0])\n",
    "]\n",
    "\n",
    "L = print_convergence_condition(\n",
    "    f=rosenbrock,\n",
    "    dim=2,\n",
    "    sample_points=sample_points,\n",
    "    assume_quadratic=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
