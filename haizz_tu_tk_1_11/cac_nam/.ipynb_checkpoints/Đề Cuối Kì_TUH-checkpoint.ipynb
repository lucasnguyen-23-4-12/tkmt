{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ececfc-cade-43bf-8edc-39b2ad38cb07",
   "metadata": {},
   "source": [
    "# Câu 1: ( 6 điểm )\n",
    "Cho ma trận \n",
    "P = $\\begin{bmatrix} 1 & 0 \\\\ 0 & 4 \\end{bmatrix}$  và vector q = $(-1, 4)$. Xét bài toán tối ưu sau:\n",
    "\\begin{align}\n",
    "\\min_{x \\in \\mathbb{R}^2} f(x) = \\frac{1}{2} x^T P x + q^T x + \\frac{5}{2} \\tag{1}  (1)\n",
    "\\end{align}\n",
    "a) ( 2 điểm ) Xác định điểm tối ưu $x^\\ast$ và giá trị tối ưu $p^\\ast$ của bài toán (1).\n",
    "\n",
    "b) ( 2 điểm ) Chuyển bài toán (1) về bài toán least square (bình phương tối thiểu) sau bằng cách chỉ ra ma trận ( A ) và vector ( b ):\n",
    "\\begin{align}\n",
    "\\min_{x \\in \\mathbb{R}^2} \\frac{1}{2} |Ax - b|_2^2 \\tag{2}\n",
    "\\end{align}\n",
    "Biết rằng ( A ) là ma trận đường chéo và vector ( b ) có các thành phần là các số thực dương.\n",
    "\n",
    "c) ( 1 điểm ) Sử dụng thuật toán Gradient Descent cho bài toán trên (bài (1) hoặc (2)), với giá trị $x$ ban đầu là $x^{(0)} = (-1, 2)$. Sử dụng learning rate lần lượt là 0.4 và 0.6 và thực hiện tối đa 100 vòng lặp. In ra giá trị của $x^{(k)}, f(x^{(k)})$ tương ứng sau mỗi vòng lặp thứ ( k ) cho tới khi sai số $|f(x^{(k)}) - p^\\ast|$ trong cả hai trường hợp của learning rate. Từ đó hãy đưa ra kết luận về sự hội tụ của thuật toán Gradient Descent trong từng trường hợp của learning rate.\n",
    "\n",
    "d) ( 1 điểm ) Trả lời câu a) với ma trận A = $\\begin{bmatrix} 1 & -2 \\\\ 2 & -4 \\end{bmatrix}$ và vector b = $(2;2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cc512",
   "metadata": {},
   "source": [
    "a) ( 2 điểm ) Xác định điểm tối ưu $x^\\ast$ và giá trị tối ưu $p^\\ast$ của bài toán (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f99221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điểm tối ưu x_opt = [ 1. -1.]\n",
      "Giá trị tối ưu p_opt = 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Giả sử chúng ta có một bài toán tối ưu khác với ma trận P và vector q khác nhau.\n",
    "# Ví dụ, chúng ta có ma trận P và vector q như sau:\n",
    "P = np.array([[1, 0], [0, 4]])\n",
    "q = np.array([-1, 4])\n",
    "\n",
    "# a) Xác định điểm tối ưu và giá trị tối ưu\n",
    "# Đầu tiên, chúng ta cần tìm điểm tối ưu x_opt bằng cách giải phương trình P @ x_opt = -q.\n",
    "# Sau đó, chúng ta tính giá trị tối ưu p_opt bằng cách thay x_opt vào hàm mục tiêu.\n",
    "# Ở đây, chúng ta sử dụng toán tử @ để biểu diễn tích ma trận. Toán tử @ được giới thiệu trong Python 3.5 và được sử dụng để thay thế cho np.dot() hoặc np.matmul() để biểu diễn tích ma trận.\n",
    "# Ví dụ, nếu chúng ta có hai ma trận A và B, chúng ta có thể tính tích của chúng bằng cách sử dụng toán tử @ như sau:\n",
    "# C = A @ B\n",
    "# Điều này tương đương với np.dot(A, B) hoặc np.matmul(A, B).\n",
    "x_opt = np.linalg.inv(P) @ (-q)\n",
    "p_opt = 0.5 * np.dot(x_opt, P @ x_opt) + np.dot(q, x_opt) + 2.5\n",
    "\n",
    "print(f\"Điểm tối ưu x_opt = {x_opt}\")\n",
    "print(f\"Giá trị tối ưu p_opt = {p_opt}\")\n",
    "\n",
    "# Các loại biểu thức có thể xảy ra:\n",
    "# 1. Biểu thức tuyến tính: f(x) = a^T x + b\n",
    "# 2. Biểu thức bậc hai: f(x) = 0.5 * x^T * P * x + q^T * x + r\n",
    "# 3. Biểu thức chứa hàm mũ: f(x) = 0.5 * x^T * P * x + q^T * x + r + e^(x^T * P * x)\n",
    "# 4. Biểu thức chứa hàm log: f(x) = 0.5 * x^T * P * x + q^T * x + r + log(x^T * P * x)\n",
    "# 5. Biểu thức chứa hàm sin, cos: f(x) = 0.5 * x^T * P * x + q^T * x + r + sin(x^T * P * x)\n",
    "\n",
    "# Ví dụ, nếu hàm mục tiêu có dạng f(x) = 0.5 * x^T * P * x + q^T * x + 2.5 + e^(x^T * P * x), chúng ta có thể tính toán như sau:\n",
    "# p_opt = 0.5 * np.dot(x_opt, P @ x_opt) + np.dot(q, x_opt) + 2.5 + np.exp(np.dot(x_opt, P @ x_opt))\n",
    "\n",
    "# Phương trình (1) được viết lại dưới dạng:\n",
    "# f(x) = 0.5 * x^T * P * x + q^T * x + 2.5 + e^(x^T * P * x)\n",
    "# Với P và q đã được định nghĩa ở trên.\n",
    "# Điểm tối ưu x_opt và giá trị tối ưu p_opt đã được tính toán và in ra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa1b057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., -1.]), 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dữ liệu đề bài cung cấp\n",
    "P = np.array([[1, 0], [0, 4]])  # Ma trận P\n",
    "q = np.array([-1, 4])           # Vector q\n",
    "\n",
    "# Công thức tính điểm tối ưu x*: P * x + q = 0 => x* = -P^(-1) * q\n",
    "x_star = -np.linalg.inv(P).dot(q)\n",
    "\n",
    "# Tính giá trị tối ưu p* bằng cách thay x* vào f(x)\n",
    "# f(x) = 0.5 * x^T * P * x + q^T * x + 5/2\n",
    "p_star = 0.5 * x_star.T.dot(P).dot(x_star) + q.T.dot(x_star) + 5 / 2\n",
    "\n",
    "x_star, p_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061ca84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede242f1",
   "metadata": {},
   "source": [
    "b) ( 2 điểm ) Chuyển bài toán (1) về bài toán least square (bình phương tối thiểu) sau bằng cách chỉ ra ma trận ( A ) và vector ( b ):\n",
    "\\begin{align}\n",
    "\\min_{x \\in \\mathbb{R}^2} \\frac{1}{2} |Ax - b|_2^2 \\tag{2}\n",
    "\\end{align}\n",
    "Biết rằng ( A ) là ma trận đường chéo và vector ( b ) có các thành phần là các số thực dương."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feba714",
   "metadata": {},
   "source": [
    "b) Least Square\n",
    "Giả sử $A = \\begin{bmatrix} x & 0 \\\\ 0 & y \\end{bmatrix}$ và b = $(b1;b2)$ với $b1, b2 > 0$. Khi đó ta có:\n",
    "\n",
    "\\begin{cases}\n",
    "A^T A &= P \\\\\n",
    "b^T b &= 5 \\\\\n",
    "A^T b &= -q\n",
    "\\end{cases}\n",
    "\n",
    "Suy ra $x^2 = 1, y^2 = 4, x, b_1 = 1, y, b_2 = -4$ và $b_1^2 + b_2^2 = 5$, Mà $b_1, b_2 > 0$ nên $x>0, y>0$. Do đó $x = 1, y = -2$ và $b_1 = 1, b_2 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b348e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ma trận A = [[1. 0.]\n",
      " [0. 2.]]\n",
      "Vector b = [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# b) Chuyển bài toán (1) về bài toán least square\n",
    "# Đầu tiên, chúng ta cần tìm ma trận A và vector b sao cho bài toán (1) trở thành bài toán least square (2).\n",
    "# Ma trận A là ma trận đường chéo với các phần tử trên đường chéo chính là căn bậc hai của các phần tử trên đường chéo chính của ma trận P.\n",
    "# Vector b là vector chứa các phần tử là căn bậc hai của các phần tử của vector q.\n",
    "\n",
    "# Có một số trường hợp khác cần xem xét:\n",
    "# 1. Trường hợp ma trận P không phải là ma trận đường chéo: trong trường hợp này, chúng ta cần tìm cách chuyển đổi ma trận P thành ma trận đường chéo trước khi tính toán.\n",
    "# 2. Trường hợp vector q chứa các phần tử âm: trong trường hợp này, chúng ta cần sử dụng hàm abs() để lấy giá trị tuyệt đối của các phần tử trong vector q trước khi tính toán.\n",
    "# 3. Trường hợp ma trận A không phải là ma trận vuông: trong trường hợp này, chúng ta cần sử dụng hàm np.linalg.pinv() để tính toán nghịch đảo của ma trận A.\n",
    "\n",
    "A = np.array([[np.sqrt(P[0, 0]), 0], [0, np.sqrt(P[1, 1])]])\n",
    "b = np.sqrt(np.abs(q))  # Sửa để tránh lỗi sqrt của số âm\n",
    "\n",
    "print(f\"Ma trận A = {A}\")\n",
    "print(f\"Vector b = {b}\")\n",
    "\n",
    "# Phương trình (2) được viết lại dưới dạng:\n",
    "# f(x) = 0.5 * |Ax - b|_2^2\n",
    "# Với A và b đã được định nghĩa ở trên.\n",
    "# Ma trận A và vector b đã được tính toán và in ra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76da51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0.],\n",
       "        [0., 2.]]),\n",
       " array([ 1., -2.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chuyển bài toán về dạng Least Squares\n",
    "# Dạng bài toán Least Squares: min (1/2) * ||A * x - b||^2_2\n",
    "\n",
    "# A là ma trận đường chéo => từ P suy ra A\n",
    "A = np.sqrt(P)  # Lấy căn bậc hai của các phần tử trên đường chéo của P\n",
    "\n",
    "# Vector b => từ q suy ra b: A * x - b = 0 <=> b = A * (-P^(-1) * q)\n",
    "b = A.dot(x_star)\n",
    "\n",
    "A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29468f46",
   "metadata": {},
   "source": [
    "c) ( 1 điểm ) Sử dụng thuật toán Gradient Descent cho bài toán trên (bài (1) hoặc (2)), với giá trị $x$ ban đầu là $x^{(0)} = (-1, 2)$. Sử dụng learning rate lần lượt là 0.4 và 0.6 và thực hiện tối đa 100 vòng lặp. In ra giá trị của $x^{(k)}, f(x^{(k)})$ tương ứng sau mỗi vòng lặp thứ ( k ) cho tới khi sai số $|f(x^{(k)}) - p^\\ast|$ trong cả hai trường hợp của learning rate. Từ đó hãy đưa ra kết luận về sự hội tụ của thuật toán Gradient Descent trong từng trường hợp của learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65fbadd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Learning Rate: 0.4 ---\n",
      "   Iteration                                            x      f(x)\n",
      "0          0  [-0.19999999999999996, -2.8000000000000007]  7.200000\n",
      "1          1                  [0.28, 0.08000000000000052]  2.592000\n",
      "2          2    [0.5680000000000001, -1.6480000000000004]  0.933120\n",
      "3          3                [0.7408, -0.6111999999999997]  0.335923\n",
      "4          4               [0.84448, -1.2332800000000002]  0.120932\n",
      "5          5              [0.906688, -0.8600319999999999]  0.043536\n",
      "6          6                      [0.9440128, -1.0839808]  0.015673\n",
      "7          7                    [0.96640768, -0.94961152]  0.005642\n",
      "8          8                  [0.979844608, -1.030233088]  0.002031\n",
      "9          9                [0.9879067648, -0.9818601472]  0.000731\n",
      "...\n",
      "\n",
      "--- Learning Rate: 0.6 ---\n",
      "   Iteration                                          x          f(x)\n",
      "0          0  [0.19999999999999996, -5.199999999999999]     35.600000\n",
      "1          1    [0.6799999999999999, 4.879999999999999]     69.200000\n",
      "2          2                [0.872, -9.231999999999998]    135.539840\n",
      "3          3               [0.9488, 10.524799999999997]    265.643341\n",
      "4          4             [0.97952, -17.134719999999994]    520.658589\n",
      "5          5             [0.991808, 21.588607999999994]   1020.490456\n",
      "6          6            [0.9967232, -32.62405119999999]   2000.161234\n",
      "7          7           [0.99868928, 43.273671679999985]   3920.316009\n",
      "8          8          [0.999475712, -62.98314035199997]   7683.819376\n",
      "9          9          [0.9997902848, 85.77639649279996]  15060.285977\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent cho bài toán Least Squares\n",
    "def gradient_descent(A, b, x_init, learning_rate, max_iters, tolerance=1e-6):\n",
    "    x = x_init\n",
    "    history = []  # Lưu lại lịch sử các giá trị x và f(x)\n",
    "    for k in range(max_iters):\n",
    "        gradient = A.T.dot(A.dot(x) - b)  # Gradient của hàm Least Squares\n",
    "        x = x - learning_rate * gradient  # Cập nhật x\n",
    "        f_val = 0.5 * np.linalg.norm(A.dot(x) - b)**2  # Giá trị hàm mục tiêu\n",
    "        history.append((k, x.copy(), f_val))\n",
    "        if len(history) > 1 and abs(history[-1][2] - history[-2][2]) < tolerance:\n",
    "            break  # Kiểm tra điều kiện hội tụ\n",
    "    return history\n",
    "\n",
    "# Dữ liệu đầu vào\n",
    "x_init = np.array([-1, 2])  # Giá trị khởi tạo x^(0)\n",
    "learning_rates = [0.4, 0.6]  # Hai learning rates\n",
    "max_iters = 100  # Số vòng lặp tối đa\n",
    "\n",
    "# Chạy thuật toán cho từng learning rate\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "    results[lr] = gradient_descent(A, b, x_init, lr, max_iters)\n",
    "\n",
    "# Hiển thị lịch sử hội tụ của thuật toán\n",
    "import pandas as pd\n",
    "\n",
    "for lr, history in results.items():\n",
    "    df = pd.DataFrame(history, columns=[\"Iteration\", \"x\", \"f(x)\"])\n",
    "    print(f\"\\n--- Learning Rate: {lr} ---\")\n",
    "    print(df.head(10))  # Hiển thị 10 vòng lặp đầu tiên\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1748cab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration                                               x      f(x)\n",
      "0           0                                 [[0.1], [-0.4]]  1.125000\n",
      "1           1                               [[0.19], [-0.64]]  0.587250\n",
      "2           2                             [[0.271], [-0.784]]  0.359032\n",
      "3           3  [[0.34390000000000004], [-0.8704000000000001]]  0.248826\n",
      "4           4  [[0.40951000000000004], [-0.9222400000000001]]  0.186432\n",
      "5           5  [[0.46855900000000006], [-0.9533440000000001]]  0.145568\n",
      "6           6            [[0.5217031000000001], [-0.9720064]]  0.115951\n",
      "7           7           [[0.5695327900000001], [-0.98320384]]  0.093215\n",
      "8           8          [[0.6125795110000001], [-0.989922304]]  0.075250\n",
      "9           9         [[0.6513215599000001], [-0.9939533824]]  0.060861\n",
      "10         10             [[0.68618940391], [-0.99637202944]]  0.049265\n",
      "11         11       [[0.717570463519], [-0.9978232176640001]]  0.039893\n",
      "12         12      [[0.7458134171671], [-0.9986939305984001]]  0.032309\n",
      "13         13     [[0.7712320754503901], [-0.99921635835904]]  0.026169\n",
      "14         14    [[0.7941088679053511], [-0.999529815015424]]  0.021196\n",
      "15         15    [[0.814697981114816], [-0.9997178890092544]]  0.017169\n",
      "16         16   [[0.8332281830033343], [-0.9998307334055526]]  0.013906\n",
      "17         17   [[0.8499053647030009], [-0.9998984400433316]]  0.011264\n",
      "18         18    [[0.8649148282327008], [-0.999939064025999]]  0.009124\n",
      "19         19   [[0.8784233454094308], [-0.9999634384155994]]  0.007390\n",
      "20         20   [[0.8905810108684877], [-0.9999780630493597]]  0.005986\n",
      "21         21    [[0.901522909781639], [-0.9999868378296158]]  0.004849\n",
      "22         22    [[0.911370618803475], [-0.9999921026977695]]  0.003928\n",
      "23         23   [[0.9202335569231275], [-0.9999952616186617]]  0.003181\n",
      "24         24   [[0.9282102012308148], [-0.9999971569711971]]  0.002577\n",
      "25         25   [[0.9353891811077333], [-0.9999982941827182]]  0.002087\n",
      "26         26      [[0.94185026299696], [-0.999998976509631]]  0.001691\n",
      "27         27    [[0.947665236697264], [-0.9999993859057786]]  0.001369\n",
      "28         28   [[0.9528987130275376], [-0.9999996315434672]]  0.001109\n",
      "29         29   [[0.9576088417247839], [-0.9999997789260803]]  0.000899\n",
      "30         30   [[0.9618479575523055], [-0.9999998673556482]]  0.000728\n",
      "31         31   [[0.9656631617970749], [-0.9999999204133889]]  0.000590\n",
      "32         32   [[0.9690968456173674], [-0.9999999522480334]]  0.000478\n",
      "33         33     [[0.9721871610556306], [-0.99999997134882]]  0.000387\n",
      "34         34    [[0.9749684449500675], [-0.999999982809292]]  0.000313\n",
      "35         35   [[0.9774716004550608], [-0.9999999896855752]]  0.000254\n",
      "36         36   [[0.9797244404095546], [-0.9999999938113452]]  0.000206\n",
      "37         37   [[0.9817519963685992], [-0.9999999962868071]]  0.000166\n",
      "38         38   [[0.9835767967317393], [-0.9999999977720843]]  0.000135\n",
      "39         39   [[0.9852191170585654], [-0.9999999986632505]]  0.000109\n",
      "40         40   [[0.9866972053527088], [-0.9999999991979504]]  0.000088\n",
      "41         41   [[0.9880274848174379], [-0.9999999995187702]]  0.000072\n",
      "42         42   [[0.9892247363356941], [-0.9999999997112621]]  0.000058\n",
      "43         43   [[0.9903022627021246], [-0.9999999998267572]]  0.000047\n",
      "44         44   [[0.9912720364319122], [-0.9999999998960544]]  0.000038\n",
      "45         45    [[0.992144832788721], [-0.9999999999376327]]  0.000031\n",
      "46         46   [[0.9929303495098489], [-0.9999999999625796]]  0.000025\n",
      "47         47   [[0.9936373145588641], [-0.9999999999775477]]  0.000020\n",
      "48         48   [[0.9942735831029776], [-0.9999999999865287]]  0.000016\n",
      "49         49   [[0.9948462247926799], [-0.9999999999919172]]  0.000013\n",
      "50         50    [[0.995361602313412], [-0.9999999999951503]]  0.000011\n",
      "51         51   [[0.9958254420820708], [-0.9999999999970902]]  0.000009\n",
      "52         52   [[0.9962428978738638], [-0.9999999999982542]]  0.000007\n",
      "53         53   [[0.9966186080864774], [-0.9999999999989525]]  0.000006\n",
      "54         54   [[0.9969567472778297], [-0.9999999999993715]]  0.000005\n",
      "55         55   [[0.9972610725500467], [-0.9999999999996229]]  0.000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dữ liệu đầu vào\n",
    "A = np.array([[1, 0], [0, -2]])  # Ma trận A\n",
    "b = np.array([[1], [2]])  # Vector b\n",
    "\n",
    "# Hàm chuẩn Euclid (norm)\n",
    "def norm_2(x):\n",
    "    return x.T @ x  # chuẩn L2\n",
    "\n",
    "# Hàm mục tiêu\n",
    "def obj(x):\n",
    "    return norm_2(A @ x - b)[0][0] / 2  # Giá trị của hàm mục tiêu\n",
    "\n",
    "# Gradient của hàm mục tiêu\n",
    "def grad_obj(x):\n",
    "    return A.T @ (A @ x - b)  # Gradient của hàm mục tiêu\n",
    "\n",
    "# Gradient Descent để tối ưu hóa\n",
    "def gradient_descent(A, b, x_init, learning_rate, max_iters, tolerance=1e-6):\n",
    "    x = x_init\n",
    "    history = []  # Lưu lại lịch sử các giá trị x và f(x)\n",
    "    for k in range(max_iters):\n",
    "        gradient = grad_obj(x)  # Tính gradient tại x\n",
    "        x = x - learning_rate * gradient  # Cập nhật giá trị x\n",
    "        f_val = obj(x)  # Tính giá trị hàm mục tiêu tại x\n",
    "        history.append((k, x.copy(), f_val))\n",
    "        if len(history) > 1 and abs(history[-1][2] - history[-2][2]) < tolerance:\n",
    "            break  # Kiểm tra điều kiện hội tụ\n",
    "    return history\n",
    "\n",
    "# Giá trị khởi tạo\n",
    "x_init = np.array([[0], [0]])  # Khởi tạo x^(0) là vector [0, 0]\n",
    "learning_rate = 0.1  # Learning rate\n",
    "max_iters = 100  # Số vòng lặp tối đa\n",
    "\n",
    "# Chạy thuật toán Gradient Descent\n",
    "results = gradient_descent(A, b, x_init, learning_rate, max_iters)\n",
    "\n",
    "# Hiển thị kết quả của 10 vòng lặp đầu tiên\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"Iteration\", \"x\", \"f(x)\"])\n",
    "print(df)  # Hiển thị 10 vòng lặp đầu tiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b51494f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giá trị x tối ưu: [-1.  1.]\n",
      "Giá trị hàm mục tiêu tối ưu: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Câu 1 (d): Giải với ma trận A và vector b mới\n",
    "# Dữ liệu mới\n",
    "\n",
    "A1 = np.array([[1, 0], [0, 4]])\n",
    "b1 = np.array([-1, 4])\n",
    "# Tìm nghiệm tối ưu\n",
    "def nghiem_toi_uu(A, b):\n",
    "    x_sao = np.linalg.pinv(A) @ b\n",
    "    q_sao = 0.5 * np.dot(x_sao, A @ x_sao) + np.dot(b, x_sao) + 2.5\n",
    "    return x_sao, q_sao\n",
    "\n",
    "# Dữ liệu đầu vào\n",
    "x_sao1, q_sao1 = nghiem_toi_uu(A1, b1)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(\"Giá trị x tối ưu:\", x_sao1)\n",
    "print(\"Giá trị hàm mục tiêu tối ưu:\", q_sao1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "194586d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giá trị x tối ưu: [ 0.24 -0.48]\n",
      "Giá trị hàm mục tiêu tối ưu: 1.5880000000000003\n"
     ]
    }
   ],
   "source": [
    "# Câu 1 (d): Giải với ma trận A và vector b mới\n",
    "# Dữ liệu mới\n",
    "A1 = np.array([[1, -2], [2, -4]])  # Ma trận mới\n",
    "b1 = np.array([2, 2])              # Vector mới\n",
    "\n",
    "# Tìm nghiệm tối ưu\n",
    "def nghiem_toi_uu(A, b):\n",
    "    x_sao = np.linalg.pinv(A) @ b\n",
    "    q_sao = 0.5 * np.dot(x_sao, A @ x_sao) + np.dot(b, x_sao) + 2.5\n",
    "    return x_sao, q_sao\n",
    "\n",
    "# Dữ liệu đầu vào\n",
    "x_sao1, q_sao1 = nghiem_toi_uu(A1, b1)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(\"Giá trị x tối ưu:\", x_sao1)\n",
    "print(\"Giá trị hàm mục tiêu tối ưu:\", q_sao1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa79e38-144a-44b0-a1a7-e84cd723e0be",
   "metadata": {},
   "source": [
    "# Câu 2: ( 4 điểm )\n",
    "Cho ma trận $U = \\begin{bmatrix} 1 & 2 \\\\ -2 & -4 \\end{bmatrix}$ và vector $w = \\begin{bmatrix} (2 ; 2) \\end{bmatrix}$.\n",
    "\n",
    "Xét bài toán tối ưu có ràng buộc của bài toán (1) như sau ( ma trận P và vector q được cho ở câu 1 )\n",
    "\n",
    "\\begin{align}\n",
    "\\min f(x) = \\frac{1}{2} x^T P x + q^T x + \\frac{5}{2} \\\\\n",
    "\\text{subject} {to} {Ux - w = 0.}\n",
    "\\end{align}\n",
    "\n",
    "Tìm giá trị nhỏ nhất của hàm mục tiêu:\n",
    "\n",
    "$f(x) = \\frac{1}{2} x^T P x + q^T x + \\frac{2}{5} \\ $\n",
    "\n",
    "Với ràng buộc:\n",
    "\n",
    "$ Ux - w = 0 $\n",
    "\n",
    "Trong đó, ma trận P và vector q được cho trước:\n",
    "\n",
    "P = $\\begin{bmatrix} 1 & 0 \\\\ 0 & 4 \\end{bmatrix}$  và vector q = $(-1, 4)$\n",
    "\n",
    "Các yêu cầu của bài toán:\n",
    "\n",
    "a) ( 2 điểm ) Gọi $x^\\ast$ và $u^\\ast$ là điểm tối ưu của bài toán trên và bài toán đối ngẫu của nó, với Điều kiện Karush-Kuhn-Tucker (KKT) cho $x^\\ast$ và $u^\\ast$\n",
    "\n",
    "b) ( 2 điểm ) Tìm một cặp giá trị ($x^\\ast$, $u^\\ast$)  thỏa mãn các điều kiện KKT vừa tìm được ở câu a). Từ đó, đưa ra giá trị tối ưu của bài toán trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70bfe5-284f-4cfc-abc7-45194382eb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
